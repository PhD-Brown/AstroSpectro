"""
AstroSpectro -- Visualizer
=========================

Interactive toolkit for **exploring, analysing, and interpreting** LAMOST spectra
inside a Jupyter notebook or via a lightweight Streamlit application.

Main capabilities
---------------------
- FITS header exploration with Markdown / Streamlit rendering
- Spectrum analysis (peak detection, line matching, quantitative metrics)
- Multi-spectrum comparison (normalisation + vertical offset)
- Dataset dashboards (zero rates, sub-class balance, sky coverage)
- Trained-model inspection and **SHAP interpretability**

Conventions
-----------
- All paths come from `paths = setup_project_env()` (see utils.py)
- Compressed FITS `.fits.gz` read in streaming mode (no on-disk extraction)
- Numpy / Astropy warnings related to `sqrt(invvar)` are silenced
  via `utils.make_stddev_uncertainty_from_invvar`.

Project dependencies
------------------
- pipeline.preprocessor.SpectraPreprocessor
- pipeline.classifier.SpectralClassifier
- pipeline.peak_detector.PeakDetector
- tools.dataset_builder.DatasetBuilder
- utils.make_stddev_uncertainty_from_invvar / utils.check_model_compat
"""

from __future__ import annotations

# --- Standard lib
import base64
import io
import os
import gzip
import glob
import warnings
import random
from typing import Dict, Tuple, Optional
from datetime import datetime, timezone

# --- Scientific / viz
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# --- Astro
from astropy.io import fits
from astropy.nddata import StdDevUncertainty
from astropy import units as u
from specutils import Spectrum, SpectralRegion
from specutils.analysis import centroid, gaussian_fwhm, equivalent_width

# --- Notebook UI
from IPython.display import display, Markdown, HTML
from ipywidgets import interact, widgets
from tqdm.notebook import tqdm

# --- ML / SHAP
import shap

# --- Projet
from pipeline.preprocessor import SpectraPreprocessor
from pipeline.feature_engineering import FeatureEngineer
from pipeline.classifier import SpectralClassifier
from pipeline.peak_detector import PeakDetector
from utils import (
    safe_sigma_from_invvar,
    latest_file,
)

# --- Constants (colours & default limits) -------------------------------

LINE_COLORS: Dict[str, str] = {
    "Hα": "#FF6B6B",
    "Hβ": "#4ECDC4",
    "CaII K": "#45B7D1",
    "CaII H": "#5A67D8",
    "Mg_b": "#F7B801",
    "Na_D": "#F18701",
}

DEFAULT_XLIM: Tuple[float, float] = (3800.0, 7000.0)
DEFAULT_YLIM: Tuple[float, float] = (-1.0, 2.5)


class AstroVisualizer:
    """
    Interactive tools for visualising and analysing LAMOST spectra.

    Parameters
    ----------
    paths : dict
        Dictionary of absolute paths produced by `setup_project_env()`.
        Must include at least: RAW_DATA_DIR, CATALOG_DIR, PROCESSED_DIR,
        MODELS_DIR, PROJECT_ROOT.

    Notes
    -----
    - The `interactive_*` methods are designed for **notebook** use.
    - The `app()` entry point targets **Streamlit**.
    """

    def __init__(self, paths: dict) -> None:
        """
        Initialise the visualiser with the project paths.

        Parameters
        ----------
        paths : dict
            Dictionary produced by `setup_project_env()`.  Must contain
            at least RAW_DATA_DIR, CATALOG_DIR, and PROJECT_ROOT.
        """
        self.paths = paths

        # Scan available .fits.gz (paths relative to RAW_DATA_DIR)
        self.available_spectra: list[str] = self._scan_for_spectra()

        # Model not loaded until actually needed
        self.classifier = None

        # (Optional) rescan on the fly:
        # self.refresh_available_spectra()

        print("AstroVisualizer initialised.  Models will be loaded on demand.")
        # (Optional) pre-load the labels catalogue
        self.labels_catalog: dict[str, str] = self._load_labels_catalog()
        if self.labels_catalog:
            print(f"  > Labels catalogue loaded ({len(self.labels_catalog)} entries).")

    def _load_labels_catalog(self) -> dict[str, str]:
        """
        Load a mapping {fits_filename_without_ext: subclass} from the
        temporary catalogue generated by the pipeline.

        Returns
        -------
        dict[str, str]
            Dictionary (key = 'spec.../filename' without '.fits.gz', value = subclass).
            Return an empty dict if the file does not exist or on error.
        """
        try:
            catalog_path = os.path.join(
                self.paths["CATALOG_DIR"], "master_catalog_temp.csv"
            )
            if not os.path.exists(catalog_path):
                return {}

            df_cat = pd.read_csv(catalog_path, sep="|")

            if "fits_name" not in df_cat.columns or "subclass" not in df_cat.columns:
                # Catalogue not in expected format
                return {}

            # Normalise the catalogue key (strip .fits.gz extension)
            # Exemple: 'M5901/spec-...-000.fits.gz' -> 'M5901/spec-...-000'
            key_series = (
                df_cat["fits_name"]
                .astype(str)
                .str.replace(".fits.gz", "", regex=False)
                .str.replace("\\", "/", regex=False)  # Windows robustness
            )

            labels = pd.Series(df_cat["subclass"].values, index=key_series).to_dict()
            return labels

        except Exception as e:
            print(f"  > Warning: unable to load the labels catalogue ({e}).")
            return {}

    def _scan_for_spectra(self) -> list[str]:
        """
        Recursively walk RAW_DATA_DIR and return a sorted list of
        '.fits.gz' files **relative** to that directory.

        Returns
        -------
        list[str]
            Relative paths under RAW_DATA_DIR, using '/' as separator (OS-independent).
            Exemple: 'M5901/spec-55859-...-000.fits.gz'
        """
        spectra: list[str] = []
        raw_dir = os.path.normpath(self.paths.get("RAW_DATA_DIR", "../data/raw"))

        if not os.path.isdir(raw_dir):
            return []

        for root, _, files in os.walk(raw_dir):
            for fname in files:
                if fname.endswith(".fits.gz"):
                    rel = os.path.relpath(os.path.join(root, fname), raw_dir)
                    # Convert Windows backslashes to POSIX for display / UI
                    spectra.append(rel.replace("\\", "/"))

        return sorted(spectra)

    # ==============================================================================
    # Tool 1: FITS Header Display
    # ==============================================================================

    def _format_header_line(self, header, label, key, unit=""):
        """
        Format a single FITS header entry as a Markdown line.

        Parameters
        ----------
        header : astropy.io.fits.Header
            FITS header object.
        label : str
            Human-readable label displayed in bold.
        key : str
            Header keyword to extract.
        unit : str, optional
            Unit string appended to the value (e.g. 'Å').

        Returns
        -------
        str
            Markdown string (value or 'N/A' when absent).
        """
        value = header.get(key, "N/A")
        unit_str = f" {unit}" if unit else ""
        return f"- **{label} :** `{value}`{unit_str}\n"

    def _display_formatted_header(self, fits_relative_path):
        """
        Open a compressed FITS (.fits.gz), read the primary header (HDU 0),
        and render it as sectioned Markdown.

        Parameters
        ----------
        fits_relative_path : str
            Relative path under RAW_DATA_DIR to the selected .fits.gz file.
        """
        full_path = os.path.join(self.paths["RAW_DATA_DIR"], fits_relative_path)
        try:
            with gzip.open(full_path, "rb") as f_gz:
                with fits.open(f_gz, memmap=False) as hdul:
                    header = hdul[0].header
            md_output = f"### File header : `{os.path.basename(full_path)}`\n---\n"
            sections = {}
            current_section = "General Information"
            sections[current_section] = ""
            for key, value in header.items():
                if key == "COMMENT" and "--------" in str(value):
                    current_section = (
                        str(value).replace("COMMENT", "").replace("-", "").strip()
                    )
                    sections.setdefault(current_section, "")
                elif key and key not in ["COMMENT", "HISTORY", ""]:
                    sections[current_section] += self._format_header_line(
                        header, key, key
                    )
            for title, content in sections.items():
                if content:
                    md_output += f"\n#### {title}\n{content}"
            display(Markdown(md_output))
        except Exception as e:
            display(Markdown(f"### Error\n**File:** `{full_path}`\n\n**Detail:** {e}"))

    def interactive_header_explorer(self):
        """
        Notebook widget for browsing FITS headers with a single click.

        Display a `Dropdown` listing every discovered spectrum and render a
        neatly structured Markdown view of the selected file header.
        """
        display(Markdown("## FITS Header Explorer"))
        display(
            Markdown(
                "Use the drop-down menu to select a spectrum and display its full metadata."
            )
        )
        if not self.available_spectra:
            print("No spectra found to display.")
            return
        interact(
            self._display_formatted_header, fits_relative_path=self.available_spectra
        )

    # ==============================================================================
    # Notebook Tool 2: Interactive Spectrum Analyser
    # ==============================================================================

    def _plot_spectrum_analysis(
        self, file_path, prominence, window, xlim, ylim
    ) -> None:
        """
        Load, normalise, and analyse a single spectrum, then plot it (Matplotlib).

        Parameters
        ----------
        file_path : str
            Relative path (under RAW_DATA_DIR) of the `.fits.gz` file to analyse.
        prominence : float
            Prominence threshold for peak detection.
        window : int
            Half-window (Å) used for local measurements around a line.
        xlim : tuple[float, float]
            X limits (Å) for the plot (may be inverted).
        ylim : tuple[float, float]
            Y limits (flux) for the plot (may be inverted).

        Notes
        -----
        - Plot the normalised spectrum, mark detected peaks, and
        highlight target lines (Balmer, Ca II, Mg_b, Na_D).
        - No local metrics (FWHM / EW) are computed here; that is handled by
        tool #3 (Analyser & Tuner).
        """
        preprocessor = SpectraPreprocessor()
        peak_detector = PeakDetector(prominence=prominence, window=window)
        full_path = os.path.join(self.paths["RAW_DATA_DIR"], file_path)

        try:
            # --- Safe read of compressed FITS ---
            with gzip.open(full_path, "rb") as f_gz:
                with fits.open(f_gz, memmap=False) as hdul:
                    wavelength, flux, invvar = preprocessor.load_spectrum(hdul)
                    header = hdul[0].header

            # --- Numpy arrays + normalisation ---
            wavelength = np.asarray(wavelength, dtype=np.float64)
            flux = np.asarray(flux, dtype=np.float64)
            flux_norm = preprocessor.normalize_spectrum(flux)

            # --- Peak detection & target-line matching ---
            peak_idx, props = peak_detector.detect_peaks(wavelength, flux_norm)
            peak_wl = wavelength[peak_idx]
            matched = peak_detector.match_known_lines(peak_idx, peak_wl, props)
            matched_wl = [data[0] for data in matched.values() if data is not None]

            # --- Plot ---
            plt.style.use("dark_background")
            fig, ax = plt.subplots(figsize=(18, 7))

            # Normalised spectrum
            ax.plot(
                wavelength,
                flux_norm,
                color="gray",
                alpha=0.75,
                label="Normalised spectrum",
            )

            # All detected peaks
            if peak_idx.size:
                ax.scatter(
                    peak_wl,
                    flux_norm[peak_idx],
                    s=40,
                    marker="v",
                    color="red",
                    label="Detected peaks",
                )

            # Peaks matched to target lines (green circles)
            if matched_wl:
                j_idx = [np.abs(wavelength - wl).argmin() for wl in matched_wl]
                ax.scatter(
                    wavelength[j_idx],
                    flux_norm[j_idx],
                    s=150,
                    facecolors="none",
                    edgecolors="lime",
                    linewidth=2,
                    label="Matched peaks",
                )

            # Vertical lines at theoretical line positions
            for name, wl in peak_detector.target_lines.items():
                ax.axvline(
                    wl,
                    color=LINE_COLORS.get(name, "dodgerblue"),
                    linestyle="--",
                    alpha=0.85,
                    label=f"Raie {name}",
                )

            # Titre + axes
            true_subclass = header.get("SUBCLASS", "N/A")
            title_obj = header.get("DESIG", os.path.basename(file_path))
            ax.set_title(
                f"Analysis: {title_obj} (True class: {true_subclass})", fontsize=16
            )
            ax.set_xlabel("Longueur d'onde (Å)")
            ax.set_ylabel("Normalised flux")
            ax.grid(True, linestyle=":")

            # Robust limits (accept inverted sliders)
            def _sorted_pair(p, default):
                if not p:
                    return default
                a, b = float(p[0]), float(p[1])
                return (a, b) if a <= b else (b, a)

            x0, x1 = _sorted_pair(xlim, DEFAULT_XLIM)
            y0, y1 = _sorted_pair(ylim, DEFAULT_YLIM)
            ax.set_xlim(x0, x1)
            ax.set_ylim(y0, y1)

            # Legend without duplicates
            handles, labels = ax.get_legend_handles_labels()
            uniq = dict(zip(labels, handles))
            ax.legend(uniq.values(), uniq.keys())

            plt.show()

        except Exception as e:
            print(f"Error: {e}")

    def interactive_spectrum_analyzer(self) -> None:
        """
        Notebook mini-UI for exploring a spectrum and tuning peak detection.

        Display a small control panel (file selector + sliders) and
        redraw the figure on every change via `_plot_spectrum_analysis`.

        Controls
        ---------
        - Spectrum: drop-down of available `.fits.gz` files (relative path).
        - Prominence: peak-detection prominence threshold.
        - Window (Å): half-window for local measurements.
        - Zoom X (Å): requested X range (Å).
        - Zoom Y (Flux): requested Y range (normalised flux).
        - *Reset* button: restore all controls to their default values.

        Notes
        -----
        - The range sliders (X / Y) use `continuous_update=False` to avoid
        redrawing on every pixel during drag (smoother and cheaper).
        - Inverted X/Y limits are accepted; `_plot_spectrum_analysis`
        always sorts before applying.
        """
        if not self.available_spectra:
            print("No spectra found to analyse.")
            return

        # --- Widgets -------------------------------------------------------------
        file_path_widget = widgets.Dropdown(
            options=sorted(self.available_spectra),
            description="Spectrum:",
            layout={"width": "max-content"},
        )

        prominence_widget = widgets.FloatSlider(
            min=0.0, max=2.0, step=0.01, value=0.2, description="Prominence:"
        )

        window_widget = widgets.IntSlider(
            min=1, max=50, step=1, value=15, description="Window (Å):"
        )

        xlim_widget = widgets.FloatRangeSlider(
            value=list(DEFAULT_XLIM),
            min=3500,
            max=9500,
            step=1,
            description="Zoom X (Å):",
            continuous_update=False,
            layout={"width": "500px"},
        )

        ylim_widget = widgets.FloatRangeSlider(
            value=list(DEFAULT_YLIM),
            min=-2,
            max=5,
            step=0.1,
            description="Zoom Y (Flux):",
            continuous_update=False,
            layout={"width": "500px"},
        )

        reset_btn = widgets.Button(
            description="Reset",
            button_style="warning",
            icon="refresh",
            tooltip="Reset all controls to their default values",
            layout={"width": "150px"},
        )

        def _on_reset(_=None):
            prominence_widget.value = 0.2
            window_widget.value = 15
            xlim_widget.value = tuple(DEFAULT_XLIM)
            ylim_widget.value = tuple(DEFAULT_YLIM)

        reset_btn.on_click(_on_reset)

        # --- Control -> plot bindings -----------------------------------------
        out = widgets.interactive_output(
            self._plot_spectrum_analysis,
            {
                "file_path": file_path_widget,
                "prominence": prominence_widget,
                "window": window_widget,
                "xlim": xlim_widget,
                "ylim": ylim_widget,
            },
        )

        # --- Mise en page --------------------------------------------------------
        controls = widgets.VBox(
            [
                file_path_widget,
                widgets.HBox([prominence_widget, window_widget, reset_btn]),
                xlim_widget,
                ylim_widget,
            ]
        )

        display(widgets.VBox([controls, out]))

    # ==============================================================================
    # Tool 3: Interactive Spectrum Analyser & Tuner (Notebook)
    # ==============================================================================

    def _plot_peak_detection_notebook(
        self,
        file_path: str,
        prominence: float,
        window: int,
        xlim: tuple[float, float],
        ylim: tuple[float, float],
        model_path: str,
    ) -> None:
        """
        Interactive spectrum analysis inside the notebook.

        - Load a compressed FITS (.fits.gz), normalise the flux, and detect peaks.
        - Match peaks to target lines (Balmer, Ca II, Mg_b, Na_D).
        - Plot the annotated spectrum (Matplotlib) with a PNG export link.
        - Compute a table of local metrics for matched peaks
        (centroid, FWHM, EW, prominence) and offer CSV / LaTeX export.
        - (Optional) Load a .pkl model and display the **predicted class**.
        - Maintain a **session log** and allow export of an **HTML report**.

        Parameters
        ----------
        file_path : str
            Path *relative to* RAW_DATA_DIR of the spectrum to analyse.
        prominence : float
            Prominence threshold (peak detection).
        window : int
            Half-window (Å) for local metrics around each line.
        xlim, ylim : tuple[float, float]
            X (Å) and Y (flux) limits requested by the sliders.
        model_path : str
            Absolute path of the selected .pkl model (ou "None").

        Notes
        -----
        - Astropy `invalid value encountered in sqrt` warnings are silenced
        via `utils.safe_sigma_from_invvar` and a `catch_warnings` block.
        - Columns expected by the model are **aligned** automatically.
        """
        import builtins

        preprocessor = SpectraPreprocessor()
        peak_detector = PeakDetector(prominence=prominence, window=window)
        full_path = os.path.join(self.paths["RAW_DATA_DIR"], file_path)

        try:
            # --- Read compressed FITS ---
            with gzip.open(full_path, "rb") as f_gz:
                with fits.open(f_gz, memmap=False) as hdul:
                    wavelength, flux, invvar = preprocessor.load_spectrum(hdul)
                    header = hdul[0].header

            true_subclass = header.get("SUBCLASS", "Unknown")

            # --- Numpy arrays + normalisation ---
            wavelength, flux, invvar = (
                np.asarray(wavelength, dtype=np.float64),
                np.asarray(flux, dtype=np.float64),
                np.asarray(invvar, dtype=np.float64),
            )
            flux_norm = preprocessor.normalize_spectrum(flux)

            # --- Peak detection & target-line matching ---
            peak_idx, props = peak_detector.detect_peaks(wavelength, flux_norm)
            peak_wl = wavelength[peak_idx]
            matched = peak_detector.match_known_lines(peak_idx, peak_wl, props)

            # --- Feature preparation (for model) ---
            fe = FeatureEngineer()
            vec = fe.extract_features(
                matched, wavelength, flux_norm, invvar
            )  # ML-side computation

            # --- ML prediction (optional) ---
            predicted_label = "No model selected"
            if model_path != "None":
                try:
                    clf = SpectralClassifier.load_model(model_path)
                    clf.prediction_target = "main_class"

                    # Map features -> DataFrame aligned with the model
                    current = {name: val for name, val in zip(fe.feature_names, vec)}

                    # Optional metadata expected by the model
                    for m in ("redshift", "snr_g", "snr_r", "snr_i", "seeing"):
                        if m in getattr(clf, "feature_names_used", []):
                            try:
                                current[m] = float(header.get(m.upper(), 0.0))
                            except (TypeError, ValueError):
                                current[m] = 0.0

                    X = pd.DataFrame([current])
                    expected = getattr(clf, "feature_names_used", list(X.columns))
                    for c in expected:
                        if c not in X.columns:
                            X[c] = 0.0
                    X = X[expected]

                    yhat = clf.model_pipeline.predict(X)[0]
                    predicted_label = (
                        clf.class_labels[yhat] if clf.model_type == "XGBoost" else yhat
                    )
                except Exception as e:
                    predicted_label = f"Prediction error: {e}"

            # --- Figure Matplotlib ---
            plt.style.use("dark_background")
            plt.figure(figsize=(18, 7))
            plt.plot(
                wavelength,
                flux_norm,
                color="gray",
                alpha=0.7,
                label="Normalised spectrum",
            )

            if len(peak_idx) > 0:
                plt.scatter(
                    peak_wl,
                    flux_norm[peak_idx],
                    color="red",
                    marker="v",
                    s=40,
                    label="Detected peaks",
                )

            matched_wl, matched_flux = [], []
            for name, data in matched.items():
                if data is None:
                    continue
                wl, prom = data
                j = int(np.abs(wavelength - wl).argmin())
                matched_wl.append(wl)
                matched_flux.append(float(flux_norm[j]))
                plt.text(
                    wl,
                    flux_norm[j] + 0.05,
                    name,
                    color=LINE_COLORS.get(name, "white"),
                    fontsize=13,
                    ha="center",
                    weight="bold",
                )

            if matched_wl:
                plt.scatter(
                    matched_wl,
                    matched_flux,
                    s=150,
                    facecolors="none",
                    edgecolors="lime",
                    linewidth=2,
                    label="Matched peaks",
                )

            for name, wl in peak_detector.target_lines.items():
                plt.axvline(
                    wl,
                    color=LINE_COLORS.get(name, "dodgerblue"),
                    linestyle="--",
                    alpha=0.8,
                    label=f"Raie {name}",
                )

            # Robust, sorted limits
            def _sorted_pair(p, default):
                if not p:
                    return default
                a, b = float(p[0]), float(p[1])
                return (a, b) if a <= b else (b, a)

            x0, x1 = _sorted_pair(xlim, DEFAULT_XLIM)
            y0, y1 = _sorted_pair(ylim, DEFAULT_YLIM)

            plt.title(
                f"Analyse : {header.get('DESIG', os.path.basename(file_path))} "
                f"(Vraie classe: {true_subclass})",
                fontsize=16,
            )
            plt.xlabel("Longueur d'onde (Å)")
            plt.ylabel("Normalised flux")
            plt.xlim((x0, x1))
            plt.ylim((y0, y1))
            plt.grid(True, linestyle=":")

            # Legend without duplicates
            handles, labels = plt.gca().get_legend_handles_labels()
            by = dict(zip(labels, handles))
            plt.legend(by.values(), by.keys())

            # Export PNG (base64)
            buf = io.BytesIO()
            plt.savefig(buf, format="png", bbox_inches="tight", dpi=150)
            buf.seek(0)
            b64 = base64.b64encode(buf.read()).decode()
            display(
                HTML(
                    f'<a download="spectre.png" href="data:image/png;base64,{b64}">⬇️ Exporter le graphique (PNG)</a>'
                )
            )
            plt.show()

            # --- Metrics table for matched lines ---
            #  -> build a full-frame Spectrum with safe uncertainties
            sigma = safe_sigma_from_invvar(invvar)  # helper utils.py
            spec_full = Spectrum(
                spectral_axis=wavelength * u.AA,
                flux=flux_norm * u.adu,
                uncertainty=StdDevUncertainty(sigma),
            )

            results: dict[str, dict] = {}
            for name, data in matched.items():
                if data is None:
                    continue
                wl, prom = data
                region = SpectralRegion((wl - window) * u.AA, (wl + window) * u.AA)
                try:
                    with warnings.catch_warnings():
                        warnings.filterwarnings(
                            "ignore",
                            message="invalid value encountered in sqrt",
                            category=RuntimeWarning,
                            module=r".*astropy\.units\.quantity.*",
                        )
                        c = float(centroid(spec_full, regions=region).to_value(u.AA))
                        w = float(
                            gaussian_fwhm(spec_full, regions=region).to_value(u.AA)
                        )
                        ew = float(
                            equivalent_width(spec_full, regions=region).to_value(u.AA)
                        )

                    results[name] = {
                        "Centroid (Å)": f"{c:.2f}",
                        "Largeur FWHM (Å)": f"{w:.2f}",
                        "Prominence": f"{prom:.3f}",
                        "Equiv. width (Å)": f"{ew:.3f}",
                    }
                except Exception as e:
                    results[name] = {"Error": "Analysis failed", "Detail": str(e)}

            df_an = None
            if results:
                display(Markdown("#### Quantitative analysis of matched lines"))
                df_an = pd.DataFrame.from_dict(results, orient="index")

                def _hl(row):
                    col = LINE_COLORS.get(row.name)
                    return [
                        f"background-color:{col}; color:black" if col else ""
                        for _ in row
                    ]

                display(df_an.style.apply(_hl, axis=1))

                # Exports CSV / LaTeX
                csv = io.StringIO()
                df_an.to_csv(csv)
                b64_csv = base64.b64encode(csv.getvalue().encode()).decode()
                tex = df_an.to_latex(
                    index=True, caption="Quantitative analysis of matched lines"
                )
                b64_tex = base64.b64encode(tex.encode()).decode()
                display(
                    HTML(
                        f"<div style='margin:8px 0'><a download='line_analysis.csv' "
                        f"href='data:text/csv;base64,{b64_csv}'>⬇️ CSV</a>"
                        " &nbsp;|&nbsp; "
                        f"<a download='line_analysis.tex' href='data:text/plain;base64,{b64_tex}'>⬇️ LaTeX</a></div>"
                    )
                )

            # --- Summary: predicted class ---
            display(Markdown(f"**Predicted spectral class (ML):** `{predicted_label}`"))

            # --- Historique de session + export rapport HTML ---
            if not hasattr(builtins, "log_analyses"):
                builtins.log_analyses = pd.DataFrame(
                    columns=[
                        "File",
                        "Parameters",
                        "Tableau",
                        "Timestamp",
                        "Predicted class",
                    ]
                )

            params = {
                "spectre": file_path,
                "prominence": prominence,
                "window": window,
                "xlim": (x0, x1),
                "ylim": (y0, y1),
            }
            builtins.log_analyses = pd.concat(
                [
                    builtins.log_analyses,
                    pd.DataFrame(
                        [
                            {
                                "File": file_path,
                                "Parameters": params,
                                "Tableau": (
                                    df_an.to_dict()
                                    if isinstance(df_an, pd.DataFrame)
                                    else {}
                                ),
                                "Timestamp": datetime.now().strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                ),
                                "Predicted class": predicted_label,
                            }
                        ]
                    ),
                ],
                ignore_index=True,
            )

            if len(builtins.log_analyses) > 0:
                display(Markdown("#### Analysis history (session)"))
                display(
                    builtins.log_analyses[
                        ["File", "Predicted class", "Parameters", "Timestamp"]
                    ]
                )

            from ipywidgets import Button

            def _report(_=None):
                html = "<h1>Spectroscopic Analysis Report</h1>"
                for i, row in builtins.log_analyses.iterrows():
                    html += f"<h2>Analysis #{i+1} -- {row['File']}</h2>"
                    if row.get("Predicted class") is not None:
                        html += f"<p><b>Predicted spectral class:</b> {row['Predicted class']}</p>"
                    html += f"<pre>{row['Parameters']}</pre>"
                    try:
                        html += pd.DataFrame.from_dict(row["Tableau"]).to_html()
                    except Exception:
                        html += "<p><i>Table unavailable.</i></p>"
                    html += f"<p><i>Timestamp : {row['Timestamp']}</i></p><hr>"

                b = io.BytesIO()
                b.write(html.encode())
                b.seek(0)
                b64 = base64.b64encode(b.read()).decode()
                display(
                    HTML(
                        f'<a download="astro_report.html" href="data:text/html;base64,{b64}">⬇️ Download HTML report</a>'
                    )
                )

            btn = Button(
                description="Generate session HTML report",
                button_style="success",
            )
            btn.on_click(_report)
            display(btn)

        except Exception as e:
            print(f"Error while processing the spectrum : {e}")

    def interactive_peak_tuner(self):
        """ipywidgets UI for tuning peak detection and (optionally) predicting with a .pkl model."""
        display(Markdown("--- \n## Augmented Spectrum Analyser"))
        display(
            Markdown(
                "Cet outil tout-en-un vous permet de visualiser un spectre, d'ajuster les "
                "peak-detection parameters in real time, and assess data and analysis "
                "quality."
            )
        )
        if not self.available_spectra:
            print("No spectra found.")
            return

        models_dir = self.paths.get("MODELS_DIR", "../data/models/")
        saved = ["None"]
        if os.path.isdir(models_dir):
            files = [
                os.path.join(models_dir, f)
                for f in os.listdir(models_dir)
                if f.endswith(".pkl")
            ]
            saved += sorted(files, key=os.path.getctime, reverse=True)

        ui = widgets.interactive(
            self._plot_peak_detection_notebook,
            file_path=widgets.Dropdown(
                options=self.available_spectra,
                description="Spectrum:",
                layout={"width": "max-content"},
            ),
            prominence=widgets.FloatSlider(
                min=0.01, max=1.0, step=0.01, value=0.2, description="Prominence:"
            ),
            window=widgets.IntSlider(
                min=1, max=50, step=1, value=15, description="Window (Å):"
            ),
            xlim=widgets.FloatRangeSlider(
                value=list(DEFAULT_XLIM),
                min=3500,
                max=9500,
                step=1,
                description="Zoom X (Å):",
                continuous_update=False,
                layout={"width": "500px"},
            ),
            ylim=widgets.FloatRangeSlider(
                value=list(DEFAULT_YLIM),
                min=-2,
                max=5,
                step=0.1,
                description="Zoom Y (Flux):",
                continuous_update=False,
                layout={"width": "500px"},
            ),
            model_path=widgets.Dropdown(
                options=saved,
                description="ML model:",
                layout={"width": "max-content"},
            ),
        )
        display(ui)

    # ==============================================================================
    # Tool 4: Feature Zero Analysis ---
    # ==============================================================================

    def analyze_feature_zeros(
        self,
        n_files: int = 100,
        *,
        prominence: float = 0.2,
        window: int = 15,
        eps: float = 1e-12,
        random_state: int = 42,
        export: bool = True,
        show_plot: bool = False,
    ) -> pd.DataFrame:
        """
        Analyse feature *sparsity* (null / constant values) on a sample of spectra.

        This routine takes a subset of files available under `data/raw/`,
        computes features via :class:`FeatureEngineer`, then measures :
        - the **count** and **rate** of zeros (|x| <= eps) per feature,
        - whether a feature is **constant** (std ~ 0 or unique values <= 1),
        - quick stats (min, max, mean, std).

        Results displayed in the notebook with optional CSV / LaTeX export.

        Parameters
        ----------
        n_files : int, default 100
            Number of spectra to sample (capped by the number available).
        prominence : float, default 0.2
            Prominence parameter for peak detection (consistent with the interactive tool).
        window : int, default 15
            Half-window in Å for line analysis (consistent with the interactive tool).
        eps : float, default 1e-12
            Near-zero threshold: |x| <= eps is treated as zero.
        random_state : int, default 42
            Random seed for file sampling.
        export : bool, default True
            If True, offer CSV and LaTeX export of the summary.

        Returns
        -------
        pandas.DataFrame
            Summary table per feature : `zero_count`, `zero_rate`, `is_constant`,
            `min`, `max`, `mean`, `std`. Sorted by `zero_rate` descending.

        Notes
        -----
        - A larger `n_files` gives a more reliable sparsity estimate (at higher CPU cost).
        - Numerical warnings from `astropy` are already silenced in the helpers
        used by the feature-extraction chain (voir `FeatureEngineer.extract_features`).
        """
        display(Markdown("--- \n## Feature Quality Analysis"))
        display(
            Markdown(
                "This tool analyses the last generated feature file and shows the percentage of null values for each feature.  This is essential for identifying uninformative features."
            )
        )

        # Retrieve and sample the list of available spectra
        files = list(self.available_spectra)
        if not files:
            display(Markdown("> No spectra found in `data/raw/`."))
            return pd.DataFrame()

        random.seed(random_state)
        if n_files < len(files):
            files = random.sample(files, k=n_files)

        preprocessor = SpectraPreprocessor()
        detector = PeakDetector(prominence=prominence, window=window)
        fe = FeatureEngineer()

        rows = []
        processed = 0
        errors = 0

        for rel in files:
            try:
                full_path = os.path.join(self.paths["RAW_DATA_DIR"], rel)
                with gzip.open(full_path, "rb") as f_gz:
                    with fits.open(f_gz, memmap=False) as hdul:
                        wl, fl, inv = preprocessor.load_spectrum(hdul)

                fl = preprocessor.normalize_spectrum(fl)
                wl = np.asarray(wl, dtype=np.float64)
                fl = np.asarray(fl, dtype=np.float64)
                inv = np.asarray(inv, dtype=np.float64)

                peak_idx, props = detector.detect_peaks(wl, fl)
                matched = detector.match_known_lines(peak_idx, wl[peak_idx], props)

                vec = fe.extract_features(matched, wl, fl, inv)
                rows.append(vec)
                processed += 1
            except Exception:
                # Continue (some spectra may be corrupt or atypical)
                errors += 1
                continue

        if not rows:
            display(Markdown("> Unable to compute features on the sample."))
            return pd.DataFrame()

        X = np.vstack(rows)
        df = pd.DataFrame(X, columns=fe.feature_names)

        # Near-zero mask
        zero_mask = df.abs() <= eps
        zero_count = zero_mask.sum(axis=0)
        zero_rate = (zero_count / len(df)).astype(float)

        # Constants (std ~ 0 or a single distinct value)
        std = df.std(ddof=0)
        nunique = df.nunique(dropna=False)
        is_constant = (std <= eps) | (nunique <= 1)

        summary = pd.DataFrame(
            {
                "zero_count": zero_count,
                "zero_rate": zero_rate,
                "is_constant": is_constant,
                "min": df.min(),
                "max": df.max(),
                "mean": df.mean(),
                "std": std,
            }
        ).sort_values(["zero_rate", "is_constant"], ascending=[False, False])

        # Display
        display(
            Markdown(
                f"**Sample analysed:** {processed} spectrum(a)"
                + (f" &nbsp;•&nbsp; **erreurs** : {errors}" if errors else "")
            )
        )
        display(
            Markdown(
                f"- **100 % null features** : {int((summary['zero_rate'] == 1.0).sum())}  \n"
                f"- **Constant features** : {int(summary['is_constant'].sum())}"
            )
        )

        def _row_style(row):
            # Highlight: all-zero -> red background, otherwise gradient on zero_rate
            if row["zero_rate"] == 1.0:
                return ["background-color:#5a1a1a;color:#fff"] * len(row)
            return [
                "background-color:rgba(0,150,255,{:.2f})".format(row["zero_rate"])
            ] + [""] * (len(row) - 1)

        display(Markdown("#### Feature sparsity (sorted by `zero_rate` ↓)"))
        try:
            display(
                summary.style.apply(_row_style, axis=1).format(
                    {
                        "zero_rate": "{:.2%}",
                        "min": "{:.3g}",
                        "max": "{:.3g}",
                        "mean": "{:.3g}",
                        "std": "{:.3g}",
                    }
                )
            )
        except Exception:
            # Fallback without styling if the environment does not support .style
            display(summary)

        # Export (optional)
        if export:
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            csv = io.StringIO()
            summary.to_csv(csv)
            b64_csv = base64.b64encode(csv.getvalue().encode()).decode()

            tex = summary.to_latex(index=True, caption="Feature sparsity (sample)")
            b64_tex = base64.b64encode(tex.encode()).decode()

            display(
                HTML(
                    "<div style='margin:8px 0'>"
                    f"<a download='feature_zero_summary_{ts}.csv' href='data:text/csv;base64,{b64_csv}'>⬇️ CSV</a>"
                    " &nbsp;|&nbsp; "
                    f"<a download='feature_zero_summary_{ts}.tex' href='data:text/plain;base64,{b64_tex}'>⬇️ LaTeX</a>"
                    "</div>"
                )
            )
        return summary

    # ==============================================================================
    # Tool 5: Sky Coverage Map ---
    # ==============================================================================

    def _generate_position_catalog(
        self, force_regenerate: bool = False
    ) -> pd.DataFrame:
        """
        Build (or reload) a small catalogue of sky positions **per plan**
        from FITS headers (.fits.gz).  The returned positions are the
        mean RA / Dec per `PLANID`.

        Parameters
        ----------
        force_regenerate : bool, default False
            If False and the CSV cache already exists, reload it directly.
            If True, rescan the FITS files and regenerate the CSV.

        Returns
        -------
        pandas.DataFrame
            DataFrame sorted by `plan_id` containing:
            - `plan_id` : identifiant de plan (header `PLANID`)
            - `ra`      : moyenne (deg) des RA des spectres du plan (0..360)
            - `dec`     : moyenne (deg) des Dec des spectres du plan (-90..+90)
            - `spectra_count` : nombre de spectres vus pour ce plan

        Notes
        -----
        - Required headers: `RA`, `DEC`, `PLANID`.  Entries missing these
        fields or with invalid coordinates are ignored.
        - The result is cached in
        `<CATALOG_DIR>/spectra_position_catalog.csv`.
        - Used by `plot_sky_coverage()`.
        """
        catalog_path = os.path.join(
            self.paths["CATALOG_DIR"], "spectra_position_catalog.csv"
        )

        def _fits_latest_mtime(raw_dir: str) -> float:
            latest = 0.0
            if os.path.isdir(raw_dir):
                for root, _, files in os.walk(raw_dir):
                    for f in files:
                        if f.endswith(".fits.gz"):
                            try:
                                t = os.path.getmtime(os.path.join(root, f))
                                if t > latest:
                                    latest = t
                            except Exception:
                                continue
            return latest

        # 1) On-disk cache -- trust only if non-empty, valid, and up to date.
        if os.path.exists(catalog_path) and not force_regenerate:
            try:
                df_cached = pd.read_csv(catalog_path)
                expected = {"plan_id", "ra", "dec"}
                cache_ok = expected.issubset(df_cached.columns) and (len(df_cached) > 0)

                # Rebuild if cache is empty / invalid
                if not cache_ok:
                    print("  > Cache present but empty/invalid -- rescanning FITS...")
                else:
                    # Rebuild if FITS are newer than the CSV
                    cache_mtime = os.path.getmtime(catalog_path)
                    latest_fits_mtime = _fits_latest_mtime(
                        self.paths.get("RAW_DATA_DIR", "../data/raw")
                    )
                    if latest_fits_mtime > cache_mtime:
                        print("  > FITS newer than cache -- regenerating...")
                    else:
                        print("  > Existing position catalogue loaded.")
                        return df_cached.sort_values("plan_id").reset_index(drop=True)
            except Exception:
                print("  > Warning: unreadable cache -- rescanning FITS...")

        # 2) Scan des FITS
        print("  > Scanning FITS files to generate position catalogue...")
        if not self.available_spectra:
            print("  > No FITS files found.")
            return pd.DataFrame(columns=["plan_id", "ra", "dec", "spectra_count"])

        rows: list[dict] = []
        invalid = 0

        for rel_path in tqdm(self.available_spectra, desc="Reading headers"):
            file_path = os.path.join(self.paths["RAW_DATA_DIR"], rel_path)
            try:
                with gzip.open(file_path, "rb") as f_gz:
                    with fits.open(f_gz, memmap=False) as hdul:
                        h = hdul[0].header

                if not all(k in h for k in ("PLANID", "RA", "DEC")):
                    continue

                plan_id = h["PLANID"]
                try:
                    ra = float(h["RA"]) % 360.0
                    dec = float(h["DEC"])
                except Exception:
                    invalid += 1
                    continue

                if (
                    (not np.isfinite(ra))
                    or (not np.isfinite(dec))
                    or not (-90.0 <= dec <= 90.0)
                ):
                    invalid += 1
                    continue

                rows.append({"plan_id": plan_id, "ra": ra, "dec": dec})

            except Exception:
                invalid += 1
                continue

        if not rows:
            print("  > No usable coordinates were extracted.")
            return pd.DataFrame(columns=["plan_id", "ra", "dec", "spectra_count"])

        df_positions = pd.DataFrame(rows)

        # 3) Aggregate by plan
        df_plan = (
            df_positions.groupby("plan_id")
            .agg(ra=("ra", "mean"), dec=("dec", "mean"), spectra_count=("ra", "size"))
            .reset_index()
            .sort_values("plan_id")
            .reset_index(drop=True)
        )

        # 4) Save cache
        try:
            os.makedirs(self.paths["CATALOG_DIR"], exist_ok=True)
            df_plan.to_csv(catalog_path, index=False)
        except Exception as e:
            print(f"  > Warning: unable to write cache CSV ({e}).")

        print(
            f"  > Position catalogue created with {len(df_plan)} unique plan(s)."
            + (f"  [{invalid} entry(ies) ignored]" if invalid else "")
        )
        return df_plan

    def plot_sky_coverage(
        self, save_path: str | None = None
    ) -> tuple[plt.Figure, plt.Axes] | None:
        """
        Display a Mollweide projection map of sky coverage (RA / Dec centroids per plan).

        Parameters
        ----------
        save_path : str | None, default None
            Save path (PNG).  If None, a timestamped file is written to
            `paths['LOGS_DIR']`.  If False / "", no file is saved.

        Returns
        -------
        (fig, ax) | None
            The Matplotlib figure and axis if data are available, else None.

        Notes
        -----
        - Uses the cache CSV generated by `_generate_position_catalog`.
        - Longitudes are converted to Mollweide coordinates
        (RA vers la gauche, centre 0h/24h).
        - Colour and size encode `spectra_count` (number of spectra per plan).
        """
        display(Markdown("--- \n## Sky Coverage Map"))
        display(
            Markdown(
                "This map shows the positions of downloaded observation plans.  Point size and colour indicate the number of spectra per plan."
            )
        )

        # -- 1) Retrieve / build the small position catalogue
        df = self._generate_position_catalog()
        if df.empty:
            print("  > Empty cache -- forced rescan...")
            df = self._generate_position_catalog(force_regenerate=True)
            if df.empty:
                print("  > No position data available for the coverage map.")
                return None

        # -- 2) Optional: background image (Milky Way in Mollweide projection)
        bg_image = None
        try:
            from PIL import Image

            image_path = os.path.join(
                self.paths["PROJECT_ROOT"],
                "static",
                "images",
                "milky_way_mollweide.jpeg",
            )
            if os.path.exists(image_path):
                bg_image = Image.open(image_path)
        except Exception:
            bg_image = None  # pas bloquant

        # -- 3) Coordinate preparation -> Mollweide
        ra_deg = df["ra"].to_numpy(dtype=float)
        dec_deg = df["dec"].to_numpy(dtype=float)

        # RA (deg) -> radians, inversion pour avoir RA vers la gauche, plage [-π, π]
        lon = np.radians(ra_deg)
        x = np.pi - lon
        x = (x + np.pi) % (2 * np.pi) - np.pi
        y = np.radians(dec_deg)

        counts = df.get("spectra_count", pd.Series(np.ones(len(df)))).to_numpy(
            dtype=float
        )
        cmax = float(np.nanmax(counts)) if np.isfinite(counts).any() else 1.0
        sizes = 14.0 + 60.0 * (counts / cmax)  # size ~ density

        # -- 4) Figure
        plt.style.use("dark_background")
        fig = plt.figure(figsize=(20, 10))
        ax = fig.add_subplot(111, projection="mollweide")

        # Milky Way background if available
        if bg_image is not None:
            ax.imshow(
                bg_image,
                extent=[-np.pi, np.pi, -np.pi / 2, np.pi / 2],
                aspect="auto",
                zorder=0,
                alpha=0.6,
            )

        # Nuage de points par plan
        sc = ax.scatter(
            x,
            y,
            c=counts,
            s=sizes,
            cmap="viridis",
            alpha=0.9,
            edgecolors="black",
            linewidths=0.3,
            zorder=1,
        )

        # Grille & habillage
        ax.grid(True, color="w", alpha=0.2, lw=0.5)

        # RA ticks (hours): 150..-150 step 30 deg°
        lon_labels_deg = np.arange(150, -181, -30)  # display leftward
        lon_ticks = np.radians(lon_labels_deg)
        ax.set_xticks(lon_ticks)
        tick_labels = [f"{((L % 360) / 15):.0f}h" for L in (lon_labels_deg % 360)]
        ax.set_xticklabels(tick_labels)

        ax.set_yticks(np.radians(np.arange(-75, 76, 15)))
        ax.set_yticklabels([f"{d}°" for d in np.arange(-75, 76, 15)])

        ax.set_title("Sky Coverage Map (DRS plans -- RA / Dec centroids)", pad=16)

        # Colour bar
        cbar = fig.colorbar(
            sc, ax=ax, orientation="horizontal", pad=0.06, fraction=0.05
        )
        cbar.set_label("Spectra per plan")

        fig.tight_layout()

        # -- 5) Sauvegarde
        if save_path is not False:
            if not save_path:
                ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
                os.makedirs(self.paths["LOGS_DIR"], exist_ok=True)
                save_path = os.path.join(
                    self.paths["LOGS_DIR"], f"sky_coverage_{ts}.png"
                )
            try:
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
                print(f"  > Coverage map saved : {save_path}")
            except Exception as e:
                print(f"  > Warning: failed to save ({e}).")

        return fig, ax

    # ==============================================================================
    # Tool 6: Trained Model Inspector ---
    # ==============================================================================

    def _analyze_saved_model(self, model_path: str) -> None:
        """
        Inspect a saved classifier (.pkl) and display:
        - a Markdown summary (model type, best hyper-parameters, classes),
        - training columns (plus those retained after selection),
        - un graphique d'importance des features si disponible.

        Parameters
        ----------
        model_path : str
            Absolute path to the saved classifier `.pkl` file.

        Notes
        -----
        - The classifier is loaded via `SpectralClassifier.load_model()`.
        - If a `SelectFromModel` step is present in the pipeline, the displayed
        importances use the *selected feature names* (`selected_features_`)
        lorsqu'ils sont disponibles; otherwise fall back to `feature_names_used`.
        - Compatible with RandomForest / XGBoost (both expose `feature_importances_`).
        """
        try:
            # --- Load the full model (SpectralClassifier object) ------------
            clf_wrapper = SpectralClassifier.load_model(model_path)

            if not hasattr(clf_wrapper, "model_pipeline"):
                display(
                    Markdown(
                        "### Erreur\nThe loaded file does **not** contain `model_pipeline`."
                    )
                )
                return

            pipe = clf_wrapper.model_pipeline
            model = pipe.named_steps.get("clf", None)
            model_type = getattr(clf_wrapper, "model_type", type(model).__name__)

            # --- Markdown summary -----------------------------------------------------
            md = []
            md.append(f"### Model Analysis : `{os.path.basename(model_path)}`")
            md.append(f"- **Model Type :** `{model_type}`")

            # Best CV hyper-parameters (if preserved)
            best_params = getattr(clf_wrapper, "best_params_", None)
            if best_params:
                md.append("\n#### Best Hyper-parameters (GridSearchCV)")
                for k, v in best_params.items():
                    md.append(f"- **{k.replace('clf__', '')} :** `{v}`")
            else:
                md.append(
                    "\n*No search hyper-parameters recorded (or training without GridSearchCV).*"
                )

            # Known classes (training order)
            class_labels = getattr(clf_wrapper, "class_labels", None)
            if class_labels is not None:
                md.append(f"\n- **Number of classes :** `{len(class_labels)}`")
                md.append(f"- **Classes :** `{', '.join(map(str, class_labels))}`")

            # Training columns & optional selection
            used_cols = getattr(clf_wrapper, "feature_names_used", None) or []
            selected_cols = getattr(clf_wrapper, "selected_features_", None)

            md.append(f"\n- **Training columns :** `{len(used_cols)}`")
            if selected_cols:
                md.append(
                    f"- **Columns retained after selection :** `{len(selected_cols)}`"
                )
            display(Markdown("\n".join(md)))

            # --- Importance des features -------------------------------------------
            # Prefer *selected* feature names if selection exists,
            # since the classifier was trained on the transformed feature space.
            feature_names_for_plot = selected_cols or used_cols

            if hasattr(model, "feature_importances_") and feature_names_for_plot:
                importances = np.asarray(model.feature_importances_, dtype=float)

                # Safeguard alignment (lengths may differ if the artefact did not
                # preserve the selected feature names).
                if importances.shape[0] != len(feature_names_for_plot):
                    # Fallback: try with training columns anyway.
                    feature_names_for_plot = used_cols[: importances.shape[0]]

                # Sort weakest-to-strongest for a readable horizontal bar chart
                order = np.argsort(importances)
                names_sorted = np.array(feature_names_for_plot)[order]
                imps_sorted = importances[order]

                # Limit display to 40 features for readability
                top_n = min(40, len(imps_sorted))
                names_sorted = names_sorted[-top_n:]
                imps_sorted = imps_sorted[-top_n:]

                plt.style.use("dark_background")
                plt.figure(figsize=(16, max(8, top_n * 0.35)))
                plt.title("Feature Importances (after pipeline)", fontsize=16)
                plt.barh(range(top_n), imps_sorted, align="center")
                plt.yticks(range(top_n), names_sorted)
                plt.xlabel("Importance (Gini)")
                plt.tight_layout()
                plt.show()
            else:
                display(
                    Markdown(
                        "#### Feature Importances\n*No importances available for this model.*"
                    )
                )

            # --- Useful pipeline info (optional) ----------------------------------
            try:
                steps_txt = " → ".join(pipe.named_steps.keys())
                display(Markdown(f"**Trained pipeline:** `{steps_txt}`"))
            except Exception:
                pass

        except Exception as e:
            display(Markdown(f"### Error during model analysis\n**Detail:** `{e}`"))

    def interactive_model_inspector(self) -> None:
        """
        Open a mini-UI (ipywidgets) for browsing saved models and launching
        leur inspection avec `_analyze_saved_model`.

        Comportement
        ------------
        - Cherche tous les fichiers `.pkl` dans `self.paths["MODELS_DIR"]` (ou `../models/`).
        - Sorted by modification date, newest first.
        - Dropdown affichant :  " <nom_fichier.pkl> -- <taille> -- <YYYY-MM-DD HH:MM> ".
        - Bouton **Analyser** : appelle `_analyze_saved_model(model_path)`.
        - **Refresh** button: rescan the directory (useful after a new training run).

        Notes
        -----
        - Requires `ipywidgets` (Jupyter).
        - Analysis is performed by `_analyze_saved_model`, which displays the summary and
        l'importance des features quand disponible.
        """
        from ipywidgets import HBox, VBox, Dropdown, Button, HTML
        from IPython.display import display, Markdown

        display(Markdown("--- \n## Trained Model Inspector"))
        display(
            Markdown(
                "Use the drop-down to select a saved `.pkl` model. "
                "This tool will display its hyper-parameters and a chart showing the importance of each feature "
                "pour la classification."
            )
        )
        models_dir = self.paths.get("MODELS_DIR", "../models/")

        header = HTML(
            value=(
                f"<b>Models directory :</b> <code>{models_dir}</code><br>"
                "Select a model and click <b>Analyse</b>."
            )
        )

        def _scan_models() -> list[tuple[str, str]]:
            """Return a list of options (human-readable label, absolute path)."""
            if not os.path.exists(models_dir):
                return []

            entries = []
            for f in os.listdir(models_dir):
                if not f.lower().endswith(".pkl"):
                    continue
                full = os.path.join(models_dir, f)
                try:
                    stat = os.stat(full)
                    size_mb = stat.st_size / (1024 * 1024)
                    mtime = datetime.fromtimestamp(stat.st_mtime).strftime(
                        "%Y-%m-%d %H:%M"
                    )
                    label = f"{f} -- {size_mb:.2f} MB -- {mtime}"
                    entries.append((label, full))
                except Exception:
                    # If stats cannot be read, keep at least the filename.
                    entries.append((f, full))

            # Sort: newest first
            entries.sort(key=lambda x: os.path.getmtime(x[1]), reverse=True)
            return entries

        options = _scan_models()

        if not options:
            display(Markdown(f"### No `.pkl` model found in `{models_dir}`."))
            return

        dd = Dropdown(options=options, description="Model:", layout={"width": "75%"})
        btn_analyze = Button(
            description="Analyser", button_style="success", icon="search"
        )
        btn_refresh = Button(description="Refresh", button_style="", icon="refresh")

        out = widgets.Output()

        def on_analyze_clicked(_):
            out.clear_output(wait=True)
            with out:
                try:
                    self._analyze_saved_model(dd.value)
                except Exception as e:
                    display(Markdown(f"### Error during analysis\n**Detail:** `{e}`"))

        def on_refresh_clicked(_):
            new_opts = _scan_models()
            if not new_opts:
                out.clear_output(wait=True)
                with out:
                    display(Markdown(f"### No `.pkl` model found in `{models_dir}`."))
                return
            dd.options = new_opts  # preserve selection if possible

        btn_analyze.on_click(on_analyze_clicked)
        btn_refresh.on_click(on_refresh_clicked)

        ui = VBox(
            [
                header,
                HBox([dd, btn_analyze, btn_refresh]),
                out,
            ]
        )
        display(ui)

    # ==============================================================================
    # Outil 7: Spectra Comparison ---
    # ==============================================================================

    def _plot_spectra_comparison(
        self,
        file_paths: list[str],
        normalize: bool = True,
        offset: float = 0.0,
        save_path: str | None = None,
    ) -> tuple[plt.Figure, plt.Axes] | None:
        """
        Overlay and compare multiple `.fits.gz` spectra.

        Parameters
        ----------
        file_paths : list[str]
            **Relative** paths (under RAW_DATA_DIR) of the spectra to plot.
        normalize : bool, default=True
            If True, normalise each spectrum before plotting (via `SpectraPreprocessor`).
        offset : float, default=0.0
            Vertical offset added per spectrum (i * offset) for improved readability.
        save_path : str | None, default=None
            Optional output path for saving the figure (PNG).

        Returns
        -------
        (fig, ax) ou None
            Matplotlib figure and axes if at least one spectrum is plotted, else None.

        Notes
        -----
        - Files must be compressed `.fits.gz`.
        - Paths are relative to `self.paths["RAW_DATA_DIR"]`.
        - Si `self.DEFAULT_XLIM` existe, on l'utilise pour la limite en X, sinon (3800, 7000).
        """
        if not file_paths:
            print("No files provided.")
            return None

        # Deduplicate and clean the list
        uniq_paths: list[str] = []
        for p in file_paths:
            if p and p not in uniq_paths:
                uniq_paths.append(p)
        file_paths = uniq_paths

        preprocessor = SpectraPreprocessor()
        plt.style.use("dark_background")
        fig, ax = plt.subplots(figsize=(18, 9))

        plotted = 0

        for i, rel_path in enumerate(file_paths):
            full_path = os.path.join(self.paths["RAW_DATA_DIR"], rel_path)

            try:
                with (
                    gzip.open(full_path, "rb") as f_gz,
                    fits.open(f_gz, memmap=False) as hdul,
                ):
                    wavelength, flux, _ = preprocessor.load_spectrum(hdul)
            except Exception as e:
                print(f"[!] Skipped (unreadable) : {rel_path} -> {e}")
                continue

            if normalize:
                try:
                    flux = preprocessor.normalize_spectrum(flux)
                except Exception:
                    # Do not interrupt display for a normalisation failure
                    pass

            # Constant vertical offset per curve (i*offset)
            y = flux + (i * offset if offset else 0.0)

            ax.plot(wavelength, y, lw=1.0, label=os.path.basename(rel_path))
            plotted += 1

        if plotted == 0:
            plt.close(fig)
            print("No spectra could be loaded / displayed.")
            return None

        # Axes, grille, titre
        ax.set_xlabel("Longueur d'onde (Å)")
        ax.set_ylabel("Flux (normalised units)" if normalize else "Flux (adu)")
        ax.set_title("Spectra Comparison")
        ax.grid(True, alpha=0.25)

        # Default X limits (if available)
        try:
            xlim = getattr(self, "DEFAULT_XLIM", (3800.0, 7000.0))
            ax.set_xlim(xlim)
        except Exception:
            pass

        # Legend
        ax.legend(loc="upper right", frameon=True, fontsize=9)

        fig.tight_layout()

        if save_path:
            try:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
            except Exception as e:
                print(f"[!] Impossible de sauvegarder la figure ({save_path}) : {e}")

        return fig, ax

    def interactive_spectra_comparator(self) -> None:
        """
        Widget interactif pour comparer plusieurs spectres.

        Interface
        ---------
        - Multi-select list of files (under `RAW_DATA_DIR`).
        - Check-box for normalising the spectra.
        - Vertical-offset slider applied to each curve.
        - PNG export option into `logs/` with automatic (or custom) filename.

        Side effects
        --------------
        - Display the figure in the current cell.
        - If export is enabled, write a PNG into `self.paths["LOGS_DIR"]`.
        """
        display(Markdown("--- \n## Spectra Comparator"))
        display(
            Markdown(
                "Select multiple spectra (hold `Ctrl` or `Shift`) to overlay them.  Adjust the offset to better distinguish them."
            )
        )

        if not getattr(self, "available_spectra", None):
            print("No spectra found.")
            return

        # --- Widgets -------------------------------------------------------------
        files = widgets.SelectMultiple(
            options=self.available_spectra,
            rows=min(12, max(6, len(self.available_spectra))),
            description="Spectres:",
            layout=widgets.Layout(width="520px"),
        )
        normalize = widgets.Checkbox(value=True, description="Normaliser")
        offset = widgets.FloatSlider(
            value=0.5, min=0.0, max=5.0, step=0.1, description="Y offset:"
        )

        export_toggle = widgets.Checkbox(
            value=False, description="Exporter PNG (logs/)"
        )
        export_name = widgets.Text(
            value="",
            placeholder="auto (spectra_compare_YYYYMMDDTHHMMSSZ.png)",
            description="Nom:",
            layout=widgets.Layout(width="420px"),
        )

        run_btn = widgets.Button(
            description="Tracer",
            icon="line-chart",
            button_style="success",
            layout=widgets.Layout(width="150px"),
        )
        out = widgets.Output()

        # --- Callback ------------------------------------------------------------
        def on_run_clicked(_):
            out.clear_output()
            with out:
                sel = list(files.value)
                if not sel:
                    print("Select at least one spectrum from the list.")
                    return

                save_path = None
                if export_toggle.value:
                    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
                    fname = export_name.value.strip() or f"spectra_compare_{ts}.png"
                    os.makedirs(self.paths["LOGS_DIR"], exist_ok=True)
                    save_path = os.path.join(self.paths["LOGS_DIR"], fname)

                res = self._plot_spectra_comparison(
                    file_paths=sel,
                    normalize=normalize.value,
                    offset=offset.value,
                    save_path=save_path,
                )
                if res is None:
                    return

                if save_path:
                    print(f"Figure saved : {save_path}")

        run_btn.on_click(on_run_clicked)

        # --- Mise en page --------------------------------------------------------
        controls_right = widgets.VBox(
            [normalize, offset, export_toggle, export_name, run_btn]
        )
        ui = widgets.VBox(
            [
                widgets.HBox([files, controls_right]),
                out,
            ]
        )

        display(ui)

    # ==============================================================================
    # Tool 8: Augmented Spectrum Analyser (Plotly Version) ---
    # ==============================================================================

    def _display_formatted_header_for_streamlit(self, st, fits_relative_path):
        """
        Display FITS header information in Streamlit.
        """
        full_path = os.path.join(self.paths["RAW_DATA_DIR"], fits_relative_path)
        try:
            with gzip.open(full_path, "rb") as f_gz:
                with fits.open(f_gz, memmap=False) as hdul:
                    header = hdul[0].header

            md_output = f"### File header : `{os.path.basename(full_path)}`\n---\n"
            sections = {}
            current_section = "General Information"
            sections[current_section] = ""
            for key, value in header.items():
                if key == "COMMENT" and "--------" in str(value):
                    current_section = (
                        str(value).replace("COMMENT", "", 1).replace("-", "").strip()
                    )
                    if current_section not in sections:
                        sections[current_section] = ""
                elif key and key not in ["COMMENT", "HISTORY", ""]:
                    sections[current_section] += self._format_header_line(
                        header, key, key
                    )
            for title, content in sections.items():
                if content:
                    md_output += f"\n#### {title}\n{content}"
            st.markdown(md_output, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"Error while opening header : {e}")

    def _plot_spectrum_analysis_plotly(
        self, st, file_path_or_buffer, prominence, window
    ):
        preprocessor = SpectraPreprocessor()
        peak_detector = PeakDetector(prominence=prominence, window=window)

        try:
            with gzip.open(file_path_or_buffer, "rb") as f_gz:
                with fits.open(f_gz, memmap=False) as hdul:
                    wavelength, flux, invvar = preprocessor.load_spectrum(hdul)
                    header = hdul[0].header

            wavelength, flux, invvar = (
                np.asarray(d, dtype=np.float64) for d in [wavelength, flux, invvar]
            )
            flux_norm = preprocessor.normalize_spectrum(flux)

            peak_indices, properties = peak_detector.detect_peaks(wavelength, flux_norm)
            peak_wavelengths = wavelength[peak_indices]
            matched_lines = peak_detector.match_known_lines(
                peak_indices, peak_wavelengths, properties
            )

            # --- Retrieve matched peaks for green circles ---
            matched_wavelengths = [
                data[0] for data in matched_lines.values() if data is not None
            ]

            # --- Create Interactive Plotly Chart ---
            fig = go.Figure()

            # 1. Ajouter le spectre
            fig.add_trace(
                go.Scatter(
                    x=wavelength,
                    y=flux_norm,
                    mode="lines",
                    name="Normalised Spectrum",
                    line=dict(color="rgba(200, 200, 200, 0.7)", width=1),
                )
            )

            # 2. Add detected peaks
            if len(peak_indices) > 0:
                fig.add_trace(
                    go.Scatter(
                        x=peak_wavelengths,
                        y=flux_norm[peak_indices],
                        mode="markers",
                        name="Detected Peaks",
                        marker=dict(color="red", symbol="triangle-down", size=8),
                    )
                )

            # Add green circles
            if matched_wavelengths:
                matched_indices = [
                    np.abs(wavelength - wl).argmin() for wl in matched_wavelengths
                ]
                fig.add_trace(
                    go.Scatter(
                        x=wavelength[matched_indices],
                        y=flux_norm[matched_indices],
                        mode="markers",
                        name="Matched Peaks",
                        marker=dict(
                            symbol="circle-open",
                            color="lime",
                            size=15,
                            line=dict(width=2),
                        ),
                    )
                )

            # 3. Prepare shapes for target lines (vertical lines)
            shapes = []

            # Separate Balmer lines from others for toggle buttons
            _balmer_lines = {
                k: v for k, v in peak_detector.target_lines.items() if "H" in k
            }
            _other_lines = {
                k: v for k, v in peak_detector.target_lines.items() if "H" not in k
            }

            for name, wl in peak_detector.target_lines.items():
                shapes.append(
                    dict(
                        type="line",
                        xref="x",
                        yref="paper",
                        x0=wl,
                        y0=0,
                        x1=wl,
                        y1=1,
                        line=dict(color="rgba(0, 150, 255, 0.5)", width=1, dash="dash"),
                        name=name,  # Store the name for filtering
                    )
                )

            # --- Mise en forme du graphique ---
            subclass = header.get("SUBCLASS", "N/A")
            title = f"Spectrum Analysis: {header.get('DESIG', 'Unknown')} (Type: {subclass})"
            fig.update_layout(
                title=title,
                xaxis_title="Longueur d'onde (Å)",
                yaxis_title="Normalised Flux",
                template="plotly_dark",  # Dark theme
                legend=dict(yanchor="top", y=0.99, xanchor="right", x=0.99),
                shapes=shapes,  # On ajoute les lignes verticales
            )

            # --- Ajout des boutons interactifs ---
            fig.update_layout(
                updatemenus=[
                    dict(
                        type="buttons",
                        direction="right",
                        x=0.5,
                        y=1.15,
                        xanchor="center",
                        yanchor="top",
                        buttons=list(
                            [
                                dict(
                                    label="All Lines",
                                    method="relayout",
                                    args=["shapes", shapes],
                                ),
                                dict(
                                    label="Balmer Lines",
                                    method="relayout",
                                    args=[
                                        "shapes",
                                        [s for s in shapes if "H" in s["name"]],
                                    ],
                                ),
                                dict(
                                    label="Calcium Lines",
                                    method="relayout",
                                    args=[
                                        "shapes",
                                        [s for s in shapes if "Ca" in s["name"]],
                                    ],
                                ),
                                dict(
                                    label="No Lines",
                                    method="relayout",
                                    args=["shapes", []],
                                ),
                            ]
                        ),
                    )
                ]
            )

            st.plotly_chart(fig, use_container_width=True)
            return header  # On retourne le header pour l'afficher

        except Exception as e:
            st.error(f"Error during analysis du spectre : {e}")
            return None

    def app(self):
        """
        Main entry point for the Streamlit application.
        """
        import streamlit as st

        st.set_page_config(page_title="AstroSpectro Visualizer", layout="wide")
        st.title("AstroSpectro - Analysis Dashboard")

        # --- Sidebar controls ---
        st.sidebar.header("Data Source")

        source_option = st.sidebar.radio(
            "Choose a spectrum source",
            ("Select a project spectrum", "Upload a FITS file"),
        )

        file_to_process = None

        if source_option == "Select a project spectrum":
            selected_file_path = st.sidebar.selectbox(
                "Select a Spectrum", self.available_spectra
            )
            if selected_file_path:
                file_to_process = os.path.join(
                    self.paths["RAW_DATA_DIR"], selected_file_path
                )
        else:
            uploaded_file = st.sidebar.file_uploader(
                "Upload a spectrum (.fits.gz)", type=["gz"]
            )
            if uploaded_file is not None:
                file_to_process = uploaded_file

        st.sidebar.markdown("---")
        st.sidebar.header("Detection Parameters")
        prominence = st.sidebar.slider("Prominence", 0.01, 1.0, 0.2, 0.01)
        window = st.sidebar.slider("Window (Å)", 1, 50, 15, 1)

        # --- Affichage Principal ---
        if file_to_process:
            header = self._plot_spectrum_analysis_plotly(
                st, file_to_process, prominence, window
            )

            if header:
                st.markdown("---")
                with st.expander("Show Full FITS Header"):
                    self._display_header_for_streamlit(st, header)

    def _display_header_for_streamlit(self, st, header):
        """
        Display a FITS header object in Streamlit.
        """
        md_output = "#### Main Metadata\n"

        # Display key header entries
        md_output += (
            f"- **Objet :** `{header.get('OBJECT', header.get('DESIG', 'N/A'))}`\n"
        )
        md_output += f"- **Type :** `{header.get('SUBCLASS', 'N/A')}`\n"
        md_output += f"- **Date Obs :** `{header.get('DATE-OBS', 'N/A')}`\n"
        md_output += f"- **RA / Dec :** `{header.get('RA', 'N/A')}` / `{header.get('DEC', 'N/A')}`\n"
        st.markdown(md_output)

        # Affiche le header complet sous forme de dictionnaire
        st.json(dict(header))

    # ==============================================================================
    # Outil 9 : Spectral Dataset Dashboard ---
    # ==============================================================================

    def _resolve_trained_log_path(self):
        """Try to locate the trained_spectra*.csv file robustly."""
        prj = self.paths.get("PROJECT_ROOT", ".")
        cat = self.paths.get("CATALOG_DIR", os.path.join(prj, "data", "catalog"))
        data_root = os.path.join(prj, "data")

        candidates = [
            os.path.join(cat, "trained_spectra.csv"),
            os.path.join(data_root, "catalog", "trained_spectra.csv"),
            os.path.join(prj, "trained_spectra.csv"),
        ]
        for p in candidates:
            if os.path.isfile(p):
                return os.path.abspath(p)

        hits = glob.glob(
            os.path.join(prj, "**", "trained_spectra*.csv"), recursive=True
        )
        if hits:
            hits.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            return os.path.abspath(hits[0])
        return None

    def display_dataset_dashboard(self, show_charts: bool = True) -> pd.DataFrame:
        """
        Aesthetic mini-dashboard showing the dataset status:
        - KPI cards (total, already used, new candidates)
        - Top sub-classes (top 15)
        - Spectra per plan (top 20)
        - Final recap (with distinct_subclasses / distinct_plans)
        """

        # ---------- aesthetic helpers ----------
        def _fmt_fr(n: int) -> str:
            try:
                return f"{int(n):,}".replace(",", " ")
            except Exception:
                return str(n)

        def _style(
            df: pd.DataFrame, formats: dict | None = None
        ) -> pd.io.formats.style.Styler:
            st = df.style.hide(axis="index")
            if formats:
                st = st.format(formats)
            st = st.set_properties(**{"text-align": "left"})
            st = st.set_table_styles(
                [
                    {
                        "selector": "thead th",
                        "props": "background-color:#1f1f1f;color:#ddd;border:0;",
                    },
                    {"selector": "tbody td", "props": "border:0;"},
                    {
                        "selector": "tbody tr:nth-child(even)",
                        "props": "background-color:#141414;",
                    },
                    {
                        "selector": "tbody tr:nth-child(odd)",
                        "props": "background-color:#0f0f0f;",
                    },
                    {
                        "selector": "table",
                        "props": "border-collapse:separate;border-spacing:0 6px;",
                    },
                ]
            )
            return st

        # ---------- 1) volumes disponibles ----------
        display(Markdown("### Spectral Dataset Dashboard"))
        display(
            Markdown(
                "This panel gives a quick overview of your spectra collection and highlights "
                "potential opportunities for new training runs."
            )
        )

        total_available = len(getattr(self, "available_spectra", []) or [])

        # ---------- 2) retrouver et lire trained_spectra.csv (robuste) ----------
        def _first_existing(paths):
            seen, out = set(), []
            for p in paths:
                if p and p not in seen and os.path.exists(p):
                    out.append(p)
                    seen.add(p)
            return out

        candidates = _first_existing(
            [
                os.path.join(self.paths.get("PROJECT_ROOT", ""), "trained_spectra.csv"),
                os.path.join(self.paths.get("CATALOG_DIR", ""), "trained_spectra.csv"),
                os.path.join(
                    self.paths.get("PROCESSED_DIR", ""), "trained_spectra.csv"
                ),
                os.path.join(self.paths.get("LOGS_DIR", ""), "trained_spectra.csv"),
            ]
        )

        def _read_any_trained(csv_path: str) -> set[str]:
            try:
                with open(csv_path, "r", encoding="utf-8", errors="ignore") as f:
                    head = f.read(4096)
                sep = (
                    "|"
                    if ("|" in head and "," not in (head.splitlines() or [""])[0])
                    else ","
                )
                df = pd.read_csv(csv_path, sep=sep)
                for col in ("fits_name", "relative_path", "file", "filename", "path"):
                    if col in df.columns:
                        s = df[col].astype(str).str.replace("\\\\", "/", regex=True)
                        return set(s.tolist())
                return set(
                    df.astype(str).apply(lambda r: "/".join(r.values), axis=1).tolist()
                )
            except Exception:
                return set()

        trained_paths_all: set[str] = set()
        for p in candidates:
            trained_paths_all |= _read_any_trained(p)

        def _base_no_gz(path: str) -> str:
            path = (path or "").replace("\\", "/")
            return path[:-8] if path.endswith(".fits.gz") else path

        avail_bases = {_base_no_gz(p) for p in (self.available_spectra or [])}
        trained_bases = {_base_no_gz(p) for p in trained_paths_all}
        total_trained = len(avail_bases & trained_bases)
        total_new_candidates = max(0, total_available - total_trained)

        # ---------- 3) KPI cards (HTML) ----------
        cards_html = f"""
        <div style="display:grid;grid-template-columns:repeat(3,minmax(220px,1fr));gap:12px;margin:8px 0 16px;">
        <div style="background:#111;border:1px solid #2a2a2a;border-radius:12px;padding:14px 16px;">
            <div style="font-size:12px;color:#bdbdbd;">Downloaded spectra (data/raw)</div>
            <div style="font-size:28px;font-weight:800;margin-top:2px;">{_fmt_fr(total_available)}</div>
        </div>
        <div style="background:#111;border:1px solid #2a2a2a;border-radius:12px;padding:14px 16px;">
            <div style="font-size:12px;color:#bdbdbd;">Already-used spectra</div>
            <div style="font-size:28px;font-weight:800;margin-top:2px;">{_fmt_fr(total_trained)}</div>
        </div>
        <div style="background:#111;border:1px solid #2a2a2a;border-radius:12px;padding:14px 16px;">
            <div style="font-size:12px;color:#bdbdbd;">New candidates</div>
            <div style="font-size:28px;font-weight:800;margin-top:2px;">{_fmt_fr(total_new_candidates)}</div>
        </div>
        </div>
        """
        display(HTML(cards_html))

        # =====================================================================
        # 4) Top sub-classes (top 15) & Spectra per plan (top 20)
        # =====================================================================
        labels = getattr(self, "labels_catalog", {}) or {}

        # sous-classes (liste et top 15)
        subclasses: list[str] = []
        if labels:
            for rel in self.available_spectra or []:
                k = _base_no_gz(rel)
                val = labels.get(k, None)
                if val is not None and str(val) != "nan":
                    subclasses.append(str(val))

        # plans (top 20)
        plans = [
            str(p).replace("\\", "/").split("/", 1)[0]
            for p in (self.available_spectra or [])
        ]

        # ---------- 5) **RECAP AT TOP** (replaces the two small tables) ----------
        distinct_subclasses = (
            int(pd.Series(subclasses).nunique()) if (labels and subclasses) else 0
        )
        distinct_plans = int(pd.Series(plans).nunique()) if plans else 0

        summary_df = pd.DataFrame(
            {
                "metric": [
                    "total_available",
                    "total_trained",
                    "total_new_candidates",
                    "distinct_subclasses",
                    "distinct_plans",
                ],
                "# value": [
                    total_available,
                    total_trained,
                    total_new_candidates,
                    distinct_subclasses,
                    distinct_plans,
                ],
            }
        )
        display(_style(summary_df, {"# value": lambda x: _fmt_fr(x)}))

        # ---------- 6) Top sous-classes / spectres par plan ----------
        if labels and subclasses:
            vc = pd.Series(subclasses).value_counts()
            subclass_counts_df = vc.head(15).rename("# count").reset_index()
            subclass_counts_df.columns = ["subclass", "# count"]
            display(Markdown("#### Top sub-classes (top 15)"))
            display(_style(subclass_counts_df, {"# count": lambda x: _fmt_fr(x)}))
        else:
            subclass_counts_df = None

        if plans:
            vc = pd.Series(plans).value_counts()
            plan_counts_df = vc.head(20).rename("# count").reset_index()
            plan_counts_df.columns = ["plan_id", "# count"]
            display(Markdown("#### Spectra per plan (top 20)"))
            display(_style(plan_counts_df, {"# count": lambda x: _fmt_fr(x)}))

        # (optional) mini-bars
        if show_charts and subclass_counts_df is not None:
            try:
                _ = plt.figure(figsize=(7.5, 4.5))
                ax = plt.gca()
                ax.barh(
                    subclass_counts_df["subclass"][::-1],
                    subclass_counts_df["# count"][::-1],
                )
                ax.set_title("Distribution des sous-classes (top 15)")
                ax.set_xlabel("#")
                ax.grid(True, axis="x", alpha=0.25)
                plt.tight_layout()
                plt.show()
            except Exception:
                pass

        return summary_df

    # ==============================================================================
    # Tool 10: Model Interpretability Analysis (SHAP) ---
    # ==============================================================================

    # ---------------------------------------------------------------------
    # Helper : rescanner les .fits.gz disponibles
    # ---------------------------------------------------------------------

    def refresh_available_spectra(self) -> int:
        """Refresh self.available_spectra by rescanning data/raw/."""
        self.available_spectra = self._scan_for_spectra()
        return len(self.available_spectra)

    # ---------------------------------------------------------------------
    # Helper: prepare X for the model (align columns + pipeline steps)
    # ---------------------------------------------------------------------

    def _prepare_for_model(self, df: pd.DataFrame, clf_wrapper):
        """
        Align a feature DataFrame with the model's training columns,
        then apply *upstream* pipeline steps (imputer / scaler / selector).
        Return (Xt, feature_names_for_plot).
        """
        pipe = clf_wrapper.model_pipeline

        # Expected columns
        used_cols = list(getattr(clf_wrapper, "feature_names_used", list(df.columns)))
        X = df.copy()
        for c in used_cols:
            if c not in X.columns:
                X[c] = np.nan
        X = X.reindex(columns=used_cols)

        # Imputer -> Scaler (if present)
        imp = getattr(pipe, "named_steps", {}).get("imputer")
        scl = getattr(pipe, "named_steps", {}).get("scaler")

        Xt = X.values
        if imp is not None:
            Xt = imp.transform(X)
        if scl is not None:
            Xt = scl.transform(Xt)

        # Feature selection (if present)
        sel = getattr(pipe, "named_steps", {}).get("feature_selector")
        if sel is not None:
            Xt = sel.transform(Xt)
            feature_names_for_plot = list(
                getattr(clf_wrapper, "selected_features_", [])
                or used_cols[: Xt.shape[1]]
            )
        else:
            feature_names_for_plot = used_cols

        # Length safety check
        if len(feature_names_for_plot) != Xt.shape[1]:
            feature_names_for_plot = used_cols[: Xt.shape[1]]

        return Xt, feature_names_for_plot

    # ---------------------------------------------------------------------
    # Helper: build a feature sample ready for the model
    # ---------------------------------------------------------------------

    def _get_features_sample(
        self, clf, sample_n: int, source: str = "auto"
    ) -> pd.DataFrame:
        """
        Return a feature-only DataFrame X_df ready for model alignment.
        source ∈ {"auto","features_csv","raw_fits"}.
        - 'auto': try the latest features_*.csv first, then fall back to FITS.
        """

        source = (source or "auto").lower()

        # 1) Attempt via latest features CSV
        if source in ("auto", "features_csv"):
            latest = latest_file(self.paths["PROCESSED_DIR"], "features_*.csv")
            if latest and os.path.exists(latest):
                df = pd.read_csv(latest)
                # Remove target columns if present
                for col in ("main_class", "label", "target", "y"):
                    if col in df.columns:
                        df = df.drop(columns=[col])
                # Keep only columns expected by the model (if known)
                expected = list(getattr(clf, "feature_names_used", list(df.columns)))
                for c in expected:
                    if c not in df.columns:
                        df[c] = 0.0
                df = df[expected]
                if len(df) > sample_n:
                    df = df.sample(sample_n, random_state=42)
                if not df.empty:
                    return df
            if source == "features_csv":
                raise RuntimeError("No usable features_*.csv found in data/processed/.")

        # 2) Fallback: direct extraction from FITS
        if not getattr(self, "available_spectra", None):
            self.refresh_available_spectra()
        files = list(self.available_spectra)
        if not files:
            raise RuntimeError("No spectra found in data/raw/.")

        rng = np.random.default_rng(42)
        if len(files) > sample_n:
            files = list(rng.choice(files, size=sample_n, replace=False))

        pre, fe, pk = SpectraPreprocessor(), FeatureEngineer(), PeakDetector()
        rows, feature_names, errors = [], None, []

        for rel in files:
            try:
                full = os.path.join(self.paths["RAW_DATA_DIR"], rel)
                with gzip.open(full, "rb") as gz:
                    with fits.open(gz, memmap=False) as hdul:
                        wl, fl, inv = pre.load_spectrum(hdul)
                wl = np.asarray(wl, float)
                fl = pre.normalize_spectrum(np.asarray(fl, float))
                inv = np.asarray(inv, float)

                peaks, props = pk.detect_peaks(wl, fl)
                matched = pk.match_known_lines(peaks, wl[peaks], props)
                vec = fe.extract_features(matched, wl, fl, inv)

                if feature_names is None:
                    feature_names = list(fe.feature_names)
                rows.append(vec)
            except Exception as e:
                errors.append(str(e))
                continue

        if not rows:
            msg = " | ".join(errors[:3]) if errors else "aucun fichier valide"
            raise RuntimeError(f"Failed to build sample (FITS). Details: {msg}")

        return pd.DataFrame(np.vstack(rows), columns=feature_names)

    # ---------------------------------------------------------------------
    # SHAP : fabrique d'explainers (modulaire)
    # ---------------------------------------------------------------------
    def _build_shap_explainer(self, *, pipe, model, X_df, mode: str):
        """
        Return (explainer, X_background) according to the mode.
        mode ∈ {"auto","tree","linear","kernel","permutation"}.
        """
        import shap

        n = len(X_df)
        # Compact background for large samples
        try:
            X_bg = (
                shap.kmeans(X_df, k=min(50, max(10, n // 200)))
                if n > 1000
                else X_df.values
            )
        except Exception:
            X_bg = X_df.sample(n=min(1000, n), random_state=42).values

        model_name = type(model).__name__.lower()
        is_tree = any(
            k in model_name
            for k in ["xgb", "randomforest", "gradientboost", "extra", "gbc", "gbm"]
        )
        is_lin = any(
            k in model_name
            for k in ["logistic", "linear", "sgd", "ridge", "lasso", "elastic"]
        )

        if mode == "auto":
            mode = "tree" if is_tree else ("linear" if is_lin else "kernel")

        if mode == "tree":
            explainer = shap.TreeExplainer(
                model, data=X_bg, feature_perturbation="interventional"
            )
        elif mode == "linear":
            try:
                explainer = shap.LinearExplainer(model, X_bg)
            except Exception:
                explainer = shap.Explainer(model, X_bg)
        elif mode == "kernel":

            def model_predict(X):
                return (
                    pipe.predict_proba(X)
                    if hasattr(pipe, "predict_proba")
                    else pipe.predict(X)
                )

            explainer = shap.KernelExplainer(model_predict, X_bg)
        elif mode == "permutation":
            explainer = shap.Explainer(model, X_bg, algorithm="permutation")
        else:
            raise ValueError(f"Unknown SHAP mode: {mode}")

        return explainer, X_bg

    # ---------------------------------------------------------------------
    # Run an end-to-end SHAP analysis on a sample
    # ---------------------------------------------------------------------

    def _run_shap_analysis(
        self,
        model_path: str,
        sample_n: int = 500,
        *,
        explainer_mode: str = "auto",
        data_source: str = "auto",
    ) -> "pd.DataFrame | None":
        """
        Load a model, compute a feature sample, align with the pipeline,
        and produce a table of SHAP importances (mean |value| per feature).
        Also export CSV / LaTeX into logs/shap and cache objects for plots.

        Parameters
        ----------
        model_path : str
            Absolute path of the .pkl (saved SpectralClassifier).
        sample_n : int, default=500
            Spectrum sample size (capped by the number available).

        Returns
        -------
        pd.DataFrame | None
            Mean absolute SHAP importance table, sorted descending,
            or None on failure.
        """
        import shap

        # 0) Load the model
        clf = SpectralClassifier.load_model(model_path)
        pipe = clf.model_pipeline
        model_core = getattr(pipe, "named_steps", {}).get("clf", pipe)

        # 1) Sample X_df
        try:
            X_df = self._get_features_sample(
                clf, sample_n=int(sample_n), source=data_source
            )
        except Exception as e:
            print(f"[!] Failed to build sample ({e}).")
            return None

        # 2) Alignement pipeline (imputer/scaler/selector)
        Xt, names_for_plot = self._prepare_for_model(X_df, clf)

        # 3) SHAP explainer - simplified version
        # TreeExplainer for XGBoost (uses already-transformed data)

        # Extract the raw XGBoost model
        try:
            xgb_model = pipe.named_steps["clf"]
            if hasattr(
                xgb_model, "base_estimator"
            ):  # Si wrapper (ThresholdTuned, etc.)
                xgb_model = xgb_model.base_estimator
            if hasattr(xgb_model, "calibrated_classifiers_"):  # Si calibration
                xgb_model = xgb_model.calibrated_classifiers_[0].estimator
        except Exception as e:
            print(f"[!] Impossible d'extraire XGBoost: {e}")
            xgb_model = model_core

        # TreeExplainer (fast and stable for XGBoost)
        try:
            explainer = shap.TreeExplainer(xgb_model)
            print(f"✓ TreeExplainer created for {type(xgb_model).__name__}")
        except Exception as e:
            print(f"[!] Erreur TreeExplainer: {e}")
            return None

        # 4) Compute SHAP values on ALREADY-TRANSFORMED data (Xt)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=RuntimeWarning)
            sv = explainer(Xt)

        # 4) SHAP values (uniform API: direct call)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=RuntimeWarning)
            sv = explainer(Xt)

        # Cache the full explanation for plots
        self._last_shap_explanation = sv

        # 5) Mean |value| importances
        vals = getattr(sv, "values", sv)
        vals = np.asarray(vals)
        if vals.ndim == 3:  # multi-classes
            vals = np.mean(np.abs(vals), axis=1)
        else:
            vals = np.abs(vals)

        mean_abs = vals.mean(axis=0).astype(float)
        if len(names_for_plot) != mean_abs.shape[0]:
            names_for_plot = names_for_plot[: mean_abs.shape[0]]

        df_out = pd.DataFrame(
            {"feature": names_for_plot, "mean_abs_shap": mean_abs}
        ).sort_values("mean_abs_shap", ascending=False, ignore_index=True)

        # Cache for plotting helpers
        self._last_shap_importances = df_out
        return df_out

    # ---------------------------------------------------------------------
    # Widget notebook
    # ---------------------------------------------------------------------

    def interactive_shap_explainer(self) -> None:
        """
        Create a small **notebook widget** for selecting a saved model
        (.pkl) and launching a **SHAP** interpretability analysis.

        Le workflow :
        1) scan du dossier `data/models/` (ou `self.paths["MODELS_DIR"]`)
        2) select a model via a Dropdown
        3) set the number of samples (for SHAP charts)
        4) clic sur "Analyser" -> appelle `self._run_shap_analysis(...)`
            et affiche les graphes + un tableau d'importances.

        Notes
        -----
        - Uses the private method `_run_shap_analysis` implemented above.
        - Charts are generated with Matplotlib + SHAP (beeswarm).
        - An importances CSV can also be exported to `logs/`.
        """
        import ipywidgets as widgets
        from IPython.display import display, Markdown
        from datetime import datetime, timezone

        display(Markdown("### Model Interpretability Analysis Tool (SHAP)"))
        display(Markdown("Select a trained model and launch the analysis."))

        # Models
        models_dir = self.paths.get("MODELS_DIR", "../data/models/")
        saved = []
        if os.path.isdir(models_dir):
            saved = sorted(
                [
                    os.path.join(models_dir, f)
                    for f in os.listdir(models_dir)
                    if f.endswith(".pkl")
                ],
                key=os.path.getmtime,
                reverse=True,
            )
        if not saved:
            print("No trained model (.pkl) found in data/models/.")
            return

        dd_model = widgets.Dropdown(
            options=saved, description="Model:", layout={"width": "700px"}
        )
        sample_slider = widgets.IntSlider(
            value=500,
            min=100,
            max=2000,
            step=50,
            description="Samples:",
            continuous_update=False,
        )
        topk_slider = widgets.IntSlider(
            value=30,
            min=5,
            max=100,
            step=5,
            description="Top N :",
            continuous_update=False,
        )
        mode_dd = widgets.Dropdown(
            options=[
                ("Auto", "auto"),
                ("Tree", "tree"),
                ("Linear", "linear"),
                ("Kernel", "kernel"),
                ("Permutation", "permutation"),
            ],
            value="auto",
            description="Explainer :",
            layout={"width": "200px"},
        )
        source_dd = widgets.Dropdown(
            options=[
                ("Auto (CSV then FITS)", "auto"),
                ("Features CSV only", "features_csv"),
                ("FITS (live extraction)", "raw_fits"),
            ],
            value="auto",
            description="Data source:",
            layout={"width": "240px"},
        )
        run_btn = widgets.Button(
            description="Analyser", button_style="success", icon="bar-chart"
        )
        out = widgets.Output()

        def _on_click(_):
            out.clear_output()
            with out:
                shap_df = self._run_shap_analysis(
                    model_path=dd_model.value,
                    sample_n=int(sample_slider.value),
                    explainer_mode=mode_dd.value,
                    data_source=source_dd.value,
                )
                if isinstance(shap_df, pd.DataFrame) and not shap_df.empty:
                    display(Markdown("**Importances SHAP (moyenne |valeur|) :**"))
                    display(shap_df.head(int(topk_slider.value)))
                    # export CSV/LaTeX
                    try:
                        ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
                        out_dir = os.path.join(self.paths["LOGS_DIR"], "shap")
                        os.makedirs(out_dir, exist_ok=True)
                        shap_df.to_csv(
                            os.path.join(out_dir, f"shap_importances_{ts}.csv"),
                            index=False,
                        )
                        shap_df.to_latex(
                            os.path.join(out_dir, f"shap_importances_{ts}.tex"),
                            index=False,
                            float_format="%.6f",
                        )
                    except Exception as e:
                        display(Markdown(f"> Export failed : `{e}`"))

        run_btn.on_click(_on_click)
        display(
            widgets.VBox(
                [
                    widgets.HBox(
                        [
                            dd_model,
                            sample_slider,
                            topk_slider,
                            mode_dd,
                            source_dd,
                            run_btn,
                        ]
                    ),
                    out,
                ]
            )
        )

    # ---------------------------------------------------------------------
    # Reusable plotting helpers for other cells
    # ---------------------------------------------------------------------

    def plot_shap_summary_bar(
        self,
        top_n: int = 20,
        save_path: str | None = None,
    ) -> tuple[plt.Figure, plt.Axes] | None:
        """
        Plot a **bar chart** of SHAP importances (mean absolute value).

        Prerequisites
        -----------
        - Call `_run_shap_analysis(...)` (or the widget `interactive_shap_explainer()`) first;
        this stores the latest result in `self._last_shap_importances`.

        Parameters
        ----------
        top_n : int, default=20
            Maximum number of features to display (most important).
        save_path : str | None, default=None
            PNG export path.  If None, the figure is not saved.
            Parent directories are created as needed.

        Returns
        -------
        (matplotlib.figure.Figure, matplotlib.axes.Axes) | None
            The figure and its axes if a SHAP result is available, else None.

        Notes
        -----
        - The chart is sorted **most important to least** (top to bottom), → bas),
        with a `mean(|SHAP value|)` scale.
        - Dark theme for consistency with other plots.
        """
        # Retrieve the last importance table computed by _run_shap_analysis
        df = getattr(self, "_last_shap_importances", None)
        if df is None or df.empty:
            print("No cached SHAP result.  Run interactive_shap_explainer() first.")
            return None
        top = df.head(int(top_n))
        plt.style.use("dark_background")
        fig, ax = plt.subplots(figsize=(12, max(5, 0.40 * len(top))))
        y_labels = top["feature"].astype(str).iloc[::-1]
        x_vals = top["mean_abs_shap"].astype(float).iloc[::-1]
        ax.barh(y_labels, x_vals)
        ax.set_xlabel("mean(|SHAP value|)")
        ax.set_title("SHAP -- importances (top)")
        ax.grid(axis="x", alpha=0.30, linestyle="--")
        fig.tight_layout()
        if save_path:
            try:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
            except Exception as e:
                print(f"[!] Impossible de sauvegarder la figure ({save_path}) : {e}")
        return fig, ax

    def plot_shap_beeswarm(
        self,
        max_display: int = 15,
        save_path: str | None = None,
    ) -> plt.Figure | None:
        """
        Plot a **SHAP beeswarm** (per-feature contribution distribution),
        with **fallback** handling if the beeswarm is not available.

        Prerequisites
        -----------
        - Call `_run_shap_analysis(...)` (or the widget `interactive_shap_explainer()`) first;
        this stores the latest explanation in `self._last_shap_explanation`.

        Parameters
        ----------
        max_display : int, default=15
            Maximum number of features to display.
        save_path : str | None, default=None
            PNG export path.  If None, the figure is not saved.
            Parent directories are created as needed.

        Returns
        -------
        (matplotlib.figure.Figure, matplotlib.axes.Axes) | None
            The figure and its axes if a SHAP explanation is available, else None.

        Notes
        -----
        - If `shap.plots.beeswarm(...)` fails (e.g. unsupported shapes),
        try `shap.plots.bar(...)`.  If that also fails, fall back to the
        sur notre bar chart custom (`plot_shap_summary_bar`).
        - Dark theme and tight layout by default.
        """
        sv = getattr(self, "_last_shap_explanation", None)
        if sv is None:
            print(
                "No cached SHAP explanation.  Run interactive_shap_explainer() first."
            )
            return None
        plt.style.use("dark_background")
        try:
            shap.plots.beeswarm(sv, max_display=int(max_display), show=False)
        except Exception as e1:
            try:
                shap.plots.bar(sv, max_display=int(max_display), show=False)
            except Exception as e2:
                print(
                    f"[!] Beeswarm indisponible ({e1}). Fallback bar indisponible ({e2})."
                )
                return self.plot_shap_summary_bar(
                    top_n=int(max_display), save_path=save_path
                )
        fig = plt.gcf()
        fig.tight_layout()
        if save_path:
            try:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
            except Exception as e:
                print(f"[!] Impossible de sauvegarder la figure ({save_path}) : {e}")
        return fig, plt.gca()

    # ==============================================================================
    # Tool 11: Sub-class Distribution Analysis ---
    # ==============================================================================

    def plot_subclass_distribution(
        self,
        top_n: int = 20,
        normalize: bool = False,
        save_path: Optional[str] = None,
    ) -> Optional[tuple[plt.Figure, plt.Axes]]:
        """
        Display (and optionally save) the **spectral sub-class distribution**
        from the temporary catalogue `master_catalog_temp.csv`.

        Parameters
        ----------
        top_n : int, default=20
            Maximum number of sub-classes to display (most frequent).
        normalize : bool, default=False
            If True, display relative frequencies (percentages); otherwise raw counts.
        save_path : str | None, default=None
            PNG export path.  If None, the figure is not saved.

        Returns
        -------
        (Figure, Axes) ou None
            Return the Matplotlib figure and axes on success.
            Return None if the catalogue is not found or empty.

        Notes
        -----
        - Look for a `'subclass'` column; fall back to `'label'`.
        - Sort by frequency, descending.
        """
        import os
        import pandas as pd
        import matplotlib.pyplot as plt
        from IPython.display import display, Markdown

        # 1) Charger le catalogue temporaire
        cat_path = os.path.join(self.paths["CATALOG_DIR"], "master_catalog_temp.csv")
        if not os.path.exists(cat_path):
            display(
                Markdown("> Temporary catalogue not found : `master_catalog_temp.csv`.")
            )
            return None

        try:
            df = pd.read_csv(cat_path, sep="|")
        except Exception as e:
            display(Markdown(f"> Catalogue read error : `{e}`"))
            return None

        if df.empty:
            display(Markdown("> Empty catalogue."))
            return None

        # 2) Identifier la colonne des sous-classes
        label_col = None
        for cand in ("subclass", "label"):
            if cand in df.columns:
                label_col = cand
                break

        if label_col is None:
            display(Markdown("> No `subclass` or `label` column found."))
            return None

        # 3) Count & select top N
        counts = df[label_col].astype(str).value_counts(dropna=False)
        if counts.empty:
            display(Markdown("> No usable labels in the catalogue."))
            return None

        counts = counts.iloc[:top_n].copy()

        # 4) Optional normalisation
        if normalize:
            total = counts.sum()
            values = (counts / total) * 100.0
            ylabel = "Frequency (%)"
        else:
            values = counts
            ylabel = "Number of examples"

        # 5) Plot
        plt.style.use("dark_background")
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.bar(counts.index.astype(str), values.values)
        ax.set_title("Spectral Sub-class Distribution")
        ax.set_xlabel("Sous-classe")
        ax.set_ylabel(ylabel)
        ax.grid(axis="y", alpha=0.3, linestyle="--")
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right")
        fig.tight_layout()

        # 6) Sauvegarde optionnelle
        if save_path:
            try:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
            except Exception as e:
                display(Markdown(f"> Impossible de sauvegarder la figure : `{e}`"))

        return fig, ax

    # ==============================================================================
    # Tool 12: Spectrum Normalisation Comparator
    # ==============================================================================

    def plot_normalization_comparison(
        self,
        sample_paths: list[str] | None = None,
        *,
        n_samples: int = 2,
        random_state: int | None = None,
        save_path: str | None = None,
    ) -> tuple[plt.Figure, tuple[plt.Axes, plt.Axes]] | None:
        """
        Visually compare the effect of **normalisation** on two spectra.

        Two sub-plots are drawn:
        1) raw flux ("Before normalisation")
        2) normalised flux ("After normalisation")

        If `save_path` is omitted, the figure is saved by default to:
            <PROJECT_ROOT>/website/static/img/avant_apres_normalisation.png

        Parameters
        ----------
        sample_paths : list[str] | None
            **Relative** paths (under `RAW_DATA_DIR`) of the spectra to use.
            If None, randomly sample `n_samples` files from
            `self.available_spectra`.
        n_samples : int, default 2
            Number of spectra to compare when `sample_paths` is not provided.
        random_state : int | None
            Random seed for reproducible sampling.
        save_path : str | None
            PNG export path.  If None, use default path.  If "", no export.

        Returns
        -------
        (fig, (ax1, ax2)) | None
            The Matplotlib figure and axes if at least 2 spectra are available,
            else `None`.

        Notes
        -----
        - Paths must be relative to `RAW_DATA_DIR`
        and point to compressed `.fits.gz` files.
        - Use `SpectraPreprocessor.normalize_spectrum` for normalisation.
        """
        import random

        # --- Preliminary checks ---------------------------------------------
        if not getattr(self, "available_spectra", []):
            print("Please run the setup cell first to initialise `visualizer`.")
            return None

        # File selection (2 by default)
        if sample_paths is None:
            pool = list(self.available_spectra)
            if len(pool) < 2:
                print(
                    "Not enough spectra available to generate the figure (at least 2 required)."
                )
                return None
            if random_state is not None:
                random.seed(random_state)
            sample_paths = random.sample(pool, k=max(2, min(n_samples, len(pool))))
        else:
            # Clean and ensure at least 2 distinct items
            sample_paths = [p for p in (sample_paths or []) if p]
            sample_paths = list(dict.fromkeys(sample_paths))  # Deduplicate
            if len(sample_paths) < 2:
                print("Provide at least two relative paths to `.fits.gz` spectra.")
                return None

        # --- Loading & preprocessing -------------------------------------------
        preprocessor = SpectraPreprocessor()
        spectra_data: list[dict] = []

        for rel in sample_paths[:2]:  # on n'affiche que 2 courbes for readability
            full_path = os.path.join(self.paths["RAW_DATA_DIR"], rel)
            try:
                with gzip.open(full_path, "rb") as f_gz:
                    with fits.open(f_gz, memmap=False) as hdul:
                        wavelength, flux, _ = preprocessor.load_spectrum(hdul)
                flux_norm = preprocessor.normalize_spectrum(np.asarray(flux, float))
                spectra_data.append(
                    {
                        "wavelength": np.asarray(wavelength, float),
                        "flux_raw": np.asarray(flux, float),
                        "flux_norm": np.asarray(flux_norm, float),
                        "name": os.path.basename(rel),
                    }
                )
            except Exception as e:
                print(f"[!] Lecture impossible pour {rel} : {e}")

        if len(spectra_data) < 2:
            print("Impossible de charger deux spectres valides.")
            return None

        # --- Figure ---------------------------------------------------------------
        plt.style.use("dark_background")
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 6), sharex=True)

        # Avant normalisation (flux bruts)
        ax1.plot(
            spectra_data[0]["wavelength"],
            spectra_data[0]["flux_raw"],
            label="Spectrum 1 (Raw)",
            alpha=0.9,
        )
        ax1.plot(
            spectra_data[1]["wavelength"],
            spectra_data[1]["flux_raw"],
            label="Spectrum 2 (Raw)",
            alpha=0.9,
        )
        ax1.set_title("Before Normalisation", fontsize=16)
        ax1.set_ylabel("Flux (arbitrary units)")
        ax1.grid(True, linestyle=":", alpha=0.5)
        ax1.legend()

        # After normalisation (normalised flux)
        ax2.plot(
            spectra_data[0]["wavelength"],
            spectra_data[0]["flux_norm"],
            label="Spectrum 1 (Normalised)",
        )
        ax2.plot(
            spectra_data[1]["wavelength"],
            spectra_data[1]["flux_norm"],
            label="Spectrum 2 (Normalised)",
        )
        ax2.set_title("After Normalisation", fontsize=16)
        ax2.set_xlabel("Longueur d'onde (Å)")
        ax2.set_ylabel("Normalised Flux")
        ax2.grid(True, linestyle=":", alpha=0.5)
        ax2.legend()

        fig.suptitle("Impact of Normalisation on Spectra", fontsize=20, y=1.02)
        fig.tight_layout(rect=(0, 0, 1, 0.98))

        # --- Sauvegarde -----------------------------------------------------------
        if save_path is None:
            # Default path in website/static/img/
            save_path = os.path.join(
                self.paths.get("PROJECT_ROOT", "."),
                "website",
                "static",
                "img",
                "avant_apres_normalisation.png",
            )

        if save_path:
            try:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
                print(f"Normalisation figure saved to : {save_path}")
            except Exception as e:
                print(f"[!] Save failed ({e})")

        return fig, (ax1, ax2)

    # ==============================================================================
    # Tool 13: Feature Explorer and Analysis
    # ==============================================================================

    def feature_explorer(
        self,
        pattern: str | None = None,
        *,
        save_dir: str | None = None,
        max_hists: int = 24,
        corr_top_n: int = 30,
        rf_estimators: int = 300,
        random_state: int = 42,
        do_permutation: bool = True,
    ) -> dict | None:
        """
        Load and concatenate feature CSV files, perform an
        exploratory analysis (EDA), plot distributions and correlations, then train
        a baseline model (RandomForest) to estimate feature importances.

        Parameters
        ----------
        pattern : str | None
            Glob pattern for feature files (e.g. ".../data/processed/features_*.csv").
            Si None, essaie automatiquement "<PROJECT_ROOT>/data/processed/features_*.csv".
        save_dir : str | None
            Export directory (PNGs + CSV).  If None => "<LOGS_DIR>/features".
        max_hists : int, default=24
            Maximum histograms to plot (highest-variance numeric columns).
        corr_top_n : int, default=30
            Number of columns (numeric, highest variance) for the correlation heat-map.
        rf_estimators : int, default=300
            n_estimators pour le RandomForestClassifier baseline.
        random_state : int, default=42
            Random seed for reproducibility (sampling / RF / split).
        do_permutation : bool, default=True
            Calcule aussi la permutation importance (validation split).

        Returns
        -------
        dict | None
            A summary dictionary:
            {
            "df": concatenated DataFrame,
            "numeric_cols": [...],
            "label_col": "subclass" | "label" | None,
            "rf_importances": DataFrame | None,
            "perm_importances": DataFrame | None,
            "save_dir": chemin d'export
            }
            Return None if no files are found.

        Notes
        -----
        - Automatically detect the CSV separator via `sep=None, engine='python'`.
        - If no label column is found ('subclass' or 'label'), baseline importances
        are skipped but the EDA is still produced.
        - Charts saved as PNG (dark theme) when `save_dir` is defined and valid.
        """
        import os
        from textwrap import dedent

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from IPython.display import display, Markdown

        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.impute import SimpleImputer
        from sklearn.pipeline import Pipeline
        from sklearn.inspection import permutation_importance

        # ---------- Path resolution ----------
        if pattern is None:
            proj = self.paths.get("PROJECT_ROOT", ".")
            pattern = os.path.join(proj, "data", "processed", "features_*.csv")
        if save_dir is None:
            save_dir = os.path.join(self.paths.get("LOGS_DIR", "./logs"), "features")
        os.makedirs(save_dir, exist_ok=True)

        # ---------- Read & concatenate ----------
        files = sorted(glob.glob(pattern))
        if not files:
            display(
                Markdown(f"> **No files** found matching the pattern : `{pattern}`")
            )
            return None

        dfs = []
        for p in files:
            try:
                df_i = pd.read_csv(p, sep=None, engine="python")
                dfs.append(df_i)
            except Exception as e:
                print(f"[!] Lecture impossible: {p} ({e})")

        if not dfs:
            display(Markdown("> No valid CSV could be loaded."))
            return None

        df = pd.concat(dfs, ignore_index=True)
        display(
            Markdown(f"**{len(files)}** file(s) loaded -- total rows : **{len(df):,}**")
        )

        # ---------- Quick EDA ----------
        # Type & missing-value summary
        nunique = df.nunique(dropna=True)
        missing = df.isna().sum()
        missing_rate = (missing / len(df)).round(4)

        info_rows = [
            ("rows", len(df)),
            ("cols", df.shape[1]),
            ("numeric_cols", int(df.select_dtypes(include=[np.number]).shape[1])),
            ("non_numeric_cols", int(df.select_dtypes(exclude=[np.number]).shape[1])),
            ("missing_total", int(missing.sum())),
            ("missing_rate_overall", float(missing_rate.mean())),
        ]
        summary_df = pd.DataFrame(info_rows, columns=["metric", "value"])
        display(Markdown("### Global summary"))
        display(summary_df)

        # Top missing
        miss_tbl = (
            pd.DataFrame(
                {"missing": missing, "missing_rate": missing_rate, "nunique": nunique}
            )
            .sort_values("missing_rate", ascending=False)
            .head(30)
        )
        display(Markdown("### Most-missing columns"))
        display(miss_tbl)

        # Numeric describe
        num_df = df.select_dtypes(include=[np.number])
        if not num_df.empty:
            display(Markdown("### Descriptive statistics (numeric)"))
            display(num_df.describe().T.head(30))

        # ---------- Figures: distributions ----------
        if not num_df.empty and max_hists > 0:
            # Columns sorted by variance (desc)
            var = num_df.var(numeric_only=True).sort_values(ascending=False)
            cols = list(var.index[:max_hists])

            plt.style.use("dark_background")
            n = len(cols)
            ncols = 4
            nrows = int(np.ceil(n / ncols))
            fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 2.8 * nrows))
            axes = np.atleast_1d(axes).ravel()

            for i, c in enumerate(cols):
                ax = axes[i]
                ax.hist(num_df[c].dropna(), bins=30)
                ax.set_title(c, fontsize=10)
                ax.grid(True, linestyle=":", alpha=0.3)

            for j in range(i + 1, len(axes)):
                axes[j].axis("off")

            fig.suptitle("Distributions (top variance)", fontsize=14)
            fig.tight_layout()
            try:
                fig.savefig(
                    os.path.join(save_dir, "feature_hists.png"),
                    dpi=140,
                    bbox_inches="tight",
                )
            except Exception:
                pass
            plt.show()

        # ---------- Figure: correlation (top variance) ----------
        corr_cols = list(num_df.var().sort_values(ascending=False).index[:corr_top_n])
        if len(corr_cols) >= 2:
            # Replace NaN with median for correlation; skip constant columns
            Xcorr = num_df[corr_cols].copy()
            for c in Xcorr.columns:
                if Xcorr[c].isna().any():
                    med = Xcorr[c].median()
                    Xcorr[c] = Xcorr[c].fillna(med)
            # Remove constant columns (zero variance)
            Xcorr = Xcorr.loc[:, Xcorr.var() > 0]
            if Xcorr.shape[1] >= 2:
                C = Xcorr.corr().values

                plt.style.use("dark_background")
                fig, ax = plt.subplots(figsize=(max(8, 0.35 * Xcorr.shape[1]), 8))
                im = ax.imshow(C, vmin=-1, vmax=1, cmap="coolwarm")
                ax.set_title("Correlations (top variance)", fontsize=14)
                ticks = range(Xcorr.shape[1])
                ax.set_xticks(ticks)
                ax.set_yticks(ticks)
                labels = list(Xcorr.columns)
                # Avoid saturating labels
                if len(labels) <= 30:
                    ax.set_xticklabels(labels, rotation=90, fontsize=8)
                    ax.set_yticklabels(labels, fontsize=8)
                else:
                    ax.set_xticklabels([])
                    ax.set_yticklabels([])
                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
                fig.tight_layout()
                try:
                    fig.savefig(
                        os.path.join(save_dir, "corr_heatmap.png"),
                        dpi=150,
                        bbox_inches="tight",
                    )
                except Exception:
                    pass
                plt.show()

        # ---------- Baseline importances (RF) ----------
        label_col = None
        for cand in ("subclass", "label"):
            if cand in df.columns:
                label_col = cand
                break

        rf_df = None
        perm_df = None

        if label_col is None:
            display(
                Markdown(
                    "> No label column found (`subclass`/`label`).  Importances skipped."
                )
            )
        else:
            # Keep only numeric columns for the simple baseline
            X_num = df.select_dtypes(include=[np.number]).copy()
            y = df[label_col].astype(str)

            # Remove constant columns
            X_num = X_num.loc[:, X_num.var() > 0]
            if X_num.empty:
                display(Markdown("> No usable numeric features (zero variance)."))
            else:
                # --- Train / test split (robust to very rare classes) ---
                # y is the label Series/array; X_num the numeric features already prepared
                class_counts = pd.Series(y).value_counts(dropna=False)

                if (class_counts < 2).any():
                    rare = class_counts[class_counts < 2]

                    # Message d'info dans le notebook
                    try:
                        display(
                            Markdown(
                                "> **Unstratified** split: some classes are too rare "
                                "("
                                + ", ".join([f"`{k}`={v}" for k, v in rare.items()])
                                + "). "
                                "Add examples or merge these classes to re-enable stratified splitting."
                            )
                        )
                    except Exception:
                        # si display/Markdown pas dispo, on log simplement
                        print(
                            "[Info] Unstratified split (too-rare classes) :",
                            dict(rare),
                        )

                    stratify_vec = None  # fallback : pas de stratification
                else:
                    stratify_vec = y  # OK pour la stratification

                # (25 % test by default; adjust as needed)
                X_train, X_test, y_train, y_test = train_test_split(
                    X_num,
                    y,
                    test_size=0.25,
                    random_state=random_state,
                    stratify=stratify_vec,
                )

                pipe = Pipeline(
                    [
                        ("imputer", SimpleImputer(strategy="median")),
                        (
                            "rf",
                            RandomForestClassifier(
                                n_estimators=rf_estimators,
                                random_state=random_state,
                                n_jobs=-1,
                            ),
                        ),
                    ]
                )
                pipe.fit(X_train, y_train)
                rf = pipe.named_steps["rf"]

                # Importances Gini
                imp = pd.Series(
                    rf.feature_importances_, index=X_num.columns
                ).sort_values(ascending=False)
                rf_df = imp.reset_index().rename(
                    columns={"index": "feature", 0: "importance"}
                )
                display(Markdown("### Importances (RandomForest, impurity-based)"))
                display(rf_df.head(30))

                # Export CSV
                try:
                    rf_df.to_csv(
                        os.path.join(save_dir, "rf_importances.csv"), index=False
                    )
                except Exception:
                    pass

                # Bar plot
                self._plot_feature_importances_bar(
                    rf_df,
                    top_n=25,
                    save_path=os.path.join(save_dir, "rf_importances_top25.png"),
                )

                # Permutation importance (on test set)
                if do_permutation:
                    try:
                        # Compute on imputed data
                        X_test_imp = pipe.named_steps["imputer"].transform(X_test)
                        result = permutation_importance(
                            rf,
                            X_test_imp,
                            y_test,
                            n_repeats=5,
                            random_state=random_state,
                            n_jobs=-1,
                        )
                        perm = pd.Series(
                            result.importances_mean, index=X_num.columns
                        ).sort_values(ascending=False)
                        perm_df = perm.reset_index().rename(
                            columns={"index": "feature", 0: "perm_importance"}
                        )
                        display(
                            Markdown("### Permutation importance (split de validation)")
                        )
                        display(perm_df.head(30))
                        try:
                            perm_df.to_csv(
                                os.path.join(save_dir, "permutation_importances.csv"),
                                index=False,
                            )
                        except Exception:
                            pass
                    except Exception as e:
                        print(f"[!] Permutation importance impossible : {e}")

        # ---------- Recap ----------
        md = dedent(
            f"""
            ### Exploration complete
            * Files loaded : **{len(files)}**
            * Concatenated rows : **{len(df):,}**
            * Label used : **{label_col or "none"}**
            * Exports : `{save_dir}`
            """
        )
        display(Markdown(md))

        # Memo for later reuse if needed
        self._last_features_df = df.copy()
        self._last_features_numeric = list(
            df.select_dtypes(include=[np.number]).columns
        )
        self._last_rf_importances = rf_df
        self._last_perm_importances = perm_df

        return {
            "df": df,
            "numeric_cols": self._last_features_numeric,
            "label_col": label_col,
            "rf_importances": rf_df,
            "perm_importances": perm_df,
            "save_dir": save_dir,
        }

    # ----------------------------------------------------------------------
    # Reusable helper: importance bar chart (top N)
    # ----------------------------------------------------------------------

    def _plot_feature_importances_bar(
        self,
        importances_df: pd.DataFrame | None = None,
        *,
        top_n: int = 25,
        save_path: str | None = None,
    ) -> tuple[plt.Figure, plt.Axes] | None:
        """
        Plot a horizontal bar chart of importances from a DataFrame
        with columns = ['feature', 'importance'].

        If `importances_df` is None, use `self._last_rf_importances`.
        """
        import os
        import matplotlib.pyplot as plt

        if importances_df is None:
            importances_df = getattr(self, "_last_rf_importances", None)

        if importances_df is None or importances_df.empty:
            print("No importances to plot.")
            return None

        top = importances_df.head(int(top_n)).copy()
        plt.style.use("dark_background")
        fig, ax = plt.subplots(figsize=(10, max(5, 0.35 * len(top))))
        ax.barh(top["feature"][::-1], top[top.columns[-1]][::-1])
        ax.set_xlabel(top.columns[-1])
        ax.set_title("Importances -- top")
        ax.grid(True, axis="x", linestyle=":", alpha=0.3)
        fig.tight_layout()

        if save_path:
            try:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                fig.savefig(save_path, dpi=150, bbox_inches="tight")
            except Exception:
                pass

        return fig, ax
