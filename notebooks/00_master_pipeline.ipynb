{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1111b9",
   "metadata": {},
   "source": [
    "# ðŸŒŒ Master Notebook â€“ Pipeline Spectroscopie DR5\n",
    "\n",
    "## Objectif du pipeline\n",
    "\n",
    "- Ce notebook tÃ©lÃ©charge, prÃ©pare et journalise des spectres .fits.gz de LAMOST DR5 pour entraÃ®ner un modÃ¨le de classification.\n",
    "- Le DatasetBuilder garantit qu'aucun spectre ne sera jamais rÃ©utilisÃ© en s'appuyant sur un log de fichiers dÃ©jÃ  traitÃ©s.\n",
    "- Ce pipeline permet dâ€™ajouter progressivement des spectres au jeu dâ€™entraÃ®nement, en assurant quâ€™aucun spectre ne soit traitÃ© deux fois. Chaque exÃ©cution sÃ©lectionne un nouveau lot, lâ€™entraÃ®ne, puis marque les spectres comme utilisÃ©s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7400fe8",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b4e36",
   "metadata": {},
   "source": [
    "## Ã‰tape 0 : SETUP & IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa78825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import hashlib\n",
    "import subprocess\n",
    "\n",
    "# --- Initialisation de l'environnement via notre outil ---\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils import setup_project_env\n",
    "paths = setup_project_env()\n",
    "\n",
    "# --- On dÃ©finit les variables de chemin globales pour tout le notebook ---\n",
    "# Cela Ã©vite de les redÃ©finir dans chaque cellule\n",
    "RAW_DATA_DIR = paths[\"RAW_DATA_DIR\"]\n",
    "CATALOG_DIR = paths[\"CATALOG_DIR\"]\n",
    "PROCESSED_DIR = paths[\"PROCESSED_DIR\"]\n",
    "MODELS_DIR = paths[\"MODELS_DIR\"]\n",
    "REPORTS_DIR = paths[\"REPORTS_DIR\"]\n",
    "\n",
    "# --- Imports des outils spÃ©cifiques au pipeline ---\n",
    "from tools.dataset_builder import DatasetBuilder\n",
    "from pipeline.processing import ProcessingPipeline\n",
    "from pipeline.classifier import SpectralClassifier\n",
    "# On importe la fonction pour gÃ©nÃ©rer le catalogue local si elle est dans un module\n",
    "from tools.generate_catalog_from_fits import generate_catalog_from_fits \n",
    "\n",
    "# --- Initialisation des outils ---\n",
    "builder = DatasetBuilder(raw_data_dir=RAW_DATA_DIR, catalog_dir=CATALOG_DIR)\n",
    "\n",
    "print(\"\\nSetup terminÃ©. Tu es prÃªt Ã  lancer ton pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656d07f",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73195d93",
   "metadata": {},
   "source": [
    "## 1) TÃ©lÃ©chargement des spectres\n",
    "Utilisation du script ``dr5_downloader.py`` encapsulÃ© en fonction.\n",
    "\n",
    "Cette Ã©tape est dÃ©sormais externalisÃ©e dans [01_download_spectra.ipynb](./01_download_spectra.ipynb) pour Ãªtre exÃ©cutÃ©e seulement au besoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491df16",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f207e",
   "metadata": {},
   "source": [
    "## 2) PrÃ©paration des donnÃ©es spectrales : SÃ©lection du lot de spectres Ã  traiter\n",
    "- Le DatasetBuilder sÃ©lectionne un lot de nouveaux spectres jamais utilisÃ©s.\n",
    "- Si tous les spectres disponibles ont dÃ©jÃ  Ã©tÃ© utilisÃ©s, le pipeline s'arrÃªte proprement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d233f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Ã‰TAPE 2 : CRÃ‰ATION D'UN NOUVEAU LOT DE SPECTRES NON DÃ‰JÃ€ UTILISÃ‰S ===\")\n",
    "\n",
    "# Pour dÃ©finir la grosseur du lot modifier la variable de batch_size= par la valeur voulu\n",
    "new_batch_paths = builder.get_new_training_batch(batch_size=3000, strategy=\"random\")\n",
    "\n",
    "if new_batch_paths:\n",
    "    print(f\"\\n{len(new_batch_paths)} nouveaux spectres proposÃ©s pour traitement.\")\n",
    "    print(f\"Exemple : {new_batch_paths[0]}\")\n",
    "else:\n",
    "    print(\"\\nAucun nouveau spectre Ã  traiter : le pipeline est Ã  jour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18109a",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8538af",
   "metadata": {},
   "source": [
    "## 3) parcours ce lot de spectres pour gÃ©nÃ©rer un CSV Ã  partir des headers des fichiers ``.fits.gz``\n",
    "Câ€™est ce CSV qui sera ton ``master_catalog`` local, alignÃ© exactement avec les spectres que tu vas traiter dans ce lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# On importe la fonction depuis le module oÃ¹ elle se trouve\n",
    "from tools.generate_catalog_from_fits import generate_catalog_from_fits\n",
    "\n",
    "print(\"\\n=== Ã‰TAPE 3 : GÃ‰NÃ‰RATION DU CATALOGUE LOCAL DE HEADERS ===\\n\")\n",
    "\n",
    "# On vÃ©rifie que le lot de fichiers est disponible\n",
    "if 'new_batch_paths' in locals() and new_batch_paths:\n",
    "    # Chemin de sortie du CSV temporaire/local\n",
    "    output_catalog_path = os.path.join(CATALOG_DIR, \"master_catalog_temp.csv\")\n",
    "\n",
    "    # CrÃ©er le dossier si nÃ©cessaire\n",
    "    os.makedirs(os.path.dirname(output_catalog_path), exist_ok=True)\n",
    "\n",
    "    # PrÃ©fixer chaque chemin relatif avec le chemin complet vers data/raw\n",
    "    # On utilise la variable RAW_DATA_DIR dÃ©finie dans la cellule de SETUP\n",
    "    full_paths = [os.path.join(RAW_DATA_DIR, path) for path in new_batch_paths]\n",
    "\n",
    "    # Appel de la fonction\n",
    "    generate_catalog_from_fits(full_paths, output_catalog_path)\n",
    "\n",
    "    print(f\"\\nCatalogue master local crÃ©Ã© : {output_catalog_path}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord exÃ©cuter la cellule 'SÃ‰LECTION DU LOT DE TRAVAIL' pour dÃ©finir 'new_batch_paths'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b0758",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66139a37",
   "metadata": {},
   "source": [
    "### effacer le contenu du master_catalog_temp.csv avant de regÃ©nÃ©rer un nouveau lot avec de nouveaux spectres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33664e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Nettoyage du catalogue temporaire ---\")\n",
    "\n",
    "# On utilise la variable CATALOG_DIR dÃ©finie dans la cellule de SETUP\n",
    "catalog_path = os.path.join(CATALOG_DIR, \"master_catalog_temp.csv\")\n",
    "\n",
    "if os.path.exists(catalog_path):\n",
    "    # CrÃ©er un DataFrame vide avec uniquement l'en-tÃªte\n",
    "    # Assure-toi que cette liste de colonnes est la mÃªme que celle gÃ©nÃ©rÃ©e par ton script\n",
    "    columns = [\n",
    "        'fits_name', 'obsid', 'plan_id', 'mjd', 'class', 'subclass',\n",
    "        'filename_original', 'author', 'data_version', 'date_creation',\n",
    "        'telescope', 'longitude_site', 'latitude_site', 'obs_date_utc',\n",
    "        'jd', 'ra', 'dec', 'fiber_id', 'fiber_type', 'object_name', 'catalog_object_type',\n",
    "        'magnitude_type', 'magnitude_u', 'magnitude_g', 'magnitude_r', 'magnitude_i', 'magnitude_z',\n",
    "        'heliocentric_correction', 'radial_velocity_corr', 'seeing',\n",
    "        'redshift', 'redshift_error', 'snr_u', 'snr_g', 'snr_r', 'snr_i', 'snr_z'\n",
    "    ]\n",
    "    empty_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # Ã‰craser le fichier existant avec le DataFrame vide\n",
    "    empty_df.to_csv(catalog_path, sep='|', index=False, encoding='utf-8')\n",
    "    print(f\"Fichier {os.path.basename(catalog_path)} vidÃ© et prÃªt pour un nouveau lot.\")\n",
    "else:\n",
    "    print(f\"Le fichier {os.path.basename(catalog_path)} n'existe pas encore, pas de nettoyage nÃ©cessaire.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d8849",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3cc38",
   "metadata": {},
   "source": [
    "## 4) Exploration des features\n",
    "### Analyse exploratoire des spectres prÃ©traitÃ©s\n",
    "Visualiser SNR, distribution de classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac41204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vÃ©rifie que la variable 'new_batch_paths' a bien Ã©tÃ© crÃ©Ã©e\n",
    "if 'new_batch_paths' in locals() and new_batch_paths:\n",
    "    print(\"\\n--- Ã‰TAPE 4: Lancement du pipeline de traitement ---\")\n",
    "\n",
    "    # --- Ã‰tape 4.1 : Charger le catalogue TEMPORAIRE ---\n",
    "    master_catalog_path = os.path.join(CATALOG_DIR, \"master_catalog_temp.csv\")\n",
    "    try:\n",
    "        master_catalog_df = pd.read_csv(master_catalog_path, sep='|')\n",
    "        print(f\"  > Catalogue temporaire chargÃ© avec succÃ¨s ({len(master_catalog_df)} entrÃ©es).\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  > ERREUR CRITIQUE : Le catalogue temporaire est introuvable.\")\n",
    "        master_catalog_df = None\n",
    "\n",
    "    # --- Ã‰tape 4.2 : Initialisation du pipeline ---\n",
    "    processing_pipeline = ProcessingPipeline(\n",
    "        raw_data_dir=RAW_DATA_DIR,\n",
    "        master_catalog_df=master_catalog_df\n",
    "    )\n",
    "    \n",
    "    # --- Ã‰tape 4.3 : Lancement du traitement ---\n",
    "    features_df = processing_pipeline.run(new_batch_paths)\n",
    "    \n",
    "    # --- Ã‰tape 4.4 : Sauvegarde et affichage des rÃ©sultats ---\n",
    "    if not features_df.empty:\n",
    "        print(\"\\n--- AperÃ§u du dataset de features gÃ©nÃ©rÃ© ---\")\n",
    "        display(features_df.head())\n",
    "        \n",
    "        # <<< LA PARTIE MANQUANTE EST ICI >>>\n",
    "        timestamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "        features_filename = f\"features_{timestamp}.csv\"\n",
    "        features_path = os.path.join(PROCESSED_DIR, features_filename)\n",
    "        \n",
    "        os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "        \n",
    "        features_df.to_csv(features_path, index=False)\n",
    "        print(f\"\\nDataset de features sauvegardÃ© avec succÃ¨s dans : {features_path}\")\n",
    "        # --- FIN DE LA PARTIE MANQUANTE ---\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n  > Aucun feature n'a pu Ãªtre extrait.\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord exÃ©cuter la cellule de sÃ©lection de lot ('Ã‰tape 2').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b2cc1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f95d0",
   "metadata": {},
   "source": [
    "## 5) EntraÃ®nement du modÃ¨le\n",
    "### Machine Learning\n",
    "SÃ©lection des features et entraÃ®nement d'un modÃ¨le de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232519eb",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pipeline.classifier import SpectralClassifier\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Charger le dernier dataset de features\n",
    "list_of_feature_files = glob.glob(os.path.join(PROCESSED_DIR, 'features_*.csv'))\n",
    "if not list_of_feature_files:\n",
    "    print(\"ERREUR : Aucun fichier de features trouvÃ©.\")\n",
    "else:\n",
    "    latest_feature_file = max(list_of_feature_files, key=os.path.getctime)\n",
    "    print(f\"--- Chargement du dataset : {os.path.basename(latest_feature_file)} ---\")\n",
    "    features_df = pd.read_csv(latest_feature_file)\n",
    "\n",
    "    # 2) CrÃ©er la colonne 'label' et nettoyer les donnÃ©es\n",
    "    if 'subclass' in features_df.columns:\n",
    "        features_df['label'] = features_df['subclass'].astype(str).str[0]\n",
    "    else:\n",
    "        features_df['label'] = 'UNKNOWN'\n",
    "        \n",
    "    initial_count = len(features_df)\n",
    "    df_trainable = features_df[features_df[\"label\"].notnull() & ~features_df[\"label\"].isin(['U', 'N', 'n', 'N'])].copy() # Ajout de 'N' majuscule\n",
    "    print(f\"  > {initial_count - len(df_trainable)} lignes avec des labels invalides ou nuls supprimÃ©es.\")\n",
    "    \n",
    "    label_counts = df_trainable[\"label\"].value_counts()\n",
    "    rare_labels = label_counts[label_counts < 5].index\n",
    "    if len(rare_labels) > 0:\n",
    "        print(f\"  > Suppression des classes trop rares : {list(rare_labels)}\")\n",
    "        df_trainable = df_trainable[~df_trainable[\"label\"].isin(rare_labels)]\n",
    "\n",
    "    # 3) PrÃ©parer X et y, puis lancer l'entraÃ®nement\n",
    "    if not df_trainable.empty:\n",
    "        feature_cols = [col for col in df_trainable.columns if col.startswith('feature_')]\n",
    "        X = df_trainable[feature_cols].values\n",
    "        y = df_trainable[\"label\"].values\n",
    "\n",
    "        print(f\"\\nFeatures utilisÃ©es : {feature_cols}\")\n",
    "        print(f\"Nombre d'Ã©chantillons final : {X.shape[0]}, Nombre de features : {X.shape[1]}\")\n",
    "\n",
    "        print(\"\\n--- Ã‰TAPE 5: EntraÃ®nement et Ã‰valuation du modÃ¨le ---\")\n",
    "        clf = SpectralClassifier(n_estimators=200)\n",
    "        clf.train_and_evaluate(X, y, test_size=0.25)\n",
    "        \n",
    "        # --- Si l'entraÃ®nement a rÃ©ussi, on continue avec la sauvegarde et le rapport ---\n",
    "        \n",
    "        # 4) Sauvegarder le modÃ¨le\n",
    "        model_path = os.path.join(MODELS_DIR, \"spectral_classifier.pkl\")\n",
    "        os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "        clf.save_model(model_path)\n",
    "        \n",
    "        # 5) Mettre Ã  jour le journal des spectres\n",
    "        print(\"\\n--- Ã‰TAPE 6: Mise Ã  jour du Journal des Spectres UtilisÃ©s ---\")\n",
    "        processed_files = df_trainable['file_path'].tolist()\n",
    "        builder.update_trained_log(processed_files)\n",
    "        \n",
    "        # 6) GÃ©nÃ©rer le rapport de session\n",
    "        print(\"\\n--- Ã‰TAPE 7: GÃ©nÃ©ration du Rapport de Session ---\")\n",
    "        timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        \n",
    "        model_hash = \"N/A\"\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                model_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            print(f\"  > Hash MD5 du modÃ¨le : {model_hash}\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        predictions = clf.model.predict(X_test)\n",
    "        report_dict = classification_report(y_test, predictions, labels=clf.class_labels, zero_division=0, output_dict=True)\n",
    "        metrics_summary = report_dict.get(\"weighted avg\", {})\n",
    "        print(f\"  > MÃ©triques extraites : Accuracy = {report_dict.get('accuracy', 0):.2f}\")\n",
    "\n",
    "        session_report = {\n",
    "            \"session_id\": timestamp,\n",
    "            \"date_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"model_path\": model_path,\n",
    "            \"model_hash_md5\": model_hash,\n",
    "            \"training_set_size\": len(X_train),\n",
    "            \"test_set_size\": len(X_test),\n",
    "            \"total_spectra_processed\": len(processed_files),\n",
    "            \"feature_columns\": feature_cols,\n",
    "            \"class_labels\": clf.class_labels,\n",
    "            \"metrics\": metrics_summary,\n",
    "            \"processed_files_list\": processed_files\n",
    "        }\n",
    "\n",
    "        report_filename = f\"session_report_{timestamp}.json\"\n",
    "        report_path = os.path.join(REPORTS_DIR, report_filename)\n",
    "        os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(session_report, f, indent=4)\n",
    "        print(f\"\\nRapport de session sauvegardÃ© dans : {report_path}\")\n",
    "        \n",
    "        print(\"\\n\\nSESSION DE RECHERCHE TERMINÃ‰E\")\n",
    "    else:\n",
    "        print(\"\\n  > Pas assez de donnÃ©es valides pour lancer l'entraÃ®nement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673b3aa",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eeb945",
   "metadata": {},
   "source": [
    "## **Permettre l'affichage des labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d390d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_df[\"label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
