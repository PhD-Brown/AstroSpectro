{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1111b9",
   "metadata": {},
   "source": [
    "# üåå Master Notebook ‚Äì Pipeline Spectroscopie DR5\n",
    "\n",
    "## Objectif du pipeline\n",
    "\n",
    "- Ce notebook t√©l√©charge, pr√©pare et journalise des spectres .fits.gz de LAMOST DR5 pour entra√Æner un mod√®le de classification.\n",
    "- Le DatasetBuilder garantit qu'aucun spectre ne sera jamais r√©utilis√© en s'appuyant sur un log de fichiers d√©j√† trait√©s.\n",
    "- Ce pipeline permet d‚Äôajouter progressivement des spectres au jeu d‚Äôentra√Ænement, en assurant qu‚Äôaucun spectre ne soit trait√© deux fois. Chaque ex√©cution s√©lectionne un nouveau lot, l‚Äôentra√Æne, puis marque les spectres comme utilis√©s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7400fe8",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b4e36",
   "metadata": {},
   "source": [
    "## √âtape 0 : SETUP & IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa78825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import hashlib\n",
    "import subprocess\n",
    "\n",
    "# --- Initialisation de l'environnement via notre outil ---\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils import setup_project_env\n",
    "paths = setup_project_env()\n",
    "\n",
    "# --- On d√©finit les variables de chemin globales pour tout le notebook ---\n",
    "# Cela √©vite de les red√©finir dans chaque cellule\n",
    "RAW_DATA_DIR = paths[\"RAW_DATA_DIR\"]\n",
    "CATALOG_DIR = paths[\"CATALOG_DIR\"]\n",
    "PROCESSED_DIR = paths[\"PROCESSED_DIR\"]\n",
    "MODELS_DIR = paths[\"MODELS_DIR\"]\n",
    "REPORTS_DIR = paths[\"REPORTS_DIR\"]\n",
    "\n",
    "# --- Imports des outils sp√©cifiques au pipeline ---\n",
    "from tools.dataset_builder import DatasetBuilder\n",
    "from pipeline.processing import ProcessingPipeline\n",
    "from pipeline.classifier import SpectralClassifier\n",
    "# On importe la fonction pour g√©n√©rer le catalogue local si elle est dans un module\n",
    "from tools.generate_catalog_from_fits import generate_catalog_from_fits \n",
    "\n",
    "# --- Initialisation des outils ---\n",
    "builder = DatasetBuilder(raw_data_dir=RAW_DATA_DIR, catalog_dir=CATALOG_DIR)\n",
    "\n",
    "print(\"\\nSetup termin√©. Tu es pr√™t √† lancer ton pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656d07f",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73195d93",
   "metadata": {},
   "source": [
    "## 1) T√©l√©chargement des spectres\n",
    "Utilisation du script ``dr5_downloader.py`` encapsul√© en fonction.\n",
    "\n",
    "Cette √©tape est d√©sormais externalis√©e dans [01_download_spectra.ipynb](./01_download_spectra.ipynb) pour √™tre ex√©cut√©e seulement au besoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491df16",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f207e",
   "metadata": {},
   "source": [
    "## 2) Pr√©paration des donn√©es spectrales : S√©lection du lot de spectres √† traiter\n",
    "- Le DatasetBuilder s√©lectionne un lot de nouveaux spectres jamais utilis√©s.\n",
    "- Si tous les spectres disponibles ont d√©j√† √©t√© utilis√©s, le pipeline s'arr√™te proprement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d233f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== √âTAPE 2 : CR√âATION D'UN NOUVEAU LOT DE SPECTRES NON D√âJ√Ä UTILIS√âS ===\")\n",
    "\n",
    "# Pour d√©finir la grosseur du lot modifier la variable de batch_size= par la valeur voulu\n",
    "new_batch_paths = builder.get_new_training_batch(batch_size=3000, strategy=\"random\")\n",
    "\n",
    "if new_batch_paths:\n",
    "    print(f\"\\n{len(new_batch_paths)} nouveaux spectres propos√©s pour traitement.\")\n",
    "    print(f\"Exemple : {new_batch_paths[0]}\")\n",
    "else:\n",
    "    print(\"\\nAucun nouveau spectre √† traiter : le pipeline est √† jour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18109a",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8538af",
   "metadata": {},
   "source": [
    "## 3) parcours ce lot de spectres pour g√©n√©rer un CSV √† partir des headers des fichiers ``.fits.gz``\n",
    "C‚Äôest ce CSV qui sera ton ``master_catalog`` local, align√© exactement avec les spectres que tu vas traiter dans ce lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# On importe la fonction depuis le module o√π elle se trouve\n",
    "from tools.generate_catalog_from_fits import generate_catalog_from_fits\n",
    "\n",
    "print(\"\\n=== √âTAPE 3 : G√âN√âRATION DU CATALOGUE LOCAL DE HEADERS ===\\n\")\n",
    "\n",
    "# On v√©rifie que le lot de fichiers est disponible\n",
    "if 'new_batch_paths' in locals() and new_batch_paths:\n",
    "    # Chemin de sortie du CSV temporaire/local\n",
    "    output_catalog_path = os.path.join(CATALOG_DIR, \"master_catalog_temp.csv\")\n",
    "\n",
    "    # Cr√©er le dossier si n√©cessaire\n",
    "    os.makedirs(os.path.dirname(output_catalog_path), exist_ok=True)\n",
    "\n",
    "    # Pr√©fixer chaque chemin relatif avec le chemin complet vers data/raw\n",
    "    # On utilise la variable RAW_DATA_DIR d√©finie dans la cellule de SETUP\n",
    "    full_paths = [os.path.join(RAW_DATA_DIR, path) for path in new_batch_paths]\n",
    "\n",
    "    # Appel de la fonction\n",
    "    generate_catalog_from_fits(full_paths, output_catalog_path)\n",
    "\n",
    "    print(f\"\\nCatalogue master local cr√©√© : {output_catalog_path}\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord ex√©cuter la cellule 'S√âLECTION DU LOT DE TRAVAIL' pour d√©finir 'new_batch_paths'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b0758",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66139a37",
   "metadata": {},
   "source": [
    "### effacer le contenu du master_catalog_temp.csv avant de reg√©n√©rer un nouveau lot avec de nouveaux spectres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33664e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Nettoyage du catalogue temporaire ---\")\n",
    "\n",
    "# On utilise la variable CATALOG_DIR d√©finie dans la cellule de SETUP\n",
    "catalog_path = os.path.join(CATALOG_DIR, \"master_catalog_temp.csv\")\n",
    "\n",
    "if os.path.exists(catalog_path):\n",
    "    # Cr√©er un DataFrame vide avec uniquement l'en-t√™te\n",
    "    # Assure-toi que cette liste de colonnes est la m√™me que celle g√©n√©r√©e par ton script\n",
    "    columns = [\n",
    "        'fits_name', 'obsid', 'plan_id', 'mjd', 'class', 'subclass',\n",
    "        'filename_original', 'author', 'data_version', 'date_creation',\n",
    "        'telescope', 'longitude_site', 'latitude_site', 'obs_date_utc',\n",
    "        'jd', 'ra', 'dec', 'fiber_id', 'fiber_type', 'object_name', 'catalog_object_type',\n",
    "        'magnitude_type', 'magnitude_u', 'magnitude_g', 'magnitude_r', 'magnitude_i', 'magnitude_z',\n",
    "        'heliocentric_correction', 'radial_velocity_corr', 'seeing',\n",
    "        'redshift', 'redshift_error', 'snr_u', 'snr_g', 'snr_r', 'snr_i', 'snr_z'\n",
    "    ]\n",
    "    empty_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # √âcraser le fichier existant avec le DataFrame vide\n",
    "    empty_df.to_csv(catalog_path, sep='|', index=False, encoding='utf-8')\n",
    "    print(f\"Fichier {os.path.basename(catalog_path)} vid√© et pr√™t pour un nouveau lot.\")\n",
    "else:\n",
    "    print(f\"Le fichier {os.path.basename(catalog_path)} n'existe pas encore, pas de nettoyage n√©cessaire.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d8849",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3cc38",
   "metadata": {},
   "source": [
    "## 4) Exploration des features\n",
    "### Analyse exploratoire des spectres pr√©trait√©s\n",
    "Visualiser SNR, distribution de classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac41204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On v√©rifie que la variable 'new_batch_paths' a bien √©t√© cr√©√©e\n",
    "if 'new_batch_paths' in locals() and new_batch_paths:\n",
    "    print(\"\\n--- √âTAPE 4: Lancement du pipeline de traitement ---\")\n",
    "\n",
    "    # --- √âtape 4.1 : Charger le catalogue TEMPORAIRE ---\n",
    "    master_catalog_path = os.path.join(CATALOG_DIR, \"master_catalog_temp.csv\")\n",
    "    try:\n",
    "        master_catalog_df = pd.read_csv(master_catalog_path, sep='|')\n",
    "        print(f\"  > Catalogue temporaire charg√© avec succ√®s ({len(master_catalog_df)} entr√©es).\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  > ERREUR CRITIQUE : Le catalogue temporaire est introuvable.\")\n",
    "        master_catalog_df = None\n",
    "\n",
    "    # --- √âtape 4.2 : Initialisation du pipeline ---\n",
    "    processing_pipeline = ProcessingPipeline(\n",
    "        raw_data_dir=RAW_DATA_DIR,\n",
    "        master_catalog_df=master_catalog_df\n",
    "    )\n",
    "    \n",
    "    # --- √âtape 4.3 : Lancement du traitement ---\n",
    "    features_df = processing_pipeline.run(new_batch_paths)\n",
    "    \n",
    "    # --- √âtape 4.4 : Sauvegarde et affichage des r√©sultats ---\n",
    "    if not features_df.empty:\n",
    "        print(\"\\n--- Aper√ßu du dataset de features g√©n√©r√© ---\")\n",
    "        display(features_df.head())\n",
    "        \n",
    "        # <<< LA PARTIE MANQUANTE EST ICI >>>\n",
    "        timestamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')\n",
    "        features_filename = f\"features_{timestamp}.csv\"\n",
    "        features_path = os.path.join(PROCESSED_DIR, features_filename)\n",
    "        \n",
    "        os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "        \n",
    "        features_df.to_csv(features_path, index=False)\n",
    "        print(f\"\\nDataset de features sauvegard√© avec succ√®s dans : {features_path}\")\n",
    "        # --- FIN DE LA PARTIE MANQUANTE ---\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n  > Aucun feature n'a pu √™tre extrait.\")\n",
    "else:\n",
    "    print(\"Veuillez d'abord ex√©cuter la cellule de s√©lection de lot ('√âtape 2').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b2cc1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f95d0",
   "metadata": {},
   "source": [
    "## 5) Entra√Ænement du mod√®le\n",
    "### Machine Learning\n",
    "S√©lection des features et entra√Ænement d'un mod√®le de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232519eb",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pipeline.classifier import SpectralClassifier\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Charger le dernier dataset de features\n",
    "list_of_feature_files = glob.glob(os.path.join(PROCESSED_DIR, 'features_*.csv'))\n",
    "if not list_of_feature_files:\n",
    "    print(\"ERREUR : Aucun fichier de features trouv√©.\")\n",
    "else:\n",
    "    latest_feature_file = max(list_of_feature_files, key=os.path.getctime)\n",
    "    print(f\"--- Chargement du dataset : {os.path.basename(latest_feature_file)} ---\")\n",
    "    features_df = pd.read_csv(latest_feature_file)\n",
    "\n",
    "    # 2) Cr√©er la colonne 'label' et nettoyer les donn√©es\n",
    "    if 'subclass' in features_df.columns:\n",
    "        features_df['label'] = features_df['subclass'].astype(str).str[0]\n",
    "    else:\n",
    "        features_df['label'] = 'UNKNOWN'\n",
    "        \n",
    "    initial_count = len(features_df)\n",
    "    df_trainable = features_df[features_df[\"label\"].notnull() & ~features_df[\"label\"].isin(['U', 'N', 'n', 'N'])].copy() # Ajout de 'N' majuscule\n",
    "    print(f\"  > {initial_count - len(df_trainable)} lignes avec des labels invalides ou nuls supprim√©es.\")\n",
    "    \n",
    "    label_counts = df_trainable[\"label\"].value_counts()\n",
    "    rare_labels = label_counts[label_counts < 5].index\n",
    "    if len(rare_labels) > 0:\n",
    "        print(f\"  > Suppression des classes trop rares : {list(rare_labels)}\")\n",
    "        df_trainable = df_trainable[~df_trainable[\"label\"].isin(rare_labels)]\n",
    "\n",
    "    # 3) Pr√©parer X et y, puis lancer l'entra√Ænement\n",
    "    if not df_trainable.empty:\n",
    "        feature_cols = [col for col in df_trainable.columns if col.startswith('feature_')]\n",
    "        X = df_trainable[feature_cols].values\n",
    "        y = df_trainable[\"label\"].values\n",
    "\n",
    "        print(f\"\\nFeatures utilis√©es : {feature_cols}\")\n",
    "        print(f\"Nombre d'√©chantillons final : {X.shape[0]}, Nombre de features : {X.shape[1]}\")\n",
    "\n",
    "        print(\"\\n--- √âTAPE 5: Entra√Ænement et √âvaluation du mod√®le ---\")\n",
    "        clf = SpectralClassifier(n_estimators=200)\n",
    "        clf.train_and_evaluate(X, y, test_size=0.25)\n",
    "        \n",
    "        # --- Si l'entra√Ænement a r√©ussi, on continue avec la sauvegarde et le rapport ---\n",
    "        \n",
    "        # 4) Sauvegarder le mod√®le\n",
    "        model_path = os.path.join(MODELS_DIR, \"spectral_classifier.pkl\")\n",
    "        os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "        clf.save_model(model_path)\n",
    "        \n",
    "        # 5) Mettre √† jour le journal des spectres\n",
    "        print(\"\\n--- √âTAPE 6: Mise √† jour du Journal des Spectres Utilis√©s ---\")\n",
    "        processed_files = df_trainable['file_path'].tolist()\n",
    "        builder.update_trained_log(processed_files)\n",
    "        \n",
    "        # 6) G√©n√©rer le rapport de session\n",
    "        print(\"\\n--- √âTAPE 7: G√©n√©ration du Rapport de Session ---\")\n",
    "        timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        \n",
    "        model_hash = \"N/A\"\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                model_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            print(f\"  > Hash MD5 du mod√®le : {model_hash}\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        predictions = clf.model.predict(X_test)\n",
    "        report_dict = classification_report(y_test, predictions, labels=clf.class_labels, zero_division=0, output_dict=True)\n",
    "        metrics_summary = report_dict.get(\"weighted avg\", {})\n",
    "        print(f\"  > M√©triques extraites : Accuracy = {report_dict.get('accuracy', 0):.2f}\")\n",
    "\n",
    "        session_report = {\n",
    "            \"session_id\": timestamp,\n",
    "            \"date_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"model_path\": model_path,\n",
    "            \"model_hash_md5\": model_hash,\n",
    "            \"training_set_size\": len(X_train),\n",
    "            \"test_set_size\": len(X_test),\n",
    "            \"total_spectra_processed\": len(processed_files),\n",
    "            \"feature_columns\": feature_cols,\n",
    "            \"class_labels\": clf.class_labels,\n",
    "            \"metrics\": metrics_summary,\n",
    "            \"processed_files_list\": processed_files\n",
    "        }\n",
    "\n",
    "        report_filename = f\"session_report_{timestamp}.json\"\n",
    "        report_path = os.path.join(REPORTS_DIR, report_filename)\n",
    "        os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(session_report, f, indent=4)\n",
    "        print(f\"\\nRapport de session sauvegard√© dans : {report_path}\")\n",
    "        \n",
    "        print(\"\\n\\nSESSION DE RECHERCHE TERMIN√âE\")\n",
    "    else:\n",
    "        print(\"\\n  > Pas assez de donn√©es valides pour lancer l'entra√Ænement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673b3aa",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eeb945",
   "metadata": {},
   "source": [
    "## **Permettre l'affichage des labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d390d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_df[\"label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
