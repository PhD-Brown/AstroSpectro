{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47dfedea",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #5C7CFA 100%);\n",
    "    padding: 40px;\n",
    "    border-radius: 15px;\n",
    "    margin: 20px 0;\n",
    "    box-shadow: 0 8px 32px rgba(102, 126, 234, 0.3);\n",
    "\">\n",
    "    <h1 style=\"\n",
    "        color: white;\n",
    "        text-align: center;\n",
    "        margin: 0;\n",
    "        font-size: 36px;\n",
    "        font-weight: 300;\n",
    "        letter-spacing: 3px;\n",
    "        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\n",
    "    \">MODEL INTERPRETABILITY</h1>\n",
    "    <p style=\"\n",
    "        color: rgba(255,255,255,0.95);\n",
    "        text-align: center;\n",
    "        margin: 15px 0 0 0;\n",
    "        font-size: 16px;\n",
    "        font-weight: 300;\n",
    "        letter-spacing: 1px;\n",
    "    \">SHAP Analysis for Physics-Grounded Feature Understanding</p>\n",
    "</div>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook uses SHAP (SHapley Additive exPlanations) to interpret the trained XGBoost classifier and validate that its decisions align with astrophysical expectations. We investigate:\n",
    "\n",
    "1. **Which features dominate classification decisions?** â€” Do physically meaningful features (HÎ±, Ca II, colour indices) rank among the most important, or does the model rely on observational artifacts?\n",
    "2. **Does the model prioritize physical features vs. observational artifacts?** â€” We compare SHAP importance of spectroscopic features (line profiles, equivalent widths) against metadata (SNR, seeing, fiber ID).\n",
    "3. **Are there unexpected feature interactions revealing new physics?** â€” SHAP dependence plots can expose non-linear interactions that simple correlation analysis cannot detect.\n",
    "\n",
    "**Validation Context:**  \n",
    "SHAP values provide a **model-agnostic** decomposition of each prediction into per-feature contributions, grounded in cooperative game theory. For tree-based models (XGBoost), exact SHAP values can be computed via `TreeExplainer` in polynomial time, making this analysis both rigorous and tractable.\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "SHAP (Lundberg & Lee, 2017) assigns each feature a **Shapley value** â€” the average marginal contribution across all possible feature coalitions. Unlike global feature importance (e.g., Gini importance), SHAP values are **local** (per-prediction) and **additive** (they sum to the model output minus the expected value). This makes SHAP the gold standard for post-hoc interpretability: it reveals not just *which* features matter, but *how* and *for which classes*.\n",
    "\n",
    "For stellar classification, we expect spectroscopic line features (HÎ± FWHM, Ca II K prominence, Mg b equivalent width) and photometric colours (gâˆ’r, Blue/Red index) to dominate. If the model instead relies heavily on metadata (fiber ID, Julian date, seeing), this signals potential data leakage or confounding.\n",
    "\n",
    "## Key References\n",
    "\n",
    "- **Lundberg, S. M. & Lee, S.-I. (2017)** â€” *\"A Unified Approach to Interpreting Model Predictions\"*, NeurIPS 2017.\n",
    "- **Lundberg, S. M. et al. (2020)** â€” *\"From Local Explanations to Global Understanding with Explainable AI for Trees\"*, Nature Machine Intelligence, 2, 56â€“67.\n",
    "- **Luo, A.-L. et al. (2015)** â€” *\"The First Data Release of the LAMOST Regular Survey\"*, RAA, 15, 1095."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7b368",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## ğŸ“‘ Table of Contents\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin: 20px 0;\">\n",
    "\n",
    "<!-- Setup -->\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3); height: 120px; display: flex; flex-direction: column; justify-content: center;\">\n",
    "    <a href=\"#setup\" style=\"color: white; text-decoration: none;\">\n",
    "        <strong style=\"font-size: 16px; display: block; margin-bottom: 8px;\">âš™ï¸ Section 1: Setup & Model Loading</strong>\n",
    "        <span style=\"font-size: 12px; opacity: 0.9; line-height: 1.4;\">Imports, model loading, SHAP explainer creation</span>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<!-- Global Importance -->\n",
    "<div style=\"background: linear-gradient(135deg, #764ba2 0%, #667eea 100%); padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(118, 75, 162, 0.3); height: 120px; display: flex; flex-direction: column; justify-content: center;\">\n",
    "    <a href=\"#global\" style=\"color: white; text-decoration: none;\">\n",
    "        <strong style=\"font-size: 16px; display: block; margin-bottom: 8px;\">ğŸ“Š Section 2: Global Feature Importance</strong>\n",
    "        <span style=\"font-size: 12px; opacity: 0.9; line-height: 1.4;\">Mean |SHAP| ranking, summary plots</span>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<!-- Class-Specific -->\n",
    "<div style=\"background: linear-gradient(135deg, #5C7CFA 0%, #764ba2 100%); padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(92, 124, 250, 0.3); height: 120px; display: flex; flex-direction: column; justify-content: center;\">\n",
    "    <a href=\"#classwise\" style=\"color: white; text-decoration: none;\">\n",
    "        <strong style=\"font-size: 16px; display: block; margin-bottom: 8px;\">ğŸ”¬ Section 3: Class-Specific SHAP</strong>\n",
    "        <span style=\"font-size: 12px; opacity: 0.9; line-height: 1.4;\">Per-class feature drivers & beeswarm plots</span>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<!-- Interactions -->\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #5C7CFA 100%); padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3); height: 120px; display: flex; flex-direction: column; justify-content: center;\">\n",
    "    <a href=\"#interactions\" style=\"color: white; text-decoration: none;\">\n",
    "        <strong style=\"font-size: 16px; display: block; margin-bottom: 8px;\">ğŸ”— Section 4: Feature Interactions</strong>\n",
    "        <span style=\"font-size: 12px; opacity: 0.9; line-height: 1.4;\">SHAP dependence plots & interaction effects</span>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<!-- Discovery -->\n",
    "<div style=\"background: linear-gradient(135deg, #764ba2 0%, #5C7CFA 100%); padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(118, 75, 162, 0.3); height: 120px; display: flex; flex-direction: column; justify-content: center;\">\n",
    "    <a href=\"#discovery\" style=\"color: white; text-decoration: none;\">\n",
    "        <strong style=\"font-size: 16px; display: block; margin-bottom: 8px;\">ğŸ’¡ Section 5: Discovery Synthesis</strong>\n",
    "        <span style=\"font-size: 12px; opacity: 0.9; line-height: 1.4;\">Physical vs. artifact features, key findings</span>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<!-- Summary -->\n",
    "<div style=\"background: linear-gradient(135deg, #5C7CFA 0%, #667eea 100%); padding: 20px; border-radius: 10px; box-shadow: 0 4px 15px rgba(92, 124, 250, 0.3); height: 120px; display: flex; flex-direction: column; justify-content: center;\">\n",
    "    <a href=\"#summary\" style=\"color: white; text-decoration: none;\">\n",
    "        <strong style=\"font-size: 16px; display: block; margin-bottom: 8px;\">ğŸ“‹ Section 6: Summary & Composite Figure</strong>\n",
    "        <span style=\"font-size: 12px; opacity: 0.9; line-height: 1.4;\">Publication-quality figure & conclusions</span>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; margin: 30px 0; padding: 15px; background: linear-gradient(135deg, rgba(102, 126, 234, 0.1), rgba(118, 75, 162, 0.1)); border-radius: 10px; border: 1px solid rgba(102, 126, 234, 0.3);\">\n",
    "    <p style=\"margin: 0; color: #667eea; font-size: 13px;\">ğŸ’¡ <strong>Quick Navigation:</strong> Click any section above to jump directly</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546a470",
   "metadata": {},
   "source": [
    "<div style=\"margin: 40px 0;\">\n",
    "    <div style=\"height: 3px; background: linear-gradient(90deg, transparent, #667eea, #764ba2, #5C7CFA, transparent); border-radius: 3px;\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a9481",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px 30px; border-radius: 12px; margin: 30px 0 20px 0; box-shadow: 0 8px 25px rgba(102, 126, 234, 0.3); border: 2px solid rgba(255, 255, 255, 0.1);\">\n",
    "    <h2 style=\"color: white; margin: 0; text-align: center; font-size: 28px; font-weight: 300; letter-spacing: 2px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">âš™ï¸ SETUP & MODEL LOADING</h2>\n",
    "</div>\n",
    "\n",
    "<a id=\"setup\"></a>\n",
    "\n",
    "<div style=\"text-align: right; margin: 10px 0;\">\n",
    "    <a href=\"#toc\" style=\"color: #667eea; text-decoration: none; font-size: 12px;\">â¬†ï¸ Back to TOC</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93430304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Setup: imports, style, reproducibility\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"shap\")\n",
    "\n",
    "# â”€â”€ Style â”€â”€\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"#1a1a2e\",\n",
    "    \"axes.facecolor\": \"#16213e\",\n",
    "    \"axes.edgecolor\": \"#e94560\",\n",
    "    \"axes.labelcolor\": \"white\",\n",
    "    \"text.color\": \"white\",\n",
    "    \"xtick.color\": \"white\",\n",
    "    \"ytick.color\": \"white\",\n",
    "    \"grid.color\": \"#333333\",\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"font.size\": 11,\n",
    "    \"figure.dpi\": 100,\n",
    "})\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# â”€â”€ Paths â”€â”€\n",
    "ROOT = Path(\"..\").resolve()\n",
    "MODEL_DIR = ROOT / \"data\" / \"models\"\n",
    "FEATURES_DIR = ROOT / \"data\" / \"processed\"\n",
    "FIG_DIR = ROOT / \"logs\" / \"shap\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add project to path\n",
    "if str(ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT / \"src\"))\n",
    "\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Figures saved to : {FIG_DIR}\")\n",
    "print(f\"Models directory : {MODEL_DIR}\")\n",
    "print(f\"Features directory: {FEATURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb422c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Load trained model + features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from pipeline.classifier import SpectralClassifier\n",
    "\n",
    "# â”€â”€ Find the most recent XGBoost model â”€â”€\n",
    "xgb_pkls = sorted(MODEL_DIR.glob(\"spectral_classifier_xgboost_*.pkl\"))\n",
    "if not xgb_pkls:\n",
    "    raise FileNotFoundError(\"No XGBoost .pkl found in data/models/\")\n",
    "\n",
    "model_path = xgb_pkls[-1]  # most recent by timestamp\n",
    "meta_path = model_path.with_suffix(\"\").with_suffix(\"\") if \"_meta\" in model_path.stem else None\n",
    "meta_json = model_path.parent / (model_path.stem + \"_meta.json\")\n",
    "\n",
    "print(f\"Loading model: {model_path.name}\")\n",
    "clf: SpectralClassifier = SpectralClassifier.load_model(str(model_path))\n",
    "\n",
    "# Read metadata\n",
    "if meta_json.exists():\n",
    "    with open(meta_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    print(f\"  Model type     : {meta.get('model_type', '?')}\")\n",
    "    print(f\"  Target         : {meta.get('prediction_target', '?')}\")\n",
    "    print(f\"  Class labels   : {meta.get('class_labels', [])}\")\n",
    "    print(f\"  Trained on     : {meta.get('trained_on_file', '?')}\")\n",
    "    print(f\"  Selected feats : {len(meta.get('selected_features_', []))}\")\n",
    "else:\n",
    "    meta = {}\n",
    "    print(\"  âš  No metadata JSON found.\")\n",
    "\n",
    "# â”€â”€ Load features dataset â”€â”€\n",
    "feat_csvs = sorted(FEATURES_DIR.glob(\"features_*.csv\"))\n",
    "if not feat_csvs:\n",
    "    raise FileNotFoundError(\"No features CSV found in data/processed/\")\n",
    "\n",
    "# Use the file the model was trained on, or the most recent\n",
    "trained_file = meta.get(\"trained_on_file\", \"\")\n",
    "feat_path = None\n",
    "for fp in feat_csvs:\n",
    "    if fp.name == trained_file:\n",
    "        feat_path = fp\n",
    "        break\n",
    "if feat_path is None:\n",
    "    feat_path = feat_csvs[-1]\n",
    "    print(f\"  âš  Trained file '{trained_file}' not found, using latest: {feat_path.name}\")\n",
    "\n",
    "print(f\"\\nLoading features: {feat_path.name}\")\n",
    "df_feat = pd.read_csv(feat_path)\n",
    "print(f\"  Shape: {df_feat.shape}\")\n",
    "\n",
    "# â”€â”€ Identify feature columns & label â”€â”€\n",
    "# The pipeline was fitted on ALL candidate features (prep â†’ fs â†’ clf).\n",
    "# We must feed the full set to prep; feature selection is handled by the fs step.\n",
    "selected_feats = meta.get(\"selected_features_\", []) or getattr(clf, \"selected_features_\", None) or []\n",
    "all_candidate_feats = meta.get(\"feature_names_used\", []) or getattr(clf, \"feature_names_used\", [])\n",
    "\n",
    "# IMPORTANT: use all candidate features (pre-feature-selection) so that\n",
    "# prep.transform(X) receives every column the ColumnTransformer expects.\n",
    "use_feats = all_candidate_feats if all_candidate_feats else selected_feats\n",
    "\n",
    "# Filter to columns actually present in the CSV\n",
    "available = [c for c in use_feats if c in df_feat.columns]\n",
    "missing = [c for c in use_feats if c not in df_feat.columns]\n",
    "if missing:\n",
    "    print(f\"  âš  {len(missing)} features missing from CSV: {missing[:5]}...\")\n",
    "print(f\"  Using {len(available)}/{len(use_feats)} candidate features for SHAP analysis\")\n",
    "\n",
    "# Build target label column\n",
    "target = meta.get(\"prediction_target\", \"main_class\")\n",
    "if target == \"main_class\":\n",
    "    df_feat[\"label\"] = df_feat[\"subclass\"].astype(str).str[0]\n",
    "elif \"label\" not in df_feat.columns:\n",
    "    df_feat[\"label\"] = df_feat[\"subclass\"]\n",
    "\n",
    "# Drop rows with missing label or features\n",
    "mask_valid = df_feat[\"label\"].notna() & df_feat[available].notna().all(axis=1)\n",
    "df_valid = df_feat.loc[mask_valid].copy()\n",
    "\n",
    "X = df_valid[available].astype(np.float32)\n",
    "y = df_valid[\"label\"]\n",
    "\n",
    "print(f\"\\nâ”€â”€ Dataset Summary â”€â”€\")\n",
    "print(f\"  Samples     : {len(X):,}\")\n",
    "print(f\"  Features    : {X.shape[1]}\")\n",
    "print(f\"  Classes     : {sorted(y.unique().tolist())}\")\n",
    "print(f\"\\nâ”€â”€ Class Distribution â”€â”€\")\n",
    "print(y.value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780319ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Create SHAP TreeExplainer\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Extract the underlying pipeline and booster\n",
    "pipeline = clf.model_pipeline\n",
    "pipe_inner = getattr(pipeline, \"base_estimator\", pipeline)  # unwrap calibration if present\n",
    "\n",
    "# Get the preprocessor and classifier steps\n",
    "if hasattr(pipe_inner, \"named_steps\"):\n",
    "    prep = pipe_inner.named_steps.get(\"prep\", None)\n",
    "    fs = pipe_inner.named_steps.get(\"fs\", None)\n",
    "    estimator = pipe_inner.named_steps.get(\"clf\", None)\n",
    "    print(f\"Pipeline steps: {list(pipe_inner.named_steps.keys())}\")\n",
    "else:\n",
    "    prep, fs, estimator = None, None, pipe_inner\n",
    "    print(\"Model is not a Pipeline â€” using directly.\")\n",
    "\n",
    "# Unwrap wrappers until we reach the tree-based estimator\n",
    "# Possible chain: ThresholdTunedClassifier â†’ CalibratedClassifierCV\n",
    "#                 â†’ _Float64ProbaWrapper â†’ XGBClassifier\n",
    "TREE_TYPES = {\"XGBClassifier\", \"XGBRegressor\", \"LGBMClassifier\",\n",
    "              \"LGBMRegressor\", \"GradientBoostingClassifier\",\n",
    "              \"RandomForestClassifier\", \"DecisionTreeClassifier\"}\n",
    "\n",
    "MAX_UNWRAP = 10\n",
    "for _i in range(MAX_UNWRAP):\n",
    "    etype = type(estimator).__name__\n",
    "    if etype in TREE_TYPES:\n",
    "        break  # found the actual tree model\n",
    "\n",
    "    unwrapped = False\n",
    "    # Try common wrapper attributes in priority order\n",
    "    for attr in (\"base\", \"base_estimator\", \"estimator\"):\n",
    "        if hasattr(estimator, attr):\n",
    "            inner = getattr(estimator, attr)\n",
    "            # CalibratedClassifierCV special case: use fitted calibrators\n",
    "            if etype == \"CalibratedClassifierCV\" and hasattr(estimator, \"calibrated_classifiers_\"):\n",
    "                inner = estimator.calibrated_classifiers_[0].estimator\n",
    "            if inner is not None and inner is not estimator:\n",
    "                print(f\"  Unwrapped {etype} (.{attr}) â†’ {type(inner).__name__}\")\n",
    "                estimator = inner\n",
    "                unwrapped = True\n",
    "                break\n",
    "    if not unwrapped:\n",
    "        print(f\"  âš  Cannot unwrap {etype} further.\")\n",
    "        break\n",
    "\n",
    "print(f\"  Final estimator type: {type(estimator).__name__}\")\n",
    "\n",
    "# â”€â”€ Transform features through the preprocessing pipeline â”€â”€\n",
    "# We need the transformed data that the booster actually sees\n",
    "if prep is not None:\n",
    "    X_prep = pd.DataFrame(\n",
    "        prep.transform(X),\n",
    "        columns=prep.get_feature_names_out() if hasattr(prep, \"get_feature_names_out\") else available,\n",
    "        index=X.index,\n",
    "    )\n",
    "    print(f\"  After prep: {X_prep.shape}\")\n",
    "else:\n",
    "    X_prep = X.copy()\n",
    "\n",
    "# Apply feature selection if present\n",
    "if fs is not None:\n",
    "    X_transformed = pd.DataFrame(\n",
    "        fs.transform(X_prep),\n",
    "        index=X.index,\n",
    "    )\n",
    "    # Get selected feature names\n",
    "    if hasattr(fs, \"get_support\"):\n",
    "        selected_mask = fs.get_support()\n",
    "        transformed_cols = X_prep.columns[selected_mask].tolist()\n",
    "    else:\n",
    "        transformed_cols = [f\"f{i}\" for i in range(X_transformed.shape[1])]\n",
    "    X_transformed.columns = transformed_cols\n",
    "    print(f\"  After FS : {X_transformed.shape}\")\n",
    "else:\n",
    "    X_transformed = X_prep.copy()\n",
    "\n",
    "# â”€â”€ Create SHAP explainer â”€â”€\n",
    "# Workaround: SHAP < 0.46 can't parse XGBoost 2.x multiclass base_score vectors.\n",
    "# We monkey-patch the SHAP loader's base_score parsing line to handle arrays.\n",
    "import shap.explainers._tree as _stree\n",
    "\n",
    "_orig_loader_init = _stree.XGBTreeModelLoader.__init__\n",
    "\n",
    "def _patched_loader_init(self, xgb_model):\n",
    "    \"\"\"Handle multiclass base_score arrays from XGBoost 2.x.\"\"\"\n",
    "    import io\n",
    "    import xgboost as _xgb\n",
    "    from shap.explainers._tree import decode_ubjson_buffer\n",
    "\n",
    "    _stree._check_xgboost_version(_xgb.__version__)\n",
    "    model = xgb_model\n",
    "\n",
    "    raw = xgb_model.save_raw(raw_format=\"ubj\")\n",
    "    with io.BytesIO(raw) as fd:\n",
    "        jmodel = decode_ubjson_buffer(fd)\n",
    "\n",
    "    learner = jmodel[\"learner\"]\n",
    "    lmp = learner[\"learner_model_param\"]\n",
    "    objective = learner[\"objective\"]\n",
    "    booster = learner[\"gradient_booster\"]\n",
    "\n",
    "    n_classes = max(int(lmp[\"num_class\"]), 1)\n",
    "    n_targets = max(int(lmp[\"num_target\"]), 1)\n",
    "    n_targets = max(n_targets, n_classes)\n",
    "\n",
    "    if \"gbtree\" in booster and \"model\" not in booster:\n",
    "        booster = booster[\"gbtree\"]\n",
    "\n",
    "    if booster[\"model\"].get(\"iteration_indptr\", None) is not None:\n",
    "        iteration_indptr = np.asarray(booster[\"model\"][\"iteration_indptr\"], dtype=np.int32)\n",
    "        diff = np.diff(iteration_indptr)\n",
    "    else:\n",
    "        n_parallel_trees = int(booster[\"model\"][\"gbtree_model_param\"][\"num_parallel_tree\"])\n",
    "        diff = np.repeat(n_targets * n_parallel_trees, model.num_boosted_rounds())\n",
    "\n",
    "    if np.any(diff != diff[0]):\n",
    "        raise ValueError(\"vector-leaf is not yet supported:\", diff)\n",
    "\n",
    "    self.n_trees_per_iter = int(diff[0])\n",
    "    self.n_targets = n_targets\n",
    "\n",
    "    # â”€â”€ PATCHED: handle multiclass base_score â”€â”€\n",
    "    bs_raw = lmp[\"base_score\"]\n",
    "    if isinstance(bs_raw, str) and bs_raw.startswith(\"[\"):\n",
    "        mc_scores = [float(x) for x in bs_raw.strip(\"[]\").split(\",\")]\n",
    "        base_score_val = float(np.mean(mc_scores))\n",
    "    else:\n",
    "        base_score_val = float(bs_raw)\n",
    "\n",
    "    self.base_score = base_score_val\n",
    "    assert self.n_trees_per_iter > 0\n",
    "\n",
    "    self.name_obj = objective[\"name\"]\n",
    "    self.name_gbm = booster[\"name\"]\n",
    "\n",
    "    if self.name_obj in (\"binary:logistic\", \"reg:logistic\"):\n",
    "        import scipy.special\n",
    "        self.base_score = scipy.special.logit(base_score_val)\n",
    "    elif self.name_obj in (\"reg:gamma\", \"reg:tweedie\", \"count:poisson\",\n",
    "                            \"survival:cox\", \"survival:aft\"):\n",
    "        self.base_score = np.log(base_score_val)\n",
    "    else:\n",
    "        self.base_score = base_score_val\n",
    "\n",
    "    self.num_feature = int(lmp[\"num_feature\"])\n",
    "    self.num_class = int(lmp[\"num_class\"])\n",
    "\n",
    "    trees = booster[\"model\"][\"trees\"]\n",
    "    self.num_trees = len(trees)\n",
    "\n",
    "    self.node_parents, self.node_cleft, self.node_cright = [], [], []\n",
    "    self.node_sindex, self.children_default, self.sum_hess = [], [], []\n",
    "    self.values, self.thresholds, self.threshold_types, self.features = [], [], [], []\n",
    "    self.split_types, self.categories = [], []\n",
    "\n",
    "    feature_types = model.feature_types\n",
    "    if feature_types is not None:\n",
    "        cat_idx = np.where(np.asarray(feature_types) == \"c\")[0]\n",
    "        self.cat_feature_indices = cat_idx if len(cat_idx) > 0 else None\n",
    "    else:\n",
    "        self.cat_feature_indices = None\n",
    "\n",
    "    def to_integers(data):\n",
    "        assert isinstance(data, list)\n",
    "        return np.asanyarray(data, dtype=np.uint8)\n",
    "\n",
    "    for i in range(self.num_trees):\n",
    "        tree = trees[i]\n",
    "        self.node_parents.append(np.asarray(tree[\"parents\"]))\n",
    "        self.node_cleft.append(np.asarray(tree[\"left_children\"], dtype=np.int32))\n",
    "        self.node_cright.append(np.asarray(tree[\"right_children\"], dtype=np.int32))\n",
    "        self.node_sindex.append(np.asarray(tree[\"split_indices\"], dtype=np.uint32))\n",
    "\n",
    "        base_weight = np.asarray(tree[\"base_weights\"], dtype=np.float32)\n",
    "        if base_weight.size != self.node_cleft[-1].size:\n",
    "            raise ValueError(\"vector-leaf is not yet supported.\")\n",
    "\n",
    "        default_left = to_integers(tree[\"default_left\"])\n",
    "        default_child = np.where(\n",
    "            default_left == 1, self.node_cleft[-1], self.node_cright[-1]\n",
    "        ).astype(np.int64)\n",
    "        self.children_default.append(default_child)\n",
    "        self.sum_hess.append(np.asarray(tree[\"sum_hessian\"], dtype=np.float64))\n",
    "\n",
    "        is_leaf = self.node_cleft[-1] == -1\n",
    "        split_cond = np.asarray(tree[\"split_conditions\"], dtype=np.float32)\n",
    "        leaf_weight = np.where(is_leaf, split_cond, 0.0)\n",
    "        thresholds = np.where(is_leaf, 0.0, split_cond)\n",
    "        thresholds = np.where(is_leaf, 0.0, np.nextafter(thresholds, -np.float32(np.inf)))\n",
    "        threshold_types = np.zeros_like(thresholds, dtype=np.int32)\n",
    "\n",
    "        self.values.append(leaf_weight.reshape(leaf_weight.size, 1))\n",
    "        self.thresholds.append(thresholds)\n",
    "        self.threshold_types.append(threshold_types)\n",
    "        self.features.append(np.asarray(tree[\"split_indices\"], dtype=np.int64))\n",
    "\n",
    "        split_types = to_integers(tree[\"split_type\"])\n",
    "        self.split_types.append(split_types)\n",
    "        cat_segments = tree[\"categories_segments\"]\n",
    "        cat_sizes = tree[\"categories_sizes\"]\n",
    "        cat_nodes = tree[\"categories_nodes\"]\n",
    "        cats = tree[\"categories\"]\n",
    "        self.categories.append(\n",
    "            self.parse_categories(cat_nodes, cat_segments, cat_sizes, cats, self.node_cleft[-1])\n",
    "        )\n",
    "\n",
    "# Apply the monkey-patch\n",
    "_stree.XGBTreeModelLoader.__init__ = _patched_loader_init\n",
    "print(\"âœ” SHAP XGBTreeModelLoader patched for multiclass base_score\")\n",
    "\n",
    "# Now create the explainer\n",
    "_booster = estimator.get_booster()\n",
    "explainer = shap.TreeExplainer(_booster)\n",
    "print(f\"\\nâœ” SHAP TreeExplainer created for {type(estimator).__name__}\")\n",
    "print(f\"  Input features: {X_transformed.shape[1]}\")\n",
    "\n",
    "# â”€â”€ Compute SHAP values (subsample if dataset is large) â”€â”€\n",
    "MAX_SAMPLES = 2000\n",
    "if len(X_transformed) > MAX_SAMPLES:\n",
    "    np.random.seed(42)\n",
    "    idx_sample = np.random.choice(len(X_transformed), MAX_SAMPLES, replace=False)\n",
    "    X_shap = X_transformed.iloc[idx_sample]\n",
    "    y_shap = y.iloc[idx_sample]\n",
    "    print(f\"  Subsampled to {MAX_SAMPLES} for SHAP computation\")\n",
    "else:\n",
    "    X_shap = X_transformed\n",
    "    y_shap = y\n",
    "    print(f\"  Using all {len(X_shap)} samples\")\n",
    "\n",
    "print(\"\\nComputing SHAP values (this may take a moment)...\")\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "# For multiclass, shap_values can be:\n",
    "#   - a list of arrays (one per class): each shape (n_samples, n_features)\n",
    "#   - a 3D array (n_samples, n_features, n_classes) when using Booster directly\n",
    "if isinstance(shap_values, list):\n",
    "    n_classes = len(shap_values)\n",
    "    print(f\"  âœ” Multiclass SHAP (list): {n_classes} classes Ã— {shap_values[0].shape} each\")\n",
    "elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "    # Convert 3D array â†’ list of 2D arrays for consistent downstream handling\n",
    "    n_classes = shap_values.shape[2]\n",
    "    shap_values = [shap_values[:, :, c] for c in range(n_classes)]\n",
    "    print(f\"  âœ” Multiclass SHAP (3Dâ†’list): {n_classes} classes Ã— {shap_values[0].shape} each\")\n",
    "else:\n",
    "    # Wrap in list for consistent handling\n",
    "    shap_values = [shap_values]\n",
    "    n_classes = 1\n",
    "    print(f\"  âœ” Binary SHAP: {shap_values[0].shape}\")\n",
    "\n",
    "# Class labels from the model\n",
    "class_labels = meta.get(\"class_labels\", sorted(y.unique().tolist()))\n",
    "print(f\"  Class labels: {class_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b480e98",
   "metadata": {},
   "source": [
    "<div style=\"margin: 40px 0;\">\n",
    "    <div style=\"height: 3px; background: linear-gradient(90deg, transparent, #667eea, #764ba2, #5C7CFA, transparent); border-radius: 3px;\"></div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #764ba2 0%, #667eea 100%); padding: 20px 30px; border-radius: 12px; margin: 30px 0 20px 0; box-shadow: 0 8px 25px rgba(118, 75, 162, 0.3); border: 2px solid rgba(255, 255, 255, 0.1);\">\n",
    "    <h2 style=\"color: white; margin: 0; text-align: center; font-size: 28px; font-weight: 300; letter-spacing: 2px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">ğŸ“Š GLOBAL FEATURE IMPORTANCE</h2>\n",
    "</div>\n",
    "\n",
    "<a id=\"global\"></a>\n",
    "\n",
    "<div style=\"text-align: right; margin: 10px 0;\">\n",
    "    <a href=\"#toc\" style=\"color: #667eea; text-decoration: none; font-size: 12px;\">â¬†ï¸ Back to TOC</a>\n",
    "</div>\n",
    "\n",
    "**Goal:** Identify which features have the largest average impact on model predictions across all classes and samples. We compute the mean absolute SHAP value per feature, aggregated across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553124cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2a. Mean |SHAP| bar chart â€” Top 15 features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "feature_names = X_shap.columns.tolist()\n",
    "\n",
    "# Aggregate mean |SHAP| across all classes\n",
    "mean_abs_shap = np.mean(\n",
    "    [np.abs(sv).mean(axis=0) for sv in shap_values], axis=0\n",
    ")\n",
    "\n",
    "# Build importance DataFrame\n",
    "df_importance = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"mean_abs_shap\": mean_abs_shap,\n",
    "}).sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# â”€â”€ Categorise features by family for colour coding â”€â”€\n",
    "def feature_family(name: str) -> str:\n",
    "    n = name.lower()\n",
    "    if \"halpha\" in n or \"hÎ±\" in n or \"ha_\" in n or \"h_alpha\" in n:\n",
    "        return \"HÎ± (Balmer)\"\n",
    "    elif \"hbeta\" in n or \"hÎ²\" in n or \"hb_\" in n:\n",
    "        return \"HÎ² (Balmer)\"\n",
    "    elif \"caii\" in n or \"cahk\" in n or \"ca_\" in n or \"ca4227\" in n or \"cah2\" in n or \"cah3\" in n:\n",
    "        return \"Ca II\"\n",
    "    elif \"mg\" in n or \"mgb\" in n:\n",
    "        return \"Mg b\"\n",
    "    elif \"na_d\" in n or \"nad\" in n:\n",
    "        return \"Na D\"\n",
    "    elif \"color\" in n or \"colour\" in n or \"_gr\" in n or \"_ri\" in n or \"bluered\" in n:\n",
    "        return \"Colour\"\n",
    "    elif \"teff\" in n or \"logg\" in n or \"mh_\" in n or \"ag_\" in n or \"ebp\" in n:\n",
    "        return \"Gaia Params\"\n",
    "    elif \"snr\" in n:\n",
    "        return \"SNR\"\n",
    "    elif \"bp_\" in n or \"rp_\" in n or \"phot_\" in n or \"flux_\" in n or \"magnitude\" in n or \"m_bp\" in n or \"m_rp\" in n or \"m_g\" in n:\n",
    "        return \"Photometry\"\n",
    "    elif \"parallax\" in n or \"pm\" in n or \"radial\" in n or \"distance\" in n or \"v_tan\" in n:\n",
    "        return \"Astrometry\"\n",
    "    elif \"index_\" in n or \"dn4000\" in n or \"g4300\" in n or \"tio5\" in n:\n",
    "        return \"Spectral Index\"\n",
    "    elif \"ratio_\" in n or \"contrast\" in n or \"ew_\" in n:\n",
    "        return \"Line Ratio\"\n",
    "    elif \"redshift\" in n:\n",
    "        return \"Redshift\"\n",
    "    elif \"cont_\" in n:\n",
    "        return \"Continuum\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df_importance[\"family\"] = df_importance[\"feature\"].apply(feature_family)\n",
    "\n",
    "# Colour palette for families\n",
    "family_palette = {\n",
    "    \"HÎ± (Balmer)\": \"#e74c3c\",\n",
    "    \"HÎ² (Balmer)\": \"#ff6b6b\",\n",
    "    \"Ca II\": \"#3498db\",\n",
    "    \"Mg b\": \"#2ecc71\",\n",
    "    \"Na D\": \"#f39c12\",\n",
    "    \"Colour\": \"#9b59b6\",\n",
    "    \"Gaia Params\": \"#1abc9c\",\n",
    "    \"SNR\": \"#95a5a6\",\n",
    "    \"Photometry\": \"#e67e22\",\n",
    "    \"Astrometry\": \"#34495e\",\n",
    "    \"Spectral Index\": \"#667eea\",\n",
    "    \"Line Ratio\": \"#764ba2\",\n",
    "    \"Redshift\": \"#d35400\",\n",
    "    \"Continuum\": \"#27ae60\",\n",
    "    \"Other\": \"#7f8c8d\",\n",
    "}\n",
    "\n",
    "# â”€â”€ Plot top 15 â”€â”€\n",
    "TOP_N = 15\n",
    "df_top = df_importance.head(TOP_N).iloc[::-1]  # reverse for horizontal bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.barh(\n",
    "    range(TOP_N), df_top[\"mean_abs_shap\"],\n",
    "    color=[family_palette.get(f, \"#7f8c8d\") for f in df_top[\"family\"]],\n",
    "    edgecolor=\"white\", linewidth=0.3, alpha=0.9,\n",
    ")\n",
    "ax.set_yticks(range(TOP_N))\n",
    "ax.set_yticklabels(df_top[\"feature\"].tolist(), fontsize=10)\n",
    "ax.set_xlabel(\"Mean |SHAP Value|\", fontsize=12)\n",
    "ax.set_title(\"Global Feature Importance (Mean |SHAP|) â€” Top 15\", fontsize=14, pad=15)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Legend for families present in top 15\n",
    "families_shown = df_top[\"family\"].unique()\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=family_palette.get(f, \"#7f8c8d\"), label=f) for f in families_shown]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=9, framealpha=0.8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"global_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Print top 5 â”€â”€\n",
    "print(\"â”€â”€ Top 5 Features by Mean |SHAP| â”€â”€\")\n",
    "for i, row in df_importance.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['feature']:40s}  |SHAP| = {row['mean_abs_shap']:.4f}  [{row['family']}]\")\n",
    "\n",
    "# â”€â”€ Quick assessment â”€â”€\n",
    "physical = {\"HÎ± (Balmer)\", \"HÎ² (Balmer)\", \"Ca II\", \"Mg b\", \"Na D\", \"Colour\", \"Spectral Index\", \"Line Ratio\", \"Continuum\"}\n",
    "n_physical_top15 = df_importance.head(TOP_N)[\"family\"].isin(physical).sum()\n",
    "print(f\"\\n  Physical features in top {TOP_N}: {n_physical_top15}/{TOP_N}\")\n",
    "print(f\"  â†’ {'âœ” Model relies primarily on physics' if n_physical_top15 >= TOP_N // 2 else 'âš  Check for potential data leakage'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2b. SHAP summary beeswarm plot (aggregated)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# For the beeswarm, we average SHAP across classes (absolute)\n",
    "if len(shap_values) > 1:\n",
    "    # Average across classes for a single summary\n",
    "    shap_mean = np.mean(np.abs(np.array(shap_values)), axis=0)\n",
    "else:\n",
    "    shap_mean = np.abs(shap_values[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values if len(shap_values) > 1 else shap_values[0],\n",
    "    X_shap,\n",
    "    plot_type=\"bar\",\n",
    "    max_display=TOP_N,\n",
    "    show=False,\n",
    "    class_names=class_labels if len(shap_values) > 1 else None,\n",
    ")\n",
    "plt.title(\"SHAP Feature Importance by Class\", fontsize=14, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"summary_bar.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ” Summary bar plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f649f19",
   "metadata": {},
   "source": [
    "<div style=\"margin: 40px 0;\">\n",
    "    <div style=\"height: 3px; background: linear-gradient(90deg, transparent, #667eea, #764ba2, #5C7CFA, transparent); border-radius: 3px;\"></div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #5C7CFA 0%, #764ba2 100%); padding: 20px 30px; border-radius: 12px; margin: 30px 0 20px 0; box-shadow: 0 8px 25px rgba(92, 124, 250, 0.3); border: 2px solid rgba(255, 255, 255, 0.1);\">\n",
    "    <h2 style=\"color: white; margin: 0; text-align: center; font-size: 28px; font-weight: 300; letter-spacing: 2px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">ğŸ”¬ CLASS-SPECIFIC SHAP ANALYSIS</h2>\n",
    "</div>\n",
    "\n",
    "<a id=\"classwise\"></a>\n",
    "\n",
    "<div style=\"text-align: right; margin: 10px 0;\">\n",
    "    <a href=\"#toc\" style=\"color: #667eea; text-decoration: none; font-size: 12px;\">â¬†ï¸ Back to TOC</a>\n",
    "</div>\n",
    "\n",
    "**Goal:** Understand which features drive predictions **for each spectral class individually**. A feature that is globally important may matter only for certain classes (e.g., HÎ± FWHM for A-type stars, TiO5 for M-type stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3a. Per-class top features (heatmap)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Build per-class mean |SHAP| matrix\n",
    "class_importance = {}\n",
    "for i, label in enumerate(class_labels):\n",
    "    if i < len(shap_values):\n",
    "        class_importance[label] = np.abs(shap_values[i]).mean(axis=0)\n",
    "\n",
    "df_class_imp = pd.DataFrame(class_importance, index=feature_names)\n",
    "\n",
    "# Select features that appear in top 10 of any class\n",
    "top_per_class = set()\n",
    "for col in df_class_imp.columns:\n",
    "    top_per_class.update(df_class_imp[col].nlargest(10).index.tolist())\n",
    "\n",
    "df_heatmap = df_class_imp.loc[list(top_per_class)].copy()\n",
    "# Sort by overall importance\n",
    "df_heatmap[\"_total\"] = df_heatmap.sum(axis=1)\n",
    "df_heatmap = df_heatmap.sort_values(\"_total\", ascending=False).drop(columns=\"_total\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(8, len(df_heatmap) * 0.35)))\n",
    "sns.heatmap(\n",
    "    df_heatmap.head(20),\n",
    "    annot=True, fmt=\".3f\", cmap=\"magma\",\n",
    "    linewidths=0.5, linecolor=\"#333\",\n",
    "    xticklabels=df_heatmap.columns,\n",
    "    yticklabels=df_heatmap.head(20).index,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Mean |SHAP| by Feature Ã— Class (Top 20)\", fontsize=14, pad=15)\n",
    "ax.set_xlabel(\"Spectral Class\")\n",
    "ax.set_ylabel(\"\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"class_feature_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"â”€â”€ Per-Class Top 3 Features â”€â”€\")\n",
    "for label in class_labels:\n",
    "    if label in df_class_imp.columns:\n",
    "        top3 = df_class_imp[label].nlargest(3)\n",
    "        feats = \", \".join([f\"{n} ({v:.4f})\" for n, v in top3.items()])\n",
    "        print(f\"  {label}: {feats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89066b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3b. Beeswarm plots per class\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "n_cols = min(3, len(class_labels))\n",
    "n_rows = (len(class_labels) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(7 * n_cols, 6 * n_rows))\n",
    "if n_rows == 1 and n_cols == 1:\n",
    "    axes = np.array([axes])\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    if i >= len(shap_values):\n",
    "        break\n",
    "    plt.sca(axes[i])\n",
    "    shap.summary_plot(\n",
    "        shap_values[i], X_shap,\n",
    "        max_display=10,\n",
    "        show=False,\n",
    "        plot_size=None,\n",
    "    )\n",
    "    axes[i].set_title(f\"Class: {label}\", fontsize=13, pad=10)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(len(class_labels), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle(\"SHAP Beeswarm Plots by Spectral Class\", fontsize=16, y=1.01)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"beeswarm_per_class.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ” Per-class beeswarm plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337da568",
   "metadata": {},
   "source": [
    "**Interpretation â€” Class-Specific SHAP:**\n",
    "\n",
    "The heatmap and beeswarm plots reveal **which features the model uses to distinguish each spectral class**. Key expectations:\n",
    "- **A-type stars:** HÎ± features (FWHM, EW) should dominate due to maximum Balmer absorption.\n",
    "- **K/M-type stars:** Ca II features, TiO5, and red colour indices should be most important.\n",
    "- **F/G-type stars:** A mix of metallic lines (Mg b, Na D) and colour features.\n",
    "\n",
    "If non-physical features (SNR, fiber ID, Julian date) appear in the top ranks for any class, this warrants investigation for potential confounding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15971a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3c. Class-specific multi-panel figure (top 10 per class)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Colorblind-friendly palette for each class\n",
    "CLASS_PALETTE = {\n",
    "    \"A\": \"#4477AA\",   # blue\n",
    "    \"F\": \"#66CCEE\",   # cyan\n",
    "    \"G\": \"#228833\",   # green\n",
    "    \"K\": \"#CCBB44\",   # yellow\n",
    "    \"M\": \"#EE6677\",   # red\n",
    "    \"s\": \"#AA3377\",   # purple\n",
    "}\n",
    "\n",
    "# Helper: clean feature names for display\n",
    "def clean_feat(name: str) -> str:\n",
    "    \"\"\"Remove common prefixes for cleaner axis labels.\"\"\"\n",
    "    for prefix in (\"feature_\", \"feat_\", \"f_\"):\n",
    "        if name.lower().startswith(prefix):\n",
    "            name = name[len(prefix):]\n",
    "    return name\n",
    "\n",
    "# Filter to stellar classes only (exclude 's' if present)\n",
    "stellar_classes = [c for c in class_labels if c != \"s\"]\n",
    "n_classes = len(stellar_classes)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_classes, figsize=(4.2 * n_classes, 6), sharey=False)\n",
    "if n_classes == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "TOP_K = 10\n",
    "for idx, label in enumerate(stellar_classes):\n",
    "    ax = axes[idx]\n",
    "    cls_idx = class_labels.index(label)\n",
    "    if cls_idx >= len(shap_values):\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "\n",
    "    # Top-K features for this class\n",
    "    mean_shap_cls = np.abs(shap_values[cls_idx]).mean(axis=0)\n",
    "    top_indices = np.argsort(mean_shap_cls)[-TOP_K:][::-1]\n",
    "    top_vals = mean_shap_cls[top_indices]\n",
    "    top_names = [clean_feat(feature_names[i]) for i in top_indices]\n",
    "\n",
    "    # Horizontal bar chart â€” reversed for top-down ordering\n",
    "    bars = ax.barh(\n",
    "        range(TOP_K - 1, -1, -1), top_vals,\n",
    "        color=CLASS_PALETTE.get(label, \"#999999\"),\n",
    "        edgecolor=\"white\", linewidth=0.3, alpha=0.9,\n",
    "    )\n",
    "    ax.set_yticks(range(TOP_K - 1, -1, -1))\n",
    "    ax.set_yticklabels(top_names, fontsize=8)\n",
    "    ax.set_xlabel(\"Mean |SHAP|\", fontsize=9)\n",
    "    ax.set_title(f\"Class {label}\", fontsize=13, fontweight=\"bold\",\n",
    "                 color=CLASS_PALETTE.get(label, \"white\"), pad=10)\n",
    "    ax.grid(axis=\"x\", alpha=0.2)\n",
    "    ax.tick_params(axis=\"y\", length=0)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Top 10 Features per Spectral Class (SHAP)\",\n",
    "    fontsize=15, fontweight=\"bold\", y=1.02,\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"class_specific_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ” Class-specific multi-panel figure saved â†’ class_specific_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3d. Cross-class comparison heatmap (top 20 global â†’ per class)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Use top 20 globally-ranked features\n",
    "top20_global = df_importance.head(20)[\"feature\"].tolist()\n",
    "df_cross = df_class_imp.loc[\n",
    "    df_class_imp.index.isin(top20_global), stellar_classes\n",
    "].copy()\n",
    "\n",
    "# Sort rows by global importance (df_importance order)\n",
    "df_cross = df_cross.reindex(top20_global)\n",
    "df_cross = df_cross.dropna(how=\"all\")\n",
    "\n",
    "# Clean names for display\n",
    "display_names = [clean_feat(f) for f in df_cross.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(9, len(df_cross) * 0.42)))\n",
    "sns.heatmap(\n",
    "    df_cross.values,\n",
    "    annot=True, fmt=\".3f\", cmap=\"viridis\",\n",
    "    linewidths=0.5, linecolor=\"#333\",\n",
    "    xticklabels=stellar_classes,\n",
    "    yticklabels=display_names,\n",
    "    ax=ax,\n",
    "    cbar_kws={\"label\": \"Mean |SHAP|\", \"shrink\": 0.8},\n",
    ")\n",
    "ax.set_title(\n",
    "    \"Cross-Class Feature Importance (Top 20 Global Features)\",\n",
    "    fontsize=14, pad=15,\n",
    ")\n",
    "ax.set_xlabel(\"Spectral Class\", fontsize=12)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.tick_params(axis=\"y\", labelsize=9)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"importance_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Per-class top 3 with physics interpretation â”€â”€\n",
    "PHYSICS_HINTS = {\n",
    "    \"A\": \"Balmer lines dominate (HÎ±, HÎ² FWHM/EW) â€” consistent with A-type physics.\",\n",
    "    \"F\": \"Mix of Balmer + metallic lines expected (Ca K, Mg b begin to appear).\",\n",
    "    \"G\": \"Metallic absorption lines (Na D, Mg b, Ca) + colour indices.\",\n",
    "    \"K\": \"Ca II, Na D, molecular bands (TiO5) + red colour indices.\",\n",
    "    \"M\": \"TiO/CaH molecular bands, very red colours, Ca II triplet.\",\n",
    "}\n",
    "\n",
    "print(\"â•\" * 60)\n",
    "print(\"  PER-CLASS TOP 3 FEATURES â€” PHYSICS CONTEXT\")\n",
    "print(\"â•\" * 60)\n",
    "for label in stellar_classes:\n",
    "    if label in df_class_imp.columns:\n",
    "        top3 = df_class_imp[label].nlargest(3)\n",
    "        feats = \", \".join([f\"{clean_feat(n)} ({v:.4f})\" for n, v in top3.items()])\n",
    "        hint = PHYSICS_HINTS.get(label, \"\")\n",
    "        print(f\"\\n  Class {label}: {feats}\")\n",
    "        print(f\"    â†³ {hint}\")\n",
    "print()\n",
    "print(\"âœ” Cross-class heatmap saved â†’ importance_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a942876",
   "metadata": {},
   "source": [
    "<div style=\"margin: 40px 0;\">\n",
    "    <div style=\"height: 3px; background: linear-gradient(90deg, transparent, #667eea, #764ba2, #5C7CFA, transparent); border-radius: 3px;\"></div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #5C7CFA 100%); padding: 20px 30px; border-radius: 12px; margin: 30px 0 20px 0; box-shadow: 0 8px 25px rgba(102, 126, 234, 0.3); border: 2px solid rgba(255, 255, 255, 0.1);\">\n",
    "    <h2 style=\"color: white; margin: 0; text-align: center; font-size: 28px; font-weight: 300; letter-spacing: 2px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">ğŸ”— FEATURE INTERACTIONS</h2>\n",
    "</div>\n",
    "\n",
    "<a id=\"interactions\"></a>\n",
    "\n",
    "<div style=\"text-align: right; margin: 10px 0;\">\n",
    "    <a href=\"#toc\" style=\"color: #667eea; text-decoration: none; font-size: 12px;\">â¬†ï¸ Back to TOC</a>\n",
    "</div>\n",
    "\n",
    "**Goal:** Use SHAP dependence plots to visualise how individual feature values affect the model output, and detect **interaction effects** between features. A dependence plot shows feature value (x-axis) vs. SHAP value (y-axis), coloured by the strongest interacting feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a328d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4a. SHAP dependence plots for top features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Pick the top 4 globally important features\n",
    "top_4_feats = df_importance.head(4)[\"feature\"].tolist()\n",
    "\n",
    "# Use class 0 SHAP values (or the single array) for dependence\n",
    "sv_for_dep = shap_values[0]  # first class (usually A)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feat in enumerate(top_4_feats):\n",
    "    if feat not in X_shap.columns:\n",
    "        continue\n",
    "    feat_idx = X_shap.columns.tolist().index(feat)\n",
    "    ax = axes[i]\n",
    "    shap.dependence_plot(\n",
    "        feat_idx, sv_for_dep, X_shap,\n",
    "        interaction_index=\"auto\",\n",
    "        ax=ax, show=False,\n",
    "    )\n",
    "    ax.set_title(f\"{feat}\", fontsize=12, pad=10)\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"SHAP Dependence Plots (Class: {class_labels[0]})\",\n",
    "    fontsize=14, y=1.01,\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"dependence_top4.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ” Dependence plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4b. Feature interaction matrix (SHAP-based)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Compute approximate interaction strengths using SHAP value correlations\n",
    "# For each pair of features, measure how correlated their SHAP values are\n",
    "top_10_feats = df_importance.head(10)[\"feature\"].tolist()\n",
    "top_10_idx = [feature_names.index(f) for f in top_10_feats if f in feature_names]\n",
    "\n",
    "# Use absolute SHAP correlations across all classes as proxy for interaction\n",
    "shap_concat = np.mean([np.abs(sv) for sv in shap_values], axis=0)\n",
    "shap_top10 = shap_concat[:, top_10_idx]\n",
    "\n",
    "corr_shap = np.corrcoef(shap_top10.T)\n",
    "np.fill_diagonal(corr_shap, 0)  # remove self-correlation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(corr_shap, index=top_10_feats, columns=top_10_feats),\n",
    "    annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "    square=True, linewidths=0.5, linecolor=\"#333\",\n",
    "    vmin=-1, vmax=1, ax=ax,\n",
    ")\n",
    "ax.set_title(\"SHAP Value Correlation Matrix (Top 10 Features)\", fontsize=14, pad=15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"shap_interaction_matrix.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Identify strongest interactions\n",
    "interaction_pairs = []\n",
    "for i in range(len(top_10_feats)):\n",
    "    for j in range(i + 1, len(top_10_feats)):\n",
    "        interaction_pairs.append({\n",
    "            \"feature_1\": top_10_feats[i],\n",
    "            \"feature_2\": top_10_feats[j],\n",
    "            \"correlation\": abs(corr_shap[i, j]),\n",
    "        })\n",
    "\n",
    "df_pairs = pd.DataFrame(interaction_pairs).sort_values(\"correlation\", ascending=False)\n",
    "print(\"â”€â”€ Strongest Feature Interactions (|SHAP correlation|) â”€â”€\")\n",
    "for _, row in df_pairs.head(5).iterrows():\n",
    "    print(f\"  {row['feature_1']} Ã— {row['feature_2']}: {row['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84338fc8",
   "metadata": {},
   "source": [
    "<div style=\"margin: 40px 0;\">\n",
    "    <div style=\"height: 3px; background: linear-gradient(90deg, transparent, #667eea, #764ba2, #5C7CFA, transparent); border-radius: 3px;\"></div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #764ba2 0%, #5C7CFA 100%); padding: 20px 30px; border-radius: 12px; margin: 30px 0 20px 0; box-shadow: 0 8px 25px rgba(118, 75, 162, 0.3); border: 2px solid rgba(255, 255, 255, 0.1);\">\n",
    "    <h2 style=\"color: white; margin: 0; text-align: center; font-size: 28px; font-weight: 300; letter-spacing: 2px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">ğŸ’¡ DISCOVERY SYNTHESIS</h2>\n",
    "</div>\n",
    "\n",
    "<a id=\"discovery\"></a>\n",
    "\n",
    "<div style=\"text-align: right; margin: 10px 0;\">\n",
    "    <a href=\"#toc\" style=\"color: #667eea; text-decoration: none; font-size: 12px;\">â¬†ï¸ Back to TOC</a>\n",
    "</div>\n",
    "\n",
    "**Goal:** Assess whether the model's feature reliance is physically justified. We categorise features as **physical** (spectroscopic lines, colour indices, spectral indices) vs. **observational** (SNR, seeing, fiber ID, metadata) and quantify how much SHAP importance falls into each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa21f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5a. Physical vs. Observational importance breakdown\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Categorise all features\n",
    "PHYSICAL_FAMILIES = {\n",
    "    \"HÎ± (Balmer)\", \"HÎ² (Balmer)\", \"Ca II\", \"Mg b\", \"Na D\",\n",
    "    \"Colour\", \"Spectral Index\", \"Line Ratio\", \"Continuum\",\n",
    "}\n",
    "OBSERVATIONAL_FAMILIES = {\"SNR\", \"Other\"}\n",
    "GAIA_FAMILIES = {\"Gaia Params\", \"Photometry\", \"Astrometry\", \"Redshift\"}\n",
    "\n",
    "df_importance[\"category\"] = df_importance[\"family\"].apply(\n",
    "    lambda f: \"Physical (spectroscopic)\" if f in PHYSICAL_FAMILIES\n",
    "    else \"Gaia / Photometric\" if f in GAIA_FAMILIES\n",
    "    else \"Observational / Metadata\"\n",
    ")\n",
    "\n",
    "# Aggregate SHAP by category\n",
    "cat_shap = df_importance.groupby(\"category\")[\"mean_abs_shap\"].agg([\"sum\", \"count\", \"mean\"])\n",
    "cat_shap[\"pct\"] = cat_shap[\"sum\"] / cat_shap[\"sum\"].sum() * 100\n",
    "cat_shap = cat_shap.sort_values(\"sum\", ascending=False)\n",
    "\n",
    "print(\"â”€â”€ SHAP Importance by Category â”€â”€\")\n",
    "print(cat_shap.to_string())\n",
    "\n",
    "# â”€â”€ Donut chart â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Left: pie by category\n",
    "cat_colors = {\n",
    "    \"Physical (spectroscopic)\": \"#667eea\",\n",
    "    \"Gaia / Photometric\": \"#764ba2\",\n",
    "    \"Observational / Metadata\": \"#95a5a6\",\n",
    "}\n",
    "colors = [cat_colors.get(c, \"#7f8c8d\") for c in cat_shap.index]\n",
    "\n",
    "wedges, texts, autotexts = axes[0].pie(\n",
    "    cat_shap[\"sum\"],\n",
    "    labels=cat_shap.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    pctdistance=0.75,\n",
    "    wedgeprops=dict(width=0.4, edgecolor=\"white\", linewidth=2),\n",
    ")\n",
    "for t in texts + autotexts:\n",
    "    t.set_fontsize(10)\n",
    "axes[0].set_title(\"SHAP Importance by Feature Category\", fontsize=13, pad=15)\n",
    "\n",
    "# Right: family breakdown for physical features only\n",
    "df_phys = df_importance[df_importance[\"category\"] == \"Physical (spectroscopic)\"]\n",
    "family_shap = df_phys.groupby(\"family\")[\"mean_abs_shap\"].sum().sort_values(ascending=True)\n",
    "\n",
    "axes[1].barh(\n",
    "    range(len(family_shap)), family_shap.values,\n",
    "    color=[family_palette.get(f, \"#7f8c8d\") for f in family_shap.index],\n",
    "    edgecolor=\"white\", linewidth=0.3,\n",
    ")\n",
    "axes[1].set_yticks(range(len(family_shap)))\n",
    "axes[1].set_yticklabels(family_shap.index, fontsize=10)\n",
    "axes[1].set_xlabel(\"Cumulative Mean |SHAP|\", fontsize=11)\n",
    "axes[1].set_title(\"Physical Feature Families\", fontsize=13, pad=15)\n",
    "axes[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"physical_vs_observational.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Interpretation â”€â”€\n",
    "phys_pct = cat_shap.loc[\"Physical (spectroscopic)\", \"pct\"] if \"Physical (spectroscopic)\" in cat_shap.index else 0\n",
    "gaia_pct = cat_shap.loc[\"Gaia / Photometric\", \"pct\"] if \"Gaia / Photometric\" in cat_shap.index else 0\n",
    "obs_pct = cat_shap.loc[\"Observational / Metadata\", \"pct\"] if \"Observational / Metadata\" in cat_shap.index else 0\n",
    "\n",
    "print(f\"\\nâ”€â”€ Interpretation â”€â”€\")\n",
    "print(f\"  Physical (spectroscopic) features account for {phys_pct:.1f}% of total SHAP importance.\")\n",
    "print(f\"  Gaia / Photometric features account for {gaia_pct:.1f}%.\")\n",
    "print(f\"  Observational metadata accounts for {obs_pct:.1f}%.\")\n",
    "if obs_pct > 20:\n",
    "    print(\"  âš  Observational features contribute >20% â€” investigate potential data leakage.\")\n",
    "else:\n",
    "    print(\"  âœ” Model decisions are primarily grounded in physical features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5b. Feature importance: SHAP vs. XGBoost native\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Get XGBoost native feature importance for comparison\n",
    "native_imp = {}\n",
    "if hasattr(estimator, \"feature_importances_\"):\n",
    "    native_imp_vals = estimator.feature_importances_\n",
    "    native_feat_names = X_transformed.columns.tolist()\n",
    "    for name, val in zip(native_feat_names, native_imp_vals):\n",
    "        native_imp[name] = val\n",
    "\n",
    "if native_imp:\n",
    "    df_compare = df_importance.head(20).copy()\n",
    "    df_compare[\"native_importance\"] = df_compare[\"feature\"].map(native_imp).fillna(0)\n",
    "    df_compare[\"shap_rank\"] = range(1, len(df_compare) + 1)\n",
    "    df_compare[\"native_rank\"] = df_compare[\"native_importance\"].rank(ascending=False).astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(\n",
    "        df_compare[\"shap_rank\"], df_compare[\"native_rank\"],\n",
    "        s=80, c=[family_palette.get(f, \"#7f8c8d\") for f in df_compare[\"family\"]],\n",
    "        edgecolors=\"white\", linewidth=0.5, alpha=0.8, zorder=5,\n",
    "    )\n",
    "    # Add feature labels\n",
    "    for _, row in df_compare.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"feature\"][:20],\n",
    "            (row[\"shap_rank\"], row[\"native_rank\"]),\n",
    "            fontsize=7, alpha=0.8,\n",
    "            xytext=(5, 5), textcoords=\"offset points\",\n",
    "        )\n",
    "    # Perfect agreement line\n",
    "    max_rank = len(df_compare)\n",
    "    ax.plot([1, max_rank], [1, max_rank], \"w--\", alpha=0.3, label=\"Perfect agreement\")\n",
    "    ax.set_xlabel(\"SHAP Rank\", fontsize=12)\n",
    "    ax.set_ylabel(\"XGBoost Native Rank\", fontsize=12)\n",
    "    ax.set_title(\"Feature Ranking: SHAP vs. XGBoost Native Importance\", fontsize=13, pad=15)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.invert_xaxis()\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / \"shap_vs_native_rank.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Spearman rank correlation\n",
    "    from scipy import stats\n",
    "    rho_rank, p_rank = stats.spearmanr(df_compare[\"shap_rank\"], df_compare[\"native_rank\"])\n",
    "    print(f\"Spearman Ï(SHAP rank, native rank) = {rho_rank:+.3f}  (p = {p_rank:.2e})\")\n",
    "    print(f\"  â†’ {'Strong agreement' if rho_rank > 0.7 else 'Moderate agreement' if rho_rank > 0.4 else 'Weak agreement'} between SHAP and native importance\")\n",
    "else:\n",
    "    print(\"âš  No native feature importances available â€” skipping comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2141f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5c. Discovery detection â€” expected vs. unexpected features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Define features expected from stellar physics literature\n",
    "EXPECTED_PHYSICS = {\n",
    "    \"halpha\", \"hbeta\", \"h_alpha\", \"h_beta\", \"ca_ii\", \"ca_k\", \"ca_h\",\n",
    "    \"mg_b\", \"na_d\", \"tio5\", \"cah\", \"balmer\", \"ew_\", \"fwhm_\",\n",
    "    \"colour\", \"color\", \"bp_rp\", \"g_rp\", \"bp_g\",\n",
    "    \"teff\", \"logg\", \"feh\", \"fe_h\",\n",
    "    \"spectral_index\", \"line_ratio\",\n",
    "}\n",
    "\n",
    "def is_expected(feature_name: str) -> bool:\n",
    "    \"\"\"Check if a feature matches known stellar physics features.\"\"\"\n",
    "    fname_low = feature_name.lower()\n",
    "    return any(exp in fname_low for exp in EXPECTED_PHYSICS)\n",
    "\n",
    "# Classify top 30 features\n",
    "df_top30 = df_importance.head(30).copy()\n",
    "df_top30[\"expected\"] = df_top30[\"feature\"].apply(is_expected)\n",
    "df_top30[\"status\"] = df_top30[\"expected\"].map({True: \"âœ” Expected\", False: \"âš  Unexpected\"})\n",
    "\n",
    "# Separate\n",
    "df_expected = df_top30[df_top30[\"expected\"]]\n",
    "df_unexpected = df_top30[~df_top30[\"expected\"]]\n",
    "\n",
    "print(\"â•\" * 60)\n",
    "print(\"  DISCOVERY DETECTION â€” TOP 30 FEATURES\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"\\n  âœ” Expected features (physics-based): {len(df_expected)}/30\")\n",
    "print(f\"  âš  Unexpected features:               {len(df_unexpected)}/30\")\n",
    "\n",
    "if len(df_unexpected) > 0:\n",
    "    print(\"\\n  â”€â”€ Unexpected Features & Hypotheses â”€â”€\")\n",
    "    for _, row in df_unexpected.iterrows():\n",
    "        fname = row[\"feature\"]\n",
    "        family = row[\"family\"]\n",
    "        shap_val = row[\"mean_abs_shap\"]\n",
    "        # Generate hypothesis based on family\n",
    "        if family in (\"SNR\", \"Other\"):\n",
    "            hypothesis = \"Possible observational confound or data quality proxy\"\n",
    "        elif family in (\"Astrometry\", \"Photometry\"):\n",
    "            hypothesis = \"May encode distance/extinction as secondary physics proxy\"\n",
    "        elif family == \"Gaia Params\":\n",
    "            hypothesis = \"Gaia parameters may correlate with stellar type through selection effects\"\n",
    "        elif family == \"Redshift\":\n",
    "            hypothesis = \"Radial velocity may encode stellar population membership\"\n",
    "        else:\n",
    "            hypothesis = \"Feature may capture non-obvious spectral information â€” investigate\"\n",
    "        print(f\"    â€¢ {clean_feat(fname):30s} |SHAP|={shap_val:.4f}  [{family}]\")\n",
    "        print(f\"      â†³ Hypothesis: {hypothesis}\")\n",
    "else:\n",
    "    print(\"\\n  âœ” All top 30 features are consistent with known stellar physics.\")\n",
    "\n",
    "# Build discoveries DataFrame\n",
    "discoveries = []\n",
    "for _, row in df_unexpected.iterrows():\n",
    "    discoveries.append({\n",
    "        \"feature\": row[\"feature\"],\n",
    "        \"family\": row[\"family\"],\n",
    "        \"mean_abs_shap\": row[\"mean_abs_shap\"],\n",
    "        \"global_rank\": df_importance[df_importance[\"feature\"] == row[\"feature\"]].index[0] + 1,\n",
    "        \"type\": \"unexpected\",\n",
    "    })\n",
    "for _, row in df_expected.iterrows():\n",
    "    discoveries.append({\n",
    "        \"feature\": row[\"feature\"],\n",
    "        \"family\": row[\"family\"],\n",
    "        \"mean_abs_shap\": row[\"mean_abs_shap\"],\n",
    "        \"global_rank\": df_importance[df_importance[\"feature\"] == row[\"feature\"]].index[0] + 1,\n",
    "        \"type\": \"expected\",\n",
    "    })\n",
    "df_discoveries = pd.DataFrame(discoveries)\n",
    "print(f\"\\n  Discovery DataFrame: {len(df_discoveries)} features classified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438352c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5d. Feature interaction values (SHAP interaction matrix)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Subsample for interaction computation (expensive: O(n * d^2))\n",
    "INTERACTION_SAMPLES = min(1000, len(X_shap))\n",
    "rng_int = np.random.RandomState(42)\n",
    "int_idx = rng_int.choice(len(X_shap), INTERACTION_SAMPLES, replace=False)\n",
    "X_int = X_shap.iloc[int_idx] if hasattr(X_shap, \"iloc\") else X_shap[int_idx]\n",
    "\n",
    "print(f\"Computing SHAP interaction values on {INTERACTION_SAMPLES} samples...\")\n",
    "print(\"  (this may take a few minutes for large feature sets)\")\n",
    "\n",
    "try:\n",
    "    shap_interaction = explainer.shap_interaction_values(X_int)\n",
    "\n",
    "    # For multiclass, average across classes\n",
    "    if isinstance(shap_interaction, list):\n",
    "        # Stack all classes and take mean of absolute values\n",
    "        interaction_3d = np.mean([np.abs(sv) for sv in shap_interaction], axis=0)\n",
    "    else:\n",
    "        interaction_3d = np.abs(shap_interaction)\n",
    "\n",
    "    # Average across samples â†’ (n_features, n_features)\n",
    "    interaction_matrix = interaction_3d.mean(axis=0)\n",
    "\n",
    "    # Zero out diagonal (self-interactions are just main effects)\n",
    "    np.fill_diagonal(interaction_matrix, 0)\n",
    "\n",
    "    # Top 10 features by main-effect SHAP for the heatmap\n",
    "    top10_idx = np.argsort(df_importance.head(10)[\"feature\"].apply(\n",
    "        lambda f: list(feature_names).index(f) if f in feature_names else -1\n",
    "    ).values)\n",
    "    top10_feat_idx = [\n",
    "        list(feature_names).index(f)\n",
    "        for f in df_importance.head(10)[\"feature\"]\n",
    "        if f in feature_names\n",
    "    ]\n",
    "\n",
    "    int_sub = interaction_matrix[np.ix_(top10_feat_idx, top10_feat_idx)]\n",
    "    int_labels = [clean_feat(feature_names[i]) for i in top10_feat_idx]\n",
    "\n",
    "    # â”€â”€ Heatmap â”€â”€\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    sns.heatmap(\n",
    "        int_sub,\n",
    "        annot=True, fmt=\".4f\", cmap=\"magma\",\n",
    "        linewidths=0.5, linecolor=\"#333\",\n",
    "        xticklabels=int_labels,\n",
    "        yticklabels=int_labels,\n",
    "        ax=ax,\n",
    "        cbar_kws={\"label\": \"Mean |SHAP Interaction|\", \"shrink\": 0.8},\n",
    "    )\n",
    "    ax.set_title(\n",
    "        \"Feature Interaction Values (Top 10 Features)\",\n",
    "        fontsize=14, pad=15,\n",
    "    )\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(FIG_DIR / \"feature_interactions.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # â”€â”€ Top 5 interaction pairs â”€â”€\n",
    "    n_feat = len(top10_feat_idx)\n",
    "    pairs = []\n",
    "    for i in range(n_feat):\n",
    "        for j in range(i + 1, n_feat):\n",
    "            pairs.append((int_labels[i], int_labels[j], int_sub[i, j]))\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\nâ”€â”€ Top 5 Feature Interaction Pairs â”€â”€\")\n",
    "    for rank, (f1, f2, val) in enumerate(pairs[:5], 1):\n",
    "        print(f\"  {rank}. {f1} Ã— {f2}  â†’  mean |interaction| = {val:.5f}\")\n",
    "\n",
    "    INTERACTION_COMPUTED = True\n",
    "    print(\"\\nâœ” Interaction heatmap saved â†’ feature_interactions.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš  SHAP interaction values failed: {e}\")\n",
    "    print(\"  Falling back to correlation-based interaction proxy (already computed).\")\n",
    "    INTERACTION_COMPUTED = False\n",
    "    interaction_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5e. Artifact vs. Physics verdict & discoveries export\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ Artifact ratio analysis â”€â”€\n",
    "n_total_feats = len(df_importance)\n",
    "n_obs_feats = len(df_importance[df_importance[\"category\"] == \"Observational / Metadata\"])\n",
    "obs_in_top10 = len(df_importance.head(10)[df_importance.head(10)[\"category\"] == \"Observational / Metadata\"])\n",
    "obs_in_top20 = len(df_importance.head(20)[df_importance.head(20)[\"category\"] == \"Observational / Metadata\"])\n",
    "\n",
    "print(\"â•\" * 60)\n",
    "print(\"  ARTIFACT vs. PHYSICS ANALYSIS\")\n",
    "print(\"â•\" * 60)\n",
    "print(f\"\\n  Total features:                {n_total_feats}\")\n",
    "print(f\"  Observational features:        {n_obs_feats}/{n_total_feats}\")\n",
    "print(f\"  Observational in top 10:       {obs_in_top10}/10\")\n",
    "print(f\"  Observational in top 20:       {obs_in_top20}/20\")\n",
    "print(f\"  Observational SHAP share:      {obs_pct:.1f}%\")\n",
    "\n",
    "# â”€â”€ Verdict â”€â”€\n",
    "print(\"\\n  â”€â”€ VERDICT â”€â”€\")\n",
    "if obs_pct < 5:\n",
    "    verdict = \"EXCELLENT â€” Model is overwhelmingly physics-driven.\"\n",
    "    verdict_emoji = \"âœ”âœ”\"\n",
    "elif obs_pct < 15:\n",
    "    verdict = \"GOOD â€” Model is primarily physics-driven with minor observational contribution.\"\n",
    "    verdict_emoji = \"âœ”\"\n",
    "elif obs_pct < 25:\n",
    "    verdict = \"ACCEPTABLE â€” Non-negligible observational contribution; monitor for confounds.\"\n",
    "    verdict_emoji = \"âš \"\n",
    "else:\n",
    "    verdict = \"CONCERNING â€” Significant observational reliance; investigate data leakage.\"\n",
    "    verdict_emoji = \"âŒ\"\n",
    "\n",
    "print(f\"  {verdict_emoji} {verdict}\")\n",
    "\n",
    "if obs_in_top10 > 0:\n",
    "    obs_top10_names = df_importance.head(10)[\n",
    "        df_importance.head(10)[\"category\"] == \"Observational / Metadata\"\n",
    "    ][\"feature\"].tolist()\n",
    "    print(f\"\\n  âš  Observational features in Top 10: {', '.join(obs_top10_names)}\")\n",
    "    print(\"    These features may encode indirect physical information or represent confounds.\")\n",
    "else:\n",
    "    print(\"\\n  âœ” No observational features in the top 10 â€” no immediate leakage concern.\")\n",
    "\n",
    "# â”€â”€ Quantify physics vs artifacts per class â”€â”€\n",
    "print(\"\\n  â”€â”€ Per-Class Physics Fraction â”€â”€\")\n",
    "for label in stellar_classes:\n",
    "    if label in df_class_imp.columns:\n",
    "        cls_imp = df_class_imp[label].sort_values(ascending=False)\n",
    "        cls_top10 = cls_imp.head(10)\n",
    "        cls_top10_phys = sum(\n",
    "            1 for f in cls_top10.index\n",
    "            if df_importance.loc[df_importance[\"feature\"] == f, \"category\"].values[0]\n",
    "            in (\"Physical (spectroscopic)\", \"Gaia / Photometric\")\n",
    "            if f in df_importance[\"feature\"].values\n",
    "        )\n",
    "        print(f\"    Class {label}: {cls_top10_phys}/10 top features are physics-based\")\n",
    "\n",
    "# â”€â”€ Save discoveries â”€â”€\n",
    "df_discoveries[\"verdict\"] = verdict\n",
    "df_discoveries.to_csv(FIG_DIR / \"discoveries.csv\", index=False)\n",
    "\n",
    "print(f\"\\n  âœ” Discoveries saved â†’ discoveries.csv ({len(df_discoveries)} entries)\")\n",
    "print(\"â•\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1964a9",
   "metadata": {},
   "source": [
    "<div style=\"margin: 40px 0;\">\n",
    "    <div style=\"height: 3px; background: linear-gradient(90deg, transparent, #667eea, #764ba2, #5C7CFA, transparent); border-radius: 3px;\"></div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #5C7CFA 0%, #667eea 100%); padding: 20px 30px; border-radius: 12px; margin: 30px 0 20px 0; box-shadow: 0 8px 25px rgba(92, 124, 250, 0.3); border: 2px solid rgba(255, 255, 255, 0.1);\">\n",
    "    <h2 style=\"color: white; margin: 0; text-align: center; font-size: 28px; font-weight: 300; letter-spacing: 2px; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">ğŸ“‹ SUMMARY & COMPOSITE FIGURE</h2>\n",
    "</div>\n",
    "\n",
    "<a id=\"summary\"></a>\n",
    "\n",
    "<div style=\"text-align: right; margin: 10px 0;\">\n",
    "    <a href=\"#toc\" style=\"color: #667eea; text-decoration: none; font-size: 12px;\">â¬†ï¸ Back to TOC</a>\n",
    "</div>\n",
    "\n",
    "## Publication-Quality Composite Figure\n",
    "\n",
    "A single \"hero\" figure summarising the SHAP analysis, suitable for README, paper, or presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3778d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FIGURE 2 â€” Enhanced Publication-Quality SHAP Composite\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Layout:  (a) Global importance     | (b) Class-specific small multiples\n",
    "#          (c) Interaction heatmap   | (d) Physics vs Artifacts summary\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16), constrained_layout=False)\n",
    "gs = GridSpec(2, 2, figure=fig, hspace=0.35, wspace=0.30,\n",
    "              left=0.06, right=0.97, top=0.93, bottom=0.04)\n",
    "\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# (a) Global importance bar chart â€” top 15\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ax_a = fig.add_subplot(gs[0, 0])\n",
    "df_top_plot = df_importance.head(15).iloc[::-1]\n",
    "bars_a = ax_a.barh(\n",
    "    range(15), df_top_plot[\"mean_abs_shap\"],\n",
    "    color=[family_palette.get(f, \"#7f8c8d\") for f in df_top_plot[\"family\"]],\n",
    "    edgecolor=\"white\", linewidth=0.3, alpha=0.9,\n",
    ")\n",
    "ax_a.set_yticks(range(15))\n",
    "ax_a.set_yticklabels([clean_feat(f) for f in df_top_plot[\"feature\"]], fontsize=8)\n",
    "ax_a.set_xlabel(\"Mean |SHAP|\", fontsize=10)\n",
    "ax_a.set_title(\"(a) Global Feature Importance â€” Top 15\", fontsize=12, pad=10)\n",
    "ax_a.grid(axis=\"x\", alpha=0.2)\n",
    "\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# (b) Class-specific small multiples (A, F, G, K, M)\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "gs_b = gs[0, 1].subgridspec(1, len(stellar_classes), wspace=0.35)\n",
    "TOP_COMPOSITE = 5  # fewer features for compact display\n",
    "\n",
    "for idx, label in enumerate(stellar_classes):\n",
    "    ax_bi = fig.add_subplot(gs_b[0, idx])\n",
    "    cls_idx = class_labels.index(label)\n",
    "    if cls_idx < len(shap_values):\n",
    "        mean_cls = np.abs(shap_values[cls_idx]).mean(axis=0)\n",
    "        top_idx = np.argsort(mean_cls)[-TOP_COMPOSITE:][::-1]\n",
    "        top_vals = mean_cls[top_idx]\n",
    "        top_names = [clean_feat(feature_names[i]) for i in top_idx]\n",
    "\n",
    "        ax_bi.barh(\n",
    "            range(TOP_COMPOSITE - 1, -1, -1), top_vals,\n",
    "            color=CLASS_PALETTE.get(label, \"#999\"),\n",
    "            edgecolor=\"white\", linewidth=0.2, alpha=0.9,\n",
    "        )\n",
    "        ax_bi.set_yticks(range(TOP_COMPOSITE - 1, -1, -1))\n",
    "        ax_bi.set_yticklabels(top_names, fontsize=6)\n",
    "        ax_bi.tick_params(axis=\"x\", labelsize=6)\n",
    "        ax_bi.set_title(label, fontsize=11, fontweight=\"bold\",\n",
    "                        color=CLASS_PALETTE.get(label, \"white\"))\n",
    "        ax_bi.grid(axis=\"x\", alpha=0.15)\n",
    "    else:\n",
    "        ax_bi.set_visible(False)\n",
    "\n",
    "# Panel title\n",
    "fig.text(0.75, 0.935, \"(b) Class-Specific Top Features\", ha=\"center\",\n",
    "         fontsize=12, fontweight=\"normal\")\n",
    "\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# (c) Feature interaction heatmap (top 10 Ã— 10)\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ax_c = fig.add_subplot(gs[1, 0])\n",
    "if INTERACTION_COMPUTED and interaction_matrix is not None:\n",
    "    sns.heatmap(\n",
    "        int_sub,\n",
    "        annot=True, fmt=\".3f\", cmap=\"magma\",\n",
    "        linewidths=0.3, linecolor=\"#333\",\n",
    "        xticklabels=int_labels,\n",
    "        yticklabels=int_labels,\n",
    "        ax=ax_c,\n",
    "        annot_kws={\"fontsize\": 7},\n",
    "        cbar_kws={\"shrink\": 0.7},\n",
    "    )\n",
    "    ax_c.tick_params(axis=\"x\", rotation=45, labelsize=7)\n",
    "    ax_c.tick_params(axis=\"y\", labelsize=7)\n",
    "    ax_c.set_title(\"(c) Feature Interactions (Top 10 Ã— 10)\", fontsize=12, pad=10)\n",
    "else:\n",
    "    # Fallback: use cross-class heatmap\n",
    "    hm_data = df_heatmap.head(10)\n",
    "    sns.heatmap(\n",
    "        hm_data, annot=True, fmt=\".3f\", cmap=\"magma\",\n",
    "        linewidths=0.3, linecolor=\"#333\", ax=ax_c,\n",
    "        annot_kws={\"fontsize\": 7}, cbar_kws={\"shrink\": 0.7},\n",
    "    )\n",
    "    ax_c.set_ylabel(\"\")\n",
    "    ax_c.set_title(\"(c) Feature Ã— Class Importance (Top 10)\", fontsize=12, pad=10)\n",
    "\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "# (d) Summary table â€” Physics vs Artifacts\n",
    "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ax_d = fig.add_subplot(gs[1, 1])\n",
    "ax_d.axis(\"off\")\n",
    "\n",
    "# Build comprehensive table\n",
    "table_data = [\n",
    "    [\"Features analysed\", str(len(feature_names))],\n",
    "    [\"Samples analysed\", f\"{len(X_shap):,}\"],\n",
    "    [\"Classes\", \", \".join(class_labels)],\n",
    "    [\"Top global feature\", clean_feat(df_importance.iloc[0][\"feature\"])],\n",
    "    [\"\", \"\"],\n",
    "    [\"Physical (spectroscopic)\", f\"{phys_pct:.1f}%\"],\n",
    "    [\"Gaia / Photometric\", f\"{gaia_pct:.1f}%\"],\n",
    "    [\"Observational / Metadata\", f\"{obs_pct:.1f}%\"],\n",
    "    [\"\", \"\"],\n",
    "    [\"Obs. features in top 10\", f\"{obs_in_top10}/10\"],\n",
    "    [\"Unexpected in top 30\", f\"{len(df_unexpected)}/30\"],\n",
    "    [\"Verdict\", verdict_emoji + \" \" + verdict.split(\"â€”\")[0].strip()],\n",
    "]\n",
    "\n",
    "table = ax_d.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=[\"Metric\", \"Value\"],\n",
    "    cellLoc=\"center\",\n",
    "    loc=\"center\",\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.0, 1.6)\n",
    "\n",
    "# Style table\n",
    "for j in range(2):\n",
    "    cell = table[0, j]\n",
    "    cell.set_facecolor(\"#2a2a2a\")\n",
    "    cell.set_text_props(color=\"white\", fontweight=\"bold\")\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    for j in range(2):\n",
    "        cell = table[i, j]\n",
    "        # Highlight verdict row\n",
    "        if table_data[i - 1][0] == \"Verdict\":\n",
    "            cell.set_facecolor(\"#1e3a5f\")\n",
    "        elif table_data[i - 1][0] == \"\":\n",
    "            cell.set_facecolor(\"#111111\")\n",
    "            cell.set_edgecolor(\"#111111\")\n",
    "        else:\n",
    "            cell.set_facecolor(\"#1a1a1a\")\n",
    "        cell.set_text_props(color=\"white\")\n",
    "\n",
    "ax_d.set_title(\"(d) Physics vs. Artifacts Summary\", fontsize=12, pad=15)\n",
    "\n",
    "# â”€â”€ Super-title â”€â”€\n",
    "fig.suptitle(\n",
    "    \"FIGURE 2 â€” SHAP Interpretability: Physics-Based ML for Stellar Classification\",\n",
    "    fontsize=16, fontweight=\"bold\", y=0.98,\n",
    ")\n",
    "fig.text(\n",
    "    0.5, 0.005,\n",
    "    \"AstroSpectro â€” LAMOST DR5 Analysis  â€¢  XGBoost / TreeSHAP\",\n",
    "    ha=\"center\", fontsize=9, style=\"italic\", alpha=0.6,\n",
    ")\n",
    "\n",
    "fig.savefig(\n",
    "    FIG_DIR / \"FIGURE_2_shap_composite.png\",\n",
    "    dpi=300, bbox_inches=\"tight\", facecolor=fig.get_facecolor(),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ” Enhanced hero figure saved â†’ {(FIG_DIR / 'FIGURE_2_shap_composite.png').resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a163da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Final SHAP analysis report\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"        SHAP INTERPRETABILITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"  Model             : {meta.get('model_type', 'XGBoost')}\")\n",
    "print(f\"  Target            : {meta.get('prediction_target', '?')}\")\n",
    "print(f\"  Features analysed : {len(feature_names)}\")\n",
    "print(f\"  Samples analysed  : {len(X_shap):,}\")\n",
    "print(f\"  Classes           : {class_labels}\")\n",
    "print()\n",
    "print(\"â”€â”€ Global Top 5 â”€â”€\")\n",
    "for i, row in df_importance.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['feature']:40s} |SHAP| = {row['mean_abs_shap']:.4f}  [{row['family']}]\")\n",
    "print()\n",
    "print(\"â”€â”€ Category Breakdown â”€â”€\")\n",
    "print(f\"  Physical (spectroscopic) : {phys_pct:.1f}%\")\n",
    "print(f\"  Gaia / Photometric       : {gaia_pct:.1f}%\")\n",
    "print(f\"  Observational / Metadata : {obs_pct:.1f}%\")\n",
    "print()\n",
    "if obs_pct < 20:\n",
    "    print(\"  âœ” Model relies primarily on physically meaningful features.\")\n",
    "    print(\"  âœ” No evidence of data leakage through observational metadata.\")\n",
    "else:\n",
    "    print(\"  âš  Significant observational feature reliance â€” investigate further.\")\n",
    "print()\n",
    "print(\"â”€\" * 60)\n",
    "print(\"  References:\")\n",
    "print(\"  â€¢ Lundberg & Lee (2017) â€” NeurIPS, SHAP framework\")\n",
    "print(\"  â€¢ Lundberg et al. (2020) â€” Nature Machine Intelligence\")\n",
    "print(\"  â€¢ Luo et al. (2015) â€” RAA 15, 1095 (LAMOST DR5)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# â”€â”€ Save importance CSV â”€â”€\n",
    "df_importance.to_csv(FIG_DIR / \"shap_feature_importance.csv\", index=False)\n",
    "print(f\"\\nâœ” Feature importance saved to {(FIG_DIR / 'shap_feature_importance.csv').resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe716",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### End of Notebook\n",
    "\n",
    "All SHAP outputs have been saved to `../logs/shap/`:\n",
    "\n",
    "| File | Description |\n",
    "|---|---|\n",
    "| `global_importance.png` | Top 15 features by mean \\|SHAP\\|, colour-coded by family |\n",
    "| `summary_bar.png` | SHAP library summary bar plot by class |\n",
    "| `class_feature_heatmap.png` | Per-class mean \\|SHAP\\| heatmap |\n",
    "| `beeswarm_per_class.png` | Beeswarm plots for each spectral class |\n",
    "| `class_specific_importance.png` | Multi-panel top 10 per class (A/F/G/K/M) |\n",
    "| `importance_heatmap.png` | Cross-class comparison heatmap (top 20 global â†’ per class) |\n",
    "| `dependence_top4.png` | SHAP dependence plots for top 4 features |\n",
    "| `shap_interaction_matrix.png` | SHAP value correlation matrix (interactions) |\n",
    "| `feature_interactions.png` | SHAP interaction values heatmap (top 10 Ã— 10) |\n",
    "| `physical_vs_observational.png` | Category breakdown: physical vs. observational |\n",
    "| `shap_vs_native_rank.png` | SHAP rank vs. XGBoost native importance rank |\n",
    "| `shap_feature_importance.csv` | Machine-readable importance table |\n",
    "| `discoveries.csv` | Feature discovery classification (expected vs. unexpected) |\n",
    "| `FIGURE_2_shap_composite.png` | Publication-quality composite (300 DPI) |\n",
    "\n",
    "**â† Previous:** Scientific Validation (notebook 03)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AstroSpectro (PC, Py3.11)",
   "language": "python",
   "name": "astrospectro-pc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
